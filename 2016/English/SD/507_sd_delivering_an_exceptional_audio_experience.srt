1
00:00:07,516 --> 00:00:17,500
[ Music ]


2
00:00:26,126 --> 00:00:26,536
>> All right.


3
00:00:27,041 --> 00:00:29,041
[ Applause ]


4
00:00:29,066 --> 00:00:29,936
>> Good afternoon, everyone.


5
00:00:30,986 --> 00:00:32,456
So, how many of you
want to build an app


6
00:00:32,526 --> 00:00:34,786
with really cool audio
effects but thought


7
00:00:34,786 --> 00:00:35,576
that that might be hard?


8
00:00:36,846 --> 00:00:38,376
Or how many of you
want to focus more


9
00:00:38,376 --> 00:00:41,196
on your application's overall
user experience but ended


10
00:00:41,196 --> 00:00:42,846
up spending a little
more time on audio?


11
00:00:43,776 --> 00:00:45,936
Well, we've been working
hard to make it easy for you.


12
00:00:46,646 --> 00:00:47,456
My name is Saleem.


13
00:00:47,776 --> 00:00:49,336
I'm a Craftsman on
the Core Audio Team.


14
00:00:49,596 --> 00:00:51,176
I want to welcome you
to today's session


15
00:00:51,176 --> 00:00:53,356
on delivering an
exceptional audio experience.


16
00:00:54,586 --> 00:00:57,046
So let's look at an overview
of what's in our stack today.


17
00:00:57,816 --> 00:01:00,626
We'll start with our
AVFoundation framework.


18
00:01:01,326 --> 00:01:03,826
We have a wide variety
of high-level APIs


19
00:01:04,686 --> 00:01:06,696
that let you simply
play and record audio.


20
00:01:08,176 --> 00:01:12,726
For more advanced use cases, we
have our AudioToolbox framework.


21
00:01:13,666 --> 00:01:15,626
And you may have
heard of AudioUnits.


22
00:01:15,776 --> 00:01:17,546
These are a fundamental
building block.


23
00:01:18,316 --> 00:01:22,416
If you have to work with
MIDI devices or MIDI data,


24
00:01:22,526 --> 00:01:23,796
we have our CoreMIDI framework.


25
00:01:24,946 --> 00:01:27,376
For game development,
there's OpenAL.


26
00:01:27,976 --> 00:01:31,006
And over the last two years,


27
00:01:31,006 --> 00:01:33,676
we've been adding many new
APIs and features as well.


28
00:01:33,736 --> 00:01:35,886
So you can see there
are many ways


29
00:01:35,886 --> 00:01:37,866
that you can use audio
in your application.


30
00:01:38,296 --> 00:01:40,506
So, our goal today
is to help guide you


31
00:01:40,506 --> 00:01:43,516
to choosing the right API
for your application's needs.


32
00:01:44,126 --> 00:01:46,406
But don't worry, we also
have a few new things


33
00:01:46,406 --> 00:01:50,116
to share with you as well.


34
00:01:50,376 --> 00:01:52,346
So, on the agenda
today, we'll first look


35
00:01:52,346 --> 00:01:54,996
at some essential setup steps
for a few of our platforms.


36
00:01:55,876 --> 00:01:58,936
Then, we'll dive straight into
simple and advanced playback


37
00:01:58,936 --> 00:01:59,956
and recording scenarios.


38
00:02:00,966 --> 00:02:02,736
We'll talk a bit about
multichannel audio.


39
00:02:03,806 --> 00:02:05,166
And then later in
the presentation,


40
00:02:05,236 --> 00:02:06,466
we'll look at real-time audio --


41
00:02:07,166 --> 00:02:09,276
how you can build your
own effects, instruments,


42
00:02:09,276 --> 00:02:11,596
and generators -- and then
we'll wrap up with MIDI.


43
00:02:12,316 --> 00:02:16,216
So, let's get started.


44
00:02:16,216 --> 00:02:20,016
iOS, watchOS, and tvOS all
have really rich audio features


45
00:02:20,306 --> 00:02:21,916
and numerous writing
capabilities.


46
00:02:22,706 --> 00:02:26,896
So users can make calls,
play music, play games,


47
00:02:27,356 --> 00:02:28,946
work with various
productivity apps.


48
00:02:29,046 --> 00:02:32,326
And they can do all of this
mixed in or independently.


49
00:02:33,306 --> 00:02:36,746
So the operating system manages
a lot of default audio behaviors


50
00:02:36,746 --> 00:02:39,486
in order to provide a
consistent user experience.


51
00:02:40,066 --> 00:02:43,576
So let's look at a diagram
showing how audio is a


52
00:02:43,576 --> 00:02:44,596
managed service.


53
00:02:45,166 --> 00:02:49,136
So you have your device,
and it has a couple


54
00:02:49,136 --> 00:02:51,086
of inputs and outputs.


55
00:02:51,646 --> 00:02:54,596
And then there's the
operating system.


56
00:02:55,906 --> 00:02:58,876
It may be hosting many apps,
some of which are using audio.


57
00:03:00,096 --> 00:03:01,876
And lastly, there's
your application.


58
00:03:03,876 --> 00:03:07,216
So AVAudioSession is your
interface, as a developer,


59
00:03:07,306 --> 00:03:09,886
for expressing your
application needs to the system.


60
00:03:11,006 --> 00:03:15,076
Let's go into a bit
more detail about that.


61
00:03:16,206 --> 00:03:17,976
Categories express
the application's


62
00:03:18,096 --> 00:03:18,996
highest-level needs.


63
00:03:19,816 --> 00:03:21,976
We have modes and
category options


64
00:03:21,976 --> 00:03:25,036
which help you further customize
and specialize your application.


65
00:03:26,996 --> 00:03:30,386
If you're into some
more advanced use cases,


66
00:03:30,636 --> 00:03:32,486
such as input selection,
you may want to be able


67
00:03:32,486 --> 00:03:33,896
to choose the front microphone


68
00:03:33,896 --> 00:03:35,336
on your iPhone instead
of the bottom.


69
00:03:36,456 --> 00:03:38,096
If you're working with
multichannel audio


70
00:03:38,096 --> 00:03:40,726
and multichannel content on
tvOS, you may be interested


71
00:03:40,726 --> 00:03:42,026
in things like channel count.


72
00:03:43,126 --> 00:03:45,846
If you had a USB audio device
connected to your iPhone,


73
00:03:45,846 --> 00:03:50,726
you may be interested in
things like sample rate.


74
00:03:52,336 --> 00:03:55,416
So when your application
is ready and configured


75
00:03:55,416 --> 00:03:58,686
to use audio, it informs the
system to apply the session.


76
00:03:59,616 --> 00:04:01,496
So this will configure
the device's hardware


77
00:04:01,576 --> 00:04:04,436
for your application's needs
and may actually result


78
00:04:04,436 --> 00:04:06,866
in interrupting other audio
applications on the system,


79
00:04:07,306 --> 00:04:09,856
mixing with them, and/or
ducking their volume level.


80
00:04:13,696 --> 00:04:15,836
So let's look at some
of the essential steps


81
00:04:15,836 --> 00:04:17,296
when working with
AVAudioSession.


82
00:04:18,055 --> 00:04:20,185
The first step is to sign
up for notifications.


83
00:04:21,386 --> 00:04:23,286
And the three most important
notifications are the


84
00:04:23,286 --> 00:04:25,216
interruption, route change,


85
00:04:25,446 --> 00:04:27,596
and mediaServicesWereReset
notification.


86
00:04:28,906 --> 00:04:31,236
You can sign up for these
notifications before you


87
00:04:31,236 --> 00:04:32,106
activate your session.


88
00:04:32,106 --> 00:04:34,576
And in a few slides, I'll show
you how you can manage them.


89
00:04:35,066 --> 00:04:39,406
Next, based on your
application's high-level needs,


90
00:04:39,626 --> 00:04:41,716
you'll want to set the
appropriate category mode


91
00:04:41,716 --> 00:04:42,296
and options.


92
00:04:42,296 --> 00:04:44,526
So, let's look at
a few examples.


93
00:04:44,526 --> 00:04:48,756
Let's just say I was
building a productivity app.


94
00:04:48,996 --> 00:04:51,426
And in that application, I
want to play a simple sound


95
00:04:51,456 --> 00:04:52,906
when the user saves
their document.


96
00:04:53,936 --> 00:04:56,376
Here, we can see that audio
enhances the experience


97
00:04:56,536 --> 00:04:58,006
but it's not necessarily
required.


98
00:04:58,736 --> 00:05:01,186
So, in this case, I'd want
to use the AmbientCategory.


99
00:05:02,646 --> 00:05:04,896
This category obeys
the ringer switch.


100
00:05:05,526 --> 00:05:07,166
It does not play audio
in the background,


101
00:05:07,556 --> 00:05:09,176
and it'll always
mix in with others.


102
00:05:09,836 --> 00:05:14,156
If I was building a podcast app,


103
00:05:14,366 --> 00:05:16,016
I'd want to use the
PlaybackCategory,


104
00:05:17,326 --> 00:05:18,396
the SpokenAudio mode.


105
00:05:19,486 --> 00:05:21,696
And here, we can see that this
app location will interrupt


106
00:05:21,776 --> 00:05:25,356
other applications
on the system.


107
00:05:25,576 --> 00:05:27,916
Now if you want your
audio to continue playing


108
00:05:27,916 --> 00:05:29,046
in the background,
you'll also have


109
00:05:29,046 --> 00:05:32,006
to specify the background
audio key in your info.plist.


110
00:05:32,376 --> 00:05:34,516
And this is essentially a
session property as well.


111
00:05:34,736 --> 00:05:37,306
It's just expressed
through a different means.


112
00:05:39,616 --> 00:05:41,006
For your navigation app,


113
00:05:41,006 --> 00:05:43,516
let's look at how you can
configure the navigation prompt.


114
00:05:44,396 --> 00:05:46,656
Here, you'd want to use
the PlaybackCategory,


115
00:05:47,506 --> 00:05:48,406
the DefaultMode.


116
00:05:49,146 --> 00:05:50,896
And there are a few
options of interest here.


117
00:05:51,556 --> 00:05:54,156
You'd want to use both
the InterruptSpokenAudio


118
00:05:54,196 --> 00:05:56,686
AndMixWithOthers as
well as the duckOthers.


119
00:05:57,776 --> 00:06:00,216
So, if you're listening to
a podcast while navigating


120
00:06:00,216 --> 00:06:02,526
and that navigation prompt
comes up saying, "Oh,


121
00:06:02,526 --> 00:06:03,876
turn left in 500 feet,"


122
00:06:04,246 --> 00:06:07,386
it'll actually interrupt
the podcast app.


123
00:06:07,386 --> 00:06:08,476
If you're listening to music,


124
00:06:08,906 --> 00:06:12,326
it'll duck the music's volume
level and mix in with it.


125
00:06:13,436 --> 00:06:15,256
For this application,
you'll also want


126
00:06:15,256 --> 00:06:16,796
to use a background
audio key as well.


127
00:06:17,426 --> 00:06:23,296
So, next, let's look at how
we can manage activation


128
00:06:23,296 --> 00:06:24,026
of our session.


129
00:06:24,646 --> 00:06:27,176
So what does it mean
to go active?


130
00:06:27,786 --> 00:06:29,906
Activating your session
informs the system


131
00:06:29,906 --> 00:06:32,426
to configure the hardware
for your application's needs.


132
00:06:33,826 --> 00:06:34,826
So let's say, for example,


133
00:06:34,826 --> 00:06:36,726
I had an application
whose category was set


134
00:06:36,726 --> 00:06:37,556
to PlayAndRecord.


135
00:06:38,446 --> 00:06:41,466
When I active my session,
it'll configure the hardware


136
00:06:41,466 --> 00:06:44,696
to use input and output.


137
00:06:46,626 --> 00:06:49,466
Now, what happens if I activate
my session while listening


138
00:06:49,566 --> 00:06:50,976
to music from the music app?


139
00:06:51,656 --> 00:06:53,096
Here, we can see that
the current state


140
00:06:53,096 --> 00:06:55,006
of the system is set
for playback only.


141
00:06:56,116 --> 00:06:59,156
So, when I activate my
session, I inform the system


142
00:06:59,156 --> 00:07:02,486
to configure the hardware
for both input and output.


143
00:07:03,416 --> 00:07:05,586
And since I'm in a
non-mixable app location,


144
00:07:05,856 --> 00:07:08,066
I've interrupted the music app.


145
00:07:09,936 --> 00:07:12,826
So let's just say my application
makes a quick recording.


146
00:07:12,826 --> 00:07:15,106
Once I'm done, I
deactivate my session.


147
00:07:16,116 --> 00:07:17,976
And if I choose to notify others


148
00:07:17,976 --> 00:07:19,246
that I've deactivated
my session,


149
00:07:19,246 --> 00:07:21,576
we'll see that the music
app would resume playback.


150
00:07:22,206 --> 00:07:28,116
Next, let's look at how we can
handle the notifications we


151
00:07:28,116 --> 00:07:28,736
signed up for.


152
00:07:29,336 --> 00:07:33,016
We'll first look at the
interruption notification,


153
00:07:33,456 --> 00:07:34,656
and we'll examine a case


154
00:07:34,656 --> 00:07:36,996
where your application
does not have playback UI.


155
00:07:37,916 --> 00:07:39,956
The first thing I do is I
get the interruptionType.


156
00:07:40,796 --> 00:07:42,626
And if it's the beginning
of an interruption,


157
00:07:43,206 --> 00:07:44,656
your session is already
inactive.


158
00:07:45,756 --> 00:07:48,896
So your players have been
paused, and you'll use this time


159
00:07:48,896 --> 00:07:51,566
to update any internal
state that you have.


160
00:07:52,636 --> 00:07:55,796
When you receive the end
interruption, you go ahead


161
00:07:55,796 --> 00:07:58,516
and activate your session,
start your players,


162
00:07:58,786 --> 00:07:59,916
and update your internal state.


163
00:08:00,446 --> 00:08:03,116
Now, let's see how that differs


164
00:08:03,116 --> 00:08:04,996
for an application
that has playback UI.


165
00:08:05,966 --> 00:08:09,306
So when you receive the
begin interruption --


166
00:08:09,836 --> 00:08:11,186
again, your session
is inactive --


167
00:08:11,316 --> 00:08:14,596
you update the internal state,
as well as your UI this time.


168
00:08:15,276 --> 00:08:18,096
So if you have a Play/Pause
button, you'd want to go ahead


169
00:08:18,096 --> 00:08:22,196
and set that to "play"
at this time.


170
00:08:22,376 --> 00:08:25,576
And now when you receive the end
interruption, you should check


171
00:08:25,576 --> 00:08:27,636
and see if the shouldResume
option was passed in.


172
00:08:28,536 --> 00:08:30,316
If that was passed in,
then you can go ahead


173
00:08:30,316 --> 00:08:32,796
and activate your
session, start playback,


174
00:08:32,796 --> 00:08:34,366
and update your internal
state and UI.


175
00:08:35,496 --> 00:08:36,936
If it wasn't passed
in, you should wait


176
00:08:36,936 --> 00:08:39,426
until the user explicitly
resumes playback.


177
00:08:41,086 --> 00:08:44,776
It's important to
note that you can have


178
00:08:44,776 --> 00:08:45,836
unmatched interruptions.


179
00:08:45,836 --> 00:08:48,846
So, not every begin interruption
is followed by a matching end.


180
00:08:48,846 --> 00:08:52,496
And an example of this are
media-player applications


181
00:08:52,496 --> 00:08:53,436
that interrupt each other.


182
00:08:53,436 --> 00:08:59,416
Now, let's look at how we
can handle route changes.


183
00:09:00,686 --> 00:09:02,466
Route changes happen for
a number of reasons --


184
00:09:03,266 --> 00:09:04,966
the connected devices
may have changed,


185
00:09:05,316 --> 00:09:06,726
a category may have changed,


186
00:09:07,196 --> 00:09:09,566
you may have selected a
different data source or port.


187
00:09:10,616 --> 00:09:12,816
So, the first thing you do is
you get the routeChangeReason.


188
00:09:15,116 --> 00:09:18,616
If you receive a reason that
the old device is unavailable


189
00:09:18,976 --> 00:09:21,116
in your media-playback
app, you should go ahead


190
00:09:21,116 --> 00:09:22,386
and stop playback at this time.


191
00:09:22,606 --> 00:09:25,416
An example of this is if
your user is streaming music


192
00:09:25,416 --> 00:09:27,826
to the headsets and they
unplug the headsets.


193
00:09:28,086 --> 00:09:30,456
They don't expect that
the music resumes playback


194
00:09:30,646 --> 00:09:31,736
through the speakers right away.


195
00:09:34,336 --> 00:09:35,886
For more advanced use cases,


196
00:09:35,886 --> 00:09:38,166
if you receive the
oldDeviceUnavailable


197
00:09:38,166 --> 00:09:41,076
or newDeviceAvailable
routeChangeReason, you may want


198
00:09:41,076 --> 00:09:43,486
to re-evaluate certain
session properties


199
00:09:43,486 --> 00:09:44,906
as it applies to
your application.


200
00:09:45,336 --> 00:09:50,986
Lastly, let's look at how we
can handle the media services


201
00:09:50,986 --> 00:09:52,326
where we set the notification.


202
00:09:53,446 --> 00:09:56,116
This notification is
rare, but it does happen


203
00:09:56,116 --> 00:09:59,976
because demons aren't
guaranteed to run forever.


204
00:10:00,056 --> 00:10:01,206
The important thing
to note here is


205
00:10:01,206 --> 00:10:04,586
that your AVAudioSession
sharedInstance is still valid.


206
00:10:05,996 --> 00:10:08,946
You will need to reset your
category mode and other options.


207
00:10:11,016 --> 00:10:13,436
You'll also need to destroy and
recreate your player objects,


208
00:10:13,436 --> 00:10:16,576
such as your AVAudioEngine,
remote I/Os,


209
00:10:16,576 --> 00:10:18,046
and other player
objects as well.


210
00:10:19,636 --> 00:10:22,686
And we provide a means for
testing this on devices by going


211
00:10:22,946 --> 00:10:25,516
to Settings, Developer,
Reset Media Services.


212
00:10:29,496 --> 00:10:32,676
OK, so that just recaps
the four steps for working


213
00:10:32,676 --> 00:10:34,766
with AVAudioSession --
the essential steps.


214
00:10:34,936 --> 00:10:36,216
You sign up for notifications.


215
00:10:36,646 --> 00:10:38,956
You set the appropriate
category mode and options.


216
00:10:39,356 --> 00:10:40,926
You manage activation
of your session.


217
00:10:41,436 --> 00:10:42,846
And you handle the
notifications.


218
00:10:43,436 --> 00:10:44,836
So let's look at some
new stuff this year.


219
00:10:46,806 --> 00:10:49,126
New this year, we're adding
two new category options --


220
00:10:49,506 --> 00:10:52,006
allowAirPlay and
allowBluetoothA2DP --


221
00:10:52,106 --> 00:10:53,446
to the PlayAndRecord category.


222
00:10:55,086 --> 00:10:58,466
So, that means that you can now
use a microphone while playing


223
00:10:58,466 --> 00:11:00,086
to a Bluetooth and
AirPlay destination.


224
00:11:00,746 --> 00:11:04,336
So if this is your
application's use case, go ahead


225
00:11:04,336 --> 00:11:06,486
and set the category
and the options,


226
00:11:06,786 --> 00:11:08,176
and then let the
user pick the route


227
00:11:08,176 --> 00:11:11,636
from either an MPVolumeView
or Control Center.


228
00:11:13,536 --> 00:11:17,326
We're also adding a new
property for VoIP apps


229
00:11:17,386 --> 00:11:19,386
on our
AVAudioSessionPortDescription


230
00:11:19,576 --> 00:11:20,426
that'll determine whether


231
00:11:20,426 --> 00:11:22,626
or not the current
route has hardware voice


232
00:11:22,626 --> 00:11:23,536
processing enabled.


233
00:11:24,726 --> 00:11:27,356
So if your user is
connected to a CarPlay system


234
00:11:27,726 --> 00:11:30,946
or a Bluetooth HFP headset that
has hardware voice processing,


235
00:11:31,416 --> 00:11:32,536
you can use this property


236
00:11:32,886 --> 00:11:34,816
to disable your software
voice processing


237
00:11:34,816 --> 00:11:38,146
so you're not double-processing
the audio.


238
00:11:39,066 --> 00:11:41,976
If you're already using Apple's
built-in voice processing IO


239
00:11:41,976 --> 00:11:43,906
unit, you don't have
to worry about this.


240
00:11:45,236 --> 00:11:46,786
And new this year, we
also introduced the


241
00:11:46,786 --> 00:11:47,646
CallKit framework.


242
00:11:48,226 --> 00:11:50,966
So, to see how you can enhance
your VoIP apps with CallKit,


243
00:11:51,286 --> 00:11:52,456
we had a session
earlier this week.


244
00:11:52,706 --> 00:11:54,906
And if you missed that, you can
go ahead and catch it online.


245
00:11:55,726 --> 00:12:00,286
So that's just an
overview of AVAudioSession.


246
00:12:00,326 --> 00:12:01,946
We've covered a lot
of this stuff in-depth


247
00:12:01,946 --> 00:12:02,866
in previous sessions.


248
00:12:02,866 --> 00:12:04,476
So we encourage you
to check those out,


249
00:12:04,476 --> 00:12:07,406
as well as a programming
guide online.


250
00:12:09,536 --> 00:12:10,456
So, moving on.


251
00:12:10,956 --> 00:12:12,446
So you set up AVAudioSession


252
00:12:12,446 --> 00:12:13,896
if it's applicable
to your platform.


253
00:12:14,286 --> 00:12:15,766
Now, let's look at how
you can simply play


254
00:12:15,766 --> 00:12:17,726
and record audio in
your application.


255
00:12:18,296 --> 00:12:21,666
We'll start with the
AVFoundation framework.


256
00:12:22,506 --> 00:12:24,766
There are a number of classes
here that can handle the job.


257
00:12:25,096 --> 00:12:26,306
We have our AVAudioPlayer,


258
00:12:26,856 --> 00:12:31,526
AVAudioRecorder,
and AVPlayer class.


259
00:12:32,316 --> 00:12:35,156
AVAudioPlayer is the simplest
way to play audio from a file.


260
00:12:36,096 --> 00:12:38,046
We support a wide
variety of formats.


261
00:12:39,326 --> 00:12:41,566
We provide all the basic
playback operations.


262
00:12:42,436 --> 00:12:44,286
We also support some
more advanced operations,


263
00:12:44,286 --> 00:12:45,546
such as setting volume level.


264
00:12:46,156 --> 00:12:47,986
You get metering on
a per-channel basis.


265
00:12:48,246 --> 00:12:50,786
You can loop your playback,
adjust the playback rate,


266
00:12:51,446 --> 00:12:52,926
work with stereo panning.


267
00:12:53,696 --> 00:12:56,406
If you're on iOS or
tvOS, you can work


268
00:12:56,406 --> 00:12:57,556
with channel assignments.


269
00:12:58,216 --> 00:13:03,096
If you had multiple files
you wanted to playback,


270
00:13:03,096 --> 00:13:05,576
you can use multiple
AVAudioPlayer objects


271
00:13:05,766 --> 00:13:07,606
and you can synchronize
your playback as well.


272
00:13:08,356 --> 00:13:12,746
And new this year, we're adding
a method that lets you fade


273
00:13:12,746 --> 00:13:14,806
to volume level over
a specified duration.


274
00:13:15,526 --> 00:13:19,496
So let's look at a code example


275
00:13:19,496 --> 00:13:21,906
of how you can use AVAudioPlayer
in your application.


276
00:13:23,006 --> 00:13:23,986
Let's just say I was working


277
00:13:23,986 --> 00:13:26,246
and building a simple
productivity app again


278
00:13:26,456 --> 00:13:27,946
where I want to play an
acknowledgement sound


279
00:13:27,946 --> 00:13:29,306
when the user saves
their document.


280
00:13:30,456 --> 00:13:33,636
In this case, I have an
AVAudioPlayer and a URL


281
00:13:33,636 --> 00:13:36,166
to my asset in my class.


282
00:13:36,826 --> 00:13:38,946
Now in my setup function,
I go ahead


283
00:13:38,996 --> 00:13:42,266
and I create the AVAudioPlayer
object with the contents


284
00:13:42,266 --> 00:13:45,156
of my URL and I prepare
the player for playback.


285
00:13:45,986 --> 00:13:49,946
And then, in my saveDocument
function, I may do some work


286
00:13:49,946 --> 00:13:52,506
to see whether or not the
document was saved successfully.


287
00:13:52,506 --> 00:13:54,476
And if it was, then I
simply play my file.


288
00:13:55,206 --> 00:13:55,906
Really easy.


289
00:13:55,906 --> 00:14:00,086
Now, let's look at
AVAudioRecorder.


290
00:14:00,666 --> 00:14:03,826
This is the simplest way
to record audio to a file.


291
00:14:04,796 --> 00:14:07,396
You can record for a specified
duration, or you can record


292
00:14:07,396 --> 00:14:09,486
until the user explicitly stops.


293
00:14:09,786 --> 00:14:12,166
You get metering on
a per-channel basis,


294
00:14:12,426 --> 00:14:14,746
and we support a wide
variety of encoded formats.


295
00:14:16,356 --> 00:14:17,566
So, to set up a format,


296
00:14:17,596 --> 00:14:19,516
we use the Recorder
Settings Dictionary.


297
00:14:20,006 --> 00:14:21,926
And now this is a
dictionary of keys that has --


298
00:14:21,926 --> 00:14:25,176
a list of keys that let you
set various format parameters


299
00:14:25,566 --> 00:14:27,846
such as sample rate,
number of channels.


300
00:14:28,536 --> 00:14:30,816
If you're working with Linear
PCM data, you can adjust things


301
00:14:30,816 --> 00:14:32,296
like the bit depth
and endian-ness.


302
00:14:32,916 --> 00:14:35,036
If you're working with encoded
formats, you can adjust things


303
00:14:35,036 --> 00:14:36,336
such as quality and bit rate.


304
00:14:37,236 --> 00:14:38,426
So, let's look at a code example


305
00:14:38,426 --> 00:14:40,006
of how you can use
AVAudioRecorder.


306
00:14:40,786 --> 00:14:45,406
So the first thing I do is
I create my format settings.


307
00:14:46,086 --> 00:14:49,116
Here, I'm creating an AAC file
with a really high bit rate.


308
00:14:50,576 --> 00:14:52,186
And then the next thing I do --


309
00:14:52,256 --> 00:14:54,346
I go ahead and create my
AVAudioRecorder object


310
00:14:54,996 --> 00:14:56,966
with a URL to the file location


311
00:14:57,546 --> 00:14:59,296
and the format settings
I've just defined.


312
00:14:59,986 --> 00:15:04,586
And in this example, I have a
simple button that I'm using


313
00:15:04,586 --> 00:15:06,136
to toggle the state
of the recorder.


314
00:15:06,496 --> 00:15:09,026
So when I press the button,
if the recorder is recording,


315
00:15:09,506 --> 00:15:10,926
I go ahead and stop recording.


316
00:15:11,486 --> 00:15:12,886
else -- I start my recording.


317
00:15:13,126 --> 00:15:14,866
And I can use the
recorders built in meters


318
00:15:14,866 --> 00:15:18,396
to provide feedback to the UI.


319
00:15:19,276 --> 00:15:20,796
Lastly, let's look at AVPlayer.


320
00:15:22,336 --> 00:15:24,586
AVPlayer works not
only with local files


321
00:15:24,586 --> 00:15:26,066
but streaming content as well.


322
00:15:27,016 --> 00:15:29,056
You have all the standard
control available.


323
00:15:30,566 --> 00:15:32,366
We also provide built-in
user interfaces


324
00:15:32,366 --> 00:15:34,856
that you can use directly,
such as the AVPlayerView


325
00:15:35,256 --> 00:15:36,636
and the AVPlayerViewController.


326
00:15:38,386 --> 00:15:40,536
And AVPlayer also works
with video content as well.


327
00:15:40,746 --> 00:15:43,416
And this year, we added a number
of new features to AVPlayer.


328
00:15:43,536 --> 00:15:46,046
So if you want to find out
what we did, you can check


329
00:15:46,046 --> 00:15:47,896
out the Advances in
AVFoundation Playback.


330
00:15:47,896 --> 00:15:49,946
And if you missed that, you can
go ahead and catch it online.


331
00:15:55,046 --> 00:15:58,346
OK, so what we've seen so far is
just some very simple examples


332
00:15:58,346 --> 00:15:59,786
of playback and recording.


333
00:16:00,406 --> 00:16:04,206
So now let's look at some
more advanced use cases.


334
00:16:04,336 --> 00:16:07,956
Advanced use cases include
playing back not only from files


335
00:16:08,006 --> 00:16:12,576
but working with buffers
of audio data as well.


336
00:16:12,776 --> 00:16:15,126
You may be interested in
doing some audio processing,


337
00:16:15,196 --> 00:16:16,466
applying certain effects


338
00:16:16,466 --> 00:16:18,196
and mixing together
multiple sources.


339
00:16:18,896 --> 00:16:22,346
Or you may be interested
in implementing 3D audio.


340
00:16:22,986 --> 00:16:26,416
So, some examples of this
are you're building a classic


341
00:16:26,416 --> 00:16:29,556
karaoke app, you want
to build a deejay app


342
00:16:29,556 --> 00:16:32,496
with really amazing effects,
or you want to build a game


343
00:16:32,496 --> 00:16:34,766
and really immerse
your user in it.


344
00:16:35,876 --> 00:16:38,446
So, for such advanced use
cases, we have a class


345
00:16:38,536 --> 00:16:40,826
in AVFoundation called
AVAudioEngine.


346
00:16:42,796 --> 00:16:45,126
AVAudioEngine is a powerful,


347
00:16:45,246 --> 00:16:47,246
feature-rich Objective-C
and Swift API.


348
00:16:48,276 --> 00:16:53,076
It's a real-time audio system,
and it simplifies working


349
00:16:53,076 --> 00:16:53,896
with real-time audio


350
00:16:53,896 --> 00:16:56,276
by providing a non-real-time
interface for you.


351
00:16:57,466 --> 00:16:59,866
So this has a lot of
complexities dealing


352
00:16:59,866 --> 00:17:00,786
with real-time audio,


353
00:17:00,866 --> 00:17:02,586
and it makes your code
that much simpler.


354
00:17:04,455 --> 00:17:06,786
The Engine manages
a graph of nodes,


355
00:17:07,276 --> 00:17:09,526
and these nodes let you
play and record audio.


356
00:17:10,306 --> 00:17:12,056
You can connect these
nodes in various ways


357
00:17:12,056 --> 00:17:14,566
to form many different
processing chains


358
00:17:15,746 --> 00:17:16,796
and perform mixing.


359
00:17:17,776 --> 00:17:19,526
You can capture audio
at any point


360
00:17:19,526 --> 00:17:21,376
in the processing chain as well.


361
00:17:22,076 --> 00:17:23,336
And we provide a special node


362
00:17:23,336 --> 00:17:24,846
that lets you spatialize
your audio.


363
00:17:25,486 --> 00:17:27,685
So, let's look


364
00:17:27,685 --> 00:17:30,376
at the fundamental building
block -- the AVAudioNode.


365
00:17:32,006 --> 00:17:33,106
We have three types of nodes.


366
00:17:34,336 --> 00:17:36,636
We have source nodes, which
provide data for rendering.


367
00:17:37,706 --> 00:17:39,156
So these could be
your PlayerNode,


368
00:17:39,546 --> 00:17:41,766
an InputNode, or a sampler unit.


369
00:17:42,496 --> 00:17:46,166
We have processing nodes that
let you process audio data.


370
00:17:46,166 --> 00:17:48,686
So these could be
effects such as delays,


371
00:17:48,866 --> 00:17:50,506
distortions, and mixers.


372
00:17:51,226 --> 00:17:55,686
And we have the destination
node,


373
00:17:55,686 --> 00:17:57,666
which is the termination
node in your graph,


374
00:17:57,786 --> 00:17:59,826
and it's connected directly
to the output hardware.


375
00:18:00,526 --> 00:18:03,996
So let's look at a sample setup.


376
00:18:05,446 --> 00:18:08,386
Let's just say I'm building
a classic karaoke app.


377
00:18:09,356 --> 00:18:11,346
In this case, I have
three source nodes.


378
00:18:11,756 --> 00:18:14,376
I'm using the InputNode to
capture the user's voice.


379
00:18:14,986 --> 00:18:17,106
I'm using a PlayerNode
to play my Backing Track.


380
00:18:18,336 --> 00:18:20,776
I'm using another PlayerNode
to play other sound effects


381
00:18:20,776 --> 00:18:22,576
and feedback used to the user.


382
00:18:23,196 --> 00:18:25,686
In terms of processing nodes,


383
00:18:25,786 --> 00:18:28,586
I may want to apply a specific
EQ to the user's voice.


384
00:18:30,046 --> 00:18:31,856
And then I'm going
to use the mixer


385
00:18:31,856 --> 00:18:34,176
to mix all three sources
into a single output.


386
00:18:35,126 --> 00:18:36,876
And then the single
output will then be played


387
00:18:36,976 --> 00:18:39,356
through the OutputNode and then
out to the output hardware.


388
00:18:40,026 --> 00:18:44,456
I can also capture the user's
voice and do some analysis


389
00:18:44,456 --> 00:18:45,986
to see how well they're
performing


390
00:18:45,986 --> 00:18:47,196
by installing a TapBlock.


391
00:18:48,326 --> 00:18:49,306
And then based on that,


392
00:18:49,306 --> 00:18:52,596
I can unconditionally
schedule these feedback queues


393
00:18:52,596 --> 00:18:54,426
to be played out.


394
00:18:55,376 --> 00:18:58,426
So let's now look at
a sample game setup.


395
00:19:00,016 --> 00:19:02,796
The main node of interest
here is the EnvironmentNode,


396
00:19:03,106 --> 00:19:04,776
which simulates a 3D space


397
00:19:05,176 --> 00:19:07,086
and spatializes its
connected sources.


398
00:19:07,756 --> 00:19:11,696
In this example, I'm using
the InputNode as well


399
00:19:11,696 --> 00:19:12,996
as a PlayerNode as my source.


400
00:19:13,566 --> 00:19:17,106
And you can also adjust
various 3D mixing properties


401
00:19:17,106 --> 00:19:20,416
on your sources as well,
such as position, occlusion.


402
00:19:21,476 --> 00:19:22,756
And in terms of the
EnvironmentNode,


403
00:19:22,756 --> 00:19:24,016
you can also adjust
properties there,


404
00:19:24,016 --> 00:19:27,866
such as the listenerPosition as
well as other reverb parameters.


405
00:19:28,646 --> 00:19:33,586
So this 3D Space can then be
mixed in with a Backing Track


406
00:19:33,786 --> 00:19:35,726
and then played through
the output.


407
00:19:39,536 --> 00:19:42,206
So before we move any
further with AVAudioEngine,


408
00:19:42,206 --> 00:19:44,476
I want to look at some
fundamental core classes


409
00:19:44,606 --> 00:19:46,086
that the Engine uses
extensively.


410
00:19:47,356 --> 00:19:49,136
I'll first start
with AVAudioFormat.


411
00:19:49,136 --> 00:19:53,336
So, AVAudioFormat
describes the data format


412
00:19:53,586 --> 00:19:55,206
in an audio file or stream.


413
00:19:56,066 --> 00:19:58,986
So we have our standard
format, common formats,


414
00:19:59,226 --> 00:20:00,576
as well as compressed formats.


415
00:20:01,716 --> 00:20:04,266
This class also contains
an AVAudioChannelLayout


416
00:20:04,546 --> 00:20:07,276
which you may use when dealing
with multichannel audio.


417
00:20:07,566 --> 00:20:08,816
It's a modern interface


418
00:20:08,816 --> 00:20:10,646
to our
AudioStreamBasicDescription


419
00:20:10,646 --> 00:20:12,766
structure and our
AudioChannelLayout structure.


420
00:20:13,406 --> 00:20:16,916
Now, let's look at
AVAudioBuffer.


421
00:20:17,496 --> 00:20:19,746
This class has two subclasses.


422
00:20:19,746 --> 00:20:24,256
It has the AVAudioPCMBuffer,
which is used to hold PCM data.


423
00:20:24,726 --> 00:20:26,736
And it has the
AVAudioCompressBuffer,


424
00:20:26,736 --> 00:20:28,796
which is used for holding
compressed audio data.


425
00:20:30,226 --> 00:20:32,516
Both of these classes
provide a modern interface


426
00:20:32,516 --> 00:20:35,416
to our AudioBufferList and our
AudioStreamPacketDescription.


427
00:20:35,986 --> 00:20:39,306
Let's look at AVAudioFile.


428
00:20:40,746 --> 00:20:43,366
This class lets you read and
write from any supported format.


429
00:20:44,656 --> 00:20:47,466
It lets you read data into
PCM buffers and write data


430
00:20:47,466 --> 00:20:49,416
into a file from PCM buffers.


431
00:20:49,926 --> 00:20:50,576
And in doing so,


432
00:20:50,576 --> 00:20:53,156
it transparently handles
any encoding and decoding.


433
00:20:53,706 --> 00:20:59,186
And it supersedes now our
AudioFile and ExtAudioFile APIs.


434
00:21:00,326 --> 00:21:02,426
Lastly, let's look
at AVAudioConverter.


435
00:21:03,166 --> 00:21:07,586
This class handles
audio format conversion.


436
00:21:08,426 --> 00:21:12,256
So, you can convert between one
form of PCM data to another.


437
00:21:13,076 --> 00:21:17,746
You can also convert between
PCM and compressed audio formats


438
00:21:18,916 --> 00:21:21,036
in which it handles the
encoding and decoding for you.


439
00:21:23,296 --> 00:21:25,816
And this class supersedes
our AudioConverter API.


440
00:21:25,816 --> 00:21:31,126
And new this year, we've also
added a minimum phase sample


441
00:21:31,126 --> 00:21:32,296
rate converter algorithm.


442
00:21:34,116 --> 00:21:38,196
So you can see that all these
core classes really work


443
00:21:38,196 --> 00:21:41,016
together when interfacing
with audio data.


444
00:21:41,976 --> 00:21:45,316
Now, let's look at how
these classes then interact


445
00:21:45,316 --> 00:21:46,346
with AVAudioEngine.


446
00:21:46,946 --> 00:21:51,546
So if you look at
AVAudioNode, it has both input


447
00:21:51,546 --> 00:21:53,956
and output AVAudio formats.


448
00:21:55,076 --> 00:21:58,726
If you look at the PlayerNode,
it can provide you to the Engine


449
00:21:58,976 --> 00:22:01,896
from an AVAudioFile or
an AVAudioPCMBuffer.


450
00:22:03,196 --> 00:22:08,736
When you install a NodeTap, the
block provides audio data to you


451
00:22:08,736 --> 00:22:10,546
in the form of PCM buffers.


452
00:22:11,106 --> 00:22:13,566
You can do analysis with
it, or then you can save it


453
00:22:13,566 --> 00:22:15,456
to a file using an AVAudio file.


454
00:22:16,256 --> 00:22:19,296
If you're working with
a compressed stream,


455
00:22:19,296 --> 00:22:21,136
you can break it down
into compress buffers,


456
00:22:21,316 --> 00:22:24,536
use an AVAudioConverter to
convert it to PCM buffers,


457
00:22:24,886 --> 00:22:27,106
and then provide it to the
Engine through the PlayerNode.


458
00:22:32,156 --> 00:22:34,246
So, new this year,
we're bringing a subset


459
00:22:34,246 --> 00:22:36,106
of AVAudioEngine to the Watch.


460
00:22:37,066 --> 00:22:39,756
Along with that, we're including
a subset of AVAudioSession,


461
00:22:39,756 --> 00:22:43,596
as well as all the core
classes you've just seen.


462
00:22:43,826 --> 00:22:45,446
So I'm sure you'd love
to see a demo of this.


463
00:22:46,606 --> 00:22:47,386
So we have that for you.


464
00:22:47,786 --> 00:22:50,956
We built a simple game
using both SceneKit


465
00:22:50,956 --> 00:22:52,326
and AVAudioEngine directly.


466
00:22:53,396 --> 00:22:55,536
And in this game, what I'm doing
is I'm launching an asteroid


467
00:22:55,536 --> 00:22:56,286
into space.


468
00:22:56,766 --> 00:22:58,806
And at the bottom of the
screen, I have a flame.


469
00:22:58,886 --> 00:23:01,836
And I can control the flame
using the Watch's Digital Crown.


470
00:23:02,756 --> 00:23:04,656
And now if the asteroid
makes contact with the flame,


471
00:23:04,656 --> 00:23:06,466
it plays this really
loud explosion sound.


472
00:23:07,026 --> 00:23:08,000
So, let's see this.


473
00:23:16,516 --> 00:23:27,606
[ Explosions ]


474
00:23:28,106 --> 00:23:30,306
I'm sure this game, like,
defies basic laws of physics


475
00:23:30,346 --> 00:23:32,146
because it's playing
audio in space.


476
00:23:32,146 --> 00:23:32,976
Right? And that's not possible.


477
00:23:33,516 --> 00:23:36,546
[ Applause ]


478
00:23:37,046 --> 00:23:37,836
All right, so let me just go


479
00:23:37,836 --> 00:23:41,456
over quickly the
AVAudioEngine code in this game.


480
00:23:41,666 --> 00:23:44,276
So, in my class, I
have my AVAudioEngine.


481
00:23:44,866 --> 00:23:46,106
And I have two PlayerNodes --


482
00:23:46,546 --> 00:23:48,176
one for playing the
explosion sound,


483
00:23:48,356 --> 00:23:50,486
and one for playing
the launch sound.


484
00:23:51,376 --> 00:23:54,246
I also have URLs
to my audio assets.


485
00:23:54,846 --> 00:23:58,406
And in this example,
I'm using buffers


486
00:23:58,796 --> 00:24:00,186
to provide data to the engine.


487
00:24:00,786 --> 00:24:05,106
So, let's look at how
we set up the engine.


488
00:24:06,436 --> 00:24:07,876
The first thing I
do is I go ahead


489
00:24:07,876 --> 00:24:09,346
and I attach my PlayerNodes.


490
00:24:09,546 --> 00:24:11,976
So I touch the explosionPlayer
and the launchPlayer.


491
00:24:12,726 --> 00:24:15,396
Next, I'm going to
use the core classes.


492
00:24:15,816 --> 00:24:19,046
I'm going to create an AVAudio
file from the URL of my assets.


493
00:24:19,766 --> 00:24:21,416
And then, I'm going to
create a PCM buffer.


494
00:24:21,496 --> 00:24:22,826
And I'm going to read the data


495
00:24:22,826 --> 00:24:24,776
from the file into
the PCM buffer.


496
00:24:25,486 --> 00:24:28,346
And I can do this because my
audio files are really short.


497
00:24:28,906 --> 00:24:33,056
Next, I'll go ahead and
make the connections


498
00:24:33,276 --> 00:24:35,976
between the source nodes
and the engine's main mixer.


499
00:24:36,606 --> 00:24:41,986
So, when the game is
about to start, I go ahead


500
00:24:41,986 --> 00:24:44,416
and I start my engine
and I start my players.


501
00:24:45,746 --> 00:24:49,606
And when I launch an asteroid, I
simply schedule the launchBuffer


502
00:24:49,606 --> 00:24:51,016
to be played on the
launchPlayer.


503
00:24:51,786 --> 00:24:54,626
And when the asteroid makes
contact with the flame,


504
00:24:54,626 --> 00:24:57,136
I simply schedule the
explosionBuffer to be played


505
00:24:57,136 --> 00:24:58,086
on the explosionPlayer.


506
00:24:59,616 --> 00:25:00,996
So, with a few lines of code,


507
00:25:00,996 --> 00:25:03,306
I'm able to build a really
rich audio experience


508
00:25:03,306 --> 00:25:04,976
for my games on watchOS.


509
00:25:06,076 --> 00:25:08,036
And that was a simple
example, so we can't wait


510
00:25:08,036 --> 00:25:11,656
to see what you come up with.


511
00:25:13,286 --> 00:25:16,106
So, before I wrap up with
AVAudioEngine, I want to talk


512
00:25:16,106 --> 00:25:17,286
about multichannel audio


513
00:25:18,246 --> 00:25:21,966
and specifically how
it relates to tvOS.


514
00:25:22,366 --> 00:25:25,026
So, last October, we
introduced tvOS along


515
00:25:25,026 --> 00:25:26,586
with the 4th generation
Apple TV.


516
00:25:27,266 --> 00:25:30,356
And so this is the first time
we can talk about it at WWDC.


517
00:25:30,966 --> 00:25:32,806
And one of the interesting
things about audio


518
00:25:32,806 --> 00:25:35,686
on Apple TV is that many
users are already connected


519
00:25:35,776 --> 00:25:37,166
to multichannel hardware


520
00:25:37,436 --> 00:25:40,706
since many home theater
systems already support 5.1


521
00:25:40,706 --> 00:25:42,706
or 7.1 surround sound systems.


522
00:25:43,276 --> 00:25:44,836
So, today, I just want to go


523
00:25:44,836 --> 00:25:46,846
over how you can render
multichannel audio


524
00:25:47,276 --> 00:25:48,546
using AVAudioEngine.


525
00:25:49,146 --> 00:25:52,956
So, first, let's review the
setup with AVAudioSession.


526
00:25:53,036 --> 00:25:57,596
I first set my category
and other options,


527
00:25:57,596 --> 00:26:00,496
and then I activate my session
to configure the hardware


528
00:26:00,496 --> 00:26:02,186
for my application's needs.


529
00:26:03,936 --> 00:26:06,766
Now, depending on the
rendering format I want to use,


530
00:26:06,856 --> 00:26:07,976
I'll first need to check and see


531
00:26:07,976 --> 00:26:09,336
if the current route
supports it.


532
00:26:09,466 --> 00:26:12,226
And I can do that by
checking if my desired number


533
00:26:12,226 --> 00:26:13,796
of channels are less
than or equal


534
00:26:13,796 --> 00:26:15,586
to the maximum number
of output channels.


535
00:26:16,136 --> 00:26:17,726
And if it is, then
I can go ahead


536
00:26:17,726 --> 00:26:19,626
and set my preferred
number of output channels.


537
00:26:20,306 --> 00:26:24,186
I can then query back the
actual number of channels


538
00:26:24,186 --> 00:26:26,806
from the session and then
use that moving forward.


539
00:26:27,246 --> 00:26:31,036
Optionally, I can
look at the array


540
00:26:31,036 --> 00:26:32,976
of ChannelDescriptions
on the current port.


541
00:26:34,216 --> 00:26:37,046
And each ChannelDescription
gives me a channelLabel


542
00:26:37,486 --> 00:26:38,396
and a channelNumber.


543
00:26:39,416 --> 00:26:41,976
So I can use this information
to figure out the exact format


544
00:26:42,436 --> 00:26:45,316
and how I can map my content
to the connected hardware.


545
00:26:47,516 --> 00:26:49,826
Now, let's switch gears and
look at the AVAudioEngine setup.


546
00:26:50,886 --> 00:26:52,276
There are two use cases here.


547
00:26:52,436 --> 00:26:53,316
The first use case is


548
00:26:53,316 --> 00:26:55,886
if you already have
multichannel content.


549
00:26:57,766 --> 00:26:59,906
And the second use case is
if you have mono content


550
00:26:59,956 --> 00:27:01,086
and you want to spatialize it.


551
00:27:01,166 --> 00:27:02,856
And this is typically
geared towards games.


552
00:27:03,546 --> 00:27:09,506
So, in the first use case,
I have multichannel content


553
00:27:09,746 --> 00:27:10,836
and multichannel hardware.


554
00:27:11,576 --> 00:27:13,066
I simply get the
hardware format.


555
00:27:13,396 --> 00:27:14,676
I set that as my connection


556
00:27:14,676 --> 00:27:16,296
between my Mixer
and my OutputNode.


557
00:27:17,036 --> 00:27:19,896
And on the source side, I get
the content format and I set


558
00:27:19,926 --> 00:27:22,476
that as my connection between
my SourceNode and the Mixer.


559
00:27:23,316 --> 00:27:26,046
And here, the Mixer handles
the channel mapping for you.


560
00:27:26,046 --> 00:27:31,996
Now, in the second use case, we
have a bunch of mono sources.


561
00:27:32,516 --> 00:27:33,836
And we'll use the
EnvironmentNode


562
00:27:33,836 --> 00:27:34,746
to spatialize them.


563
00:27:35,936 --> 00:27:38,266
So, like before, we get
the hardware format.


564
00:27:38,806 --> 00:27:42,556
But before we set the compatible
format, we have to map it to one


565
00:27:42,556 --> 00:27:44,066
that the EnvironmentNode
supports.


566
00:27:45,296 --> 00:27:47,176
And for a list of
supported formats,


567
00:27:47,176 --> 00:27:48,716
you can check our
documentation online.


568
00:27:49,406 --> 00:27:51,326
So, I set the compatible format.


569
00:27:51,876 --> 00:27:55,176
And now on the source side, like
before, I get the content format


570
00:27:55,276 --> 00:27:57,736
and I set that as my
connection between my player


571
00:27:58,076 --> 00:27:59,006
and the EnvironmentNode.


572
00:28:00,106 --> 00:28:02,546
Lastly, I'll also have


573
00:28:02,796 --> 00:28:05,036
to set the multichannel
rendering algorithm


574
00:28:05,036 --> 00:28:07,306
to SoundField, which is
what the EnvironmentNode


575
00:28:07,386 --> 00:28:08,256
currently supports.


576
00:28:08,796 --> 00:28:12,716
And at this point, I can start
my engine, start playback,


577
00:28:13,336 --> 00:28:15,656
and then adjust all the
various 3D mixing properties


578
00:28:15,656 --> 00:28:16,366
that we support.


579
00:28:17,006 --> 00:28:20,756
So, just a recap.


580
00:28:21,266 --> 00:28:24,606
AVAudioEngine is a
powerful, feature-rich API.


581
00:28:25,636 --> 00:28:28,946
It simplifies working
with real-time audio.


582
00:28:30,546 --> 00:28:34,066
It enables you to work with
multichannel audio and 3D audio.


583
00:28:34,976 --> 00:28:36,196
And now, you can build games


584
00:28:36,196 --> 00:28:38,166
with really rich audio
experiences on your Watch.


585
00:28:38,766 --> 00:28:43,976
And it supersedes our
AUGraph and OpenAL APIs.


586
00:28:44,766 --> 00:28:47,086
So we've talked a bit about the
Engine in previous sessions,


587
00:28:47,086 --> 00:28:50,556
so we encourage you to
check those out if you can.


588
00:28:51,326 --> 00:28:53,246
And at this point, I'd like to
hand it over to my colleague,


589
00:28:53,246 --> 00:28:54,576
Doug, to keep it
rolling from here.


590
00:28:55,066 --> 00:28:55,286
Doug?


591
00:28:56,516 --> 00:29:00,626
[ Applause ]


592
00:29:01,126 --> 00:29:01,796
>> Thank you, Saleem.


593
00:29:02,806 --> 00:29:05,476
So, I'd like to continue
our tour


594
00:29:05,476 --> 00:29:07,136
through the audio APIs here.


595
00:29:07,646 --> 00:29:12,436
We talked about real-time audio
in passing with AVAudioEngine.


596
00:29:13,236 --> 00:29:14,896
Saleem emphasized that,


597
00:29:15,676 --> 00:29:19,236
while the audio processing is
happening in real-time context,


598
00:29:19,466 --> 00:29:22,286
we're controlling it from
non-real-time context.


599
00:29:22,286 --> 00:29:24,026
And that's the essence
of its simplicity.


600
00:29:24,706 --> 00:29:26,766
But there are times when
you actually want to do work


601
00:29:26,766 --> 00:29:30,316
in that real-time
process, or context.


602
00:29:30,566 --> 00:29:32,146
So I'd like to go
into that a bit.


603
00:29:32,586 --> 00:29:34,576
So, what is real-time audio?


604
00:29:34,696 --> 00:29:37,486
The use cases where
we need to do things


605
00:29:37,546 --> 00:29:40,036
in real-time are
characterized by low latency.


606
00:29:40,826 --> 00:29:44,536
Possibly the oldest
example I'm familiar


607
00:29:44,536 --> 00:29:47,296
with on our platforms is
with music applications.


608
00:29:47,696 --> 00:29:51,006
For example, you may
be synthesizing a sound


609
00:29:51,086 --> 00:29:54,086
when the user presses a
key on the MIDI keyboard.


610
00:29:54,766 --> 00:29:56,986
And we want to minimize
the time from when


611
00:29:56,986 --> 00:30:00,036
that MIDI note was struck
to when the note plays.


612
00:30:00,516 --> 00:30:04,316
And so we have real-time audio
effects like guitar pedals.


613
00:30:05,296 --> 00:30:09,056
We want to minimize the time it
takes from when the audio input


614
00:30:09,056 --> 00:30:11,096
of the guitar comes
into the computer


615
00:30:12,076 --> 00:30:15,846
through which we process it,
apply delays, distortion,


616
00:30:15,846 --> 00:30:17,646
and then send it back
out to the amplifier.


617
00:30:18,246 --> 00:30:19,586
So we need low latency there


618
00:30:19,586 --> 00:30:21,966
so that the instrument,
again, is responsive.


619
00:30:22,596 --> 00:30:26,186
Telephony is also characterized
by low latency requirements.


620
00:30:26,876 --> 00:30:29,986
We've all been on phone calls
with people in other countries


621
00:30:29,986 --> 00:30:31,966
and had very long delay times.


622
00:30:31,966 --> 00:30:33,756
It's no good in telephony.


623
00:30:34,156 --> 00:30:35,706
We do a lot of signal
processing.


624
00:30:36,076 --> 00:30:37,536
We need to keep the
latency down.


625
00:30:38,416 --> 00:30:41,646
Also, in game engines, we
like to keep the latency down.


626
00:30:42,166 --> 00:30:43,476
The user is doing things --


627
00:30:43,476 --> 00:30:45,926
interacting with
joysticks, whatever.


628
00:30:46,506 --> 00:30:49,006
We want to produce those
sounds as quickly as possible.


629
00:30:49,006 --> 00:30:51,126
Sometimes, we want to
manipulate those sounds


630
00:30:51,396 --> 00:30:52,366
as they're being rendered.


631
00:30:53,026 --> 00:30:55,716
Or maybe we just have
an existing game engine.


632
00:30:56,236 --> 00:31:00,066
In all these cases, we have a
need to write code that runs


633
00:31:00,306 --> 00:31:01,536
in a real-time context.


634
00:31:02,776 --> 00:31:10,476
In this real-time context,
the main characteristic of --


635
00:31:10,476 --> 00:31:13,126
our constraint is that we're
operating under deadlines.


636
00:31:14,176 --> 00:31:16,396
Right? Every some-number
of milliseconds,


637
00:31:16,456 --> 00:31:20,106
the system is waking us up,
asking us to produce some audio


638
00:31:20,106 --> 00:31:22,186
for that equally-small
slice of time.


639
00:31:22,696 --> 00:31:26,306
And we either accomplish it
and produce audio seamlessly.


640
00:31:26,836 --> 00:31:30,236
Or if we fail, if we take too
long to produce that audio,


641
00:31:30,806 --> 00:31:32,916
we create a gap in the output.


642
00:31:32,916 --> 00:31:35,236
And the user hears
that as a glitch.


643
00:31:36,006 --> 00:31:38,306
And this is a very small
interval that we have


644
00:31:38,786 --> 00:31:40,276
to create our audio in.


645
00:31:40,276 --> 00:31:43,226
Our deadlines are typically
as small as 3 milliseconds.


646
00:31:43,226 --> 00:31:46,726
And 20 milliseconds,
which is default on iOS,


647
00:31:46,726 --> 00:31:50,496
is still a pretty
constrained deadline.


648
00:31:51,536 --> 00:31:52,986
So, in this environment, we have


649
00:31:52,986 --> 00:31:55,146
to be really careful
about what we do.


650
00:31:56,176 --> 00:31:57,526
We can't really block.


651
00:31:57,716 --> 00:31:59,616
We can't allocate memory.


652
00:31:59,726 --> 00:32:01,496
We can't use mutexes.


653
00:32:01,566 --> 00:32:04,226
We can't access the
file system or sockets.


654
00:32:04,636 --> 00:32:05,416
We can't log.


655
00:32:06,026 --> 00:32:07,866
We can't even call
a dispatch "async"


656
00:32:07,946 --> 00:32:10,076
because it allocates
continuations.


657
00:32:10,876 --> 00:32:14,086
And we have to be careful not
to interact with the Objective-C


658
00:32:14,086 --> 00:32:18,506
and Swift runtimes because they
are not entirely real-time safe.


659
00:32:18,576 --> 00:32:21,786
There are cases when they,
too, will take mutexes.


660
00:32:22,756 --> 00:32:24,256
So that's a partial list.


661
00:32:24,306 --> 00:32:25,716
There other things we can't do.


662
00:32:25,716 --> 00:32:27,776
The primary thing
to ask yourself is,


663
00:32:28,106 --> 00:32:32,136
"Does this thing I'm doing
allocate memory or use mutexes?"


664
00:32:32,136 --> 00:32:35,266
And if the answer is yes,
then it's not real-time safe.


665
00:32:36,076 --> 00:32:37,216
Well, what can we do?


666
00:32:37,456 --> 00:32:39,896
I'll show you an example
of that in a little bit.


667
00:32:41,396 --> 00:32:44,086
But, first, I'd like
to just talk


668
00:32:44,086 --> 00:32:47,186
about how we manage this problem


669
00:32:47,186 --> 00:32:50,296
of packaging real-time
audio components.


670
00:32:50,596 --> 00:32:53,736
And we do this with an API
set called Audio Units.


671
00:32:54,446 --> 00:32:58,626
So this is a way
for us to package --


672
00:32:58,626 --> 00:33:00,616
and for you, for that matter,
as another developer --


673
00:33:01,036 --> 00:33:03,416
to package your signal
processing and modules


674
00:33:03,416 --> 00:33:06,316
that can be reused in
other applications.


675
00:33:06,776 --> 00:33:11,386
And it also provides an API
to manage the transitions


676
00:33:11,386 --> 00:33:15,516
and interactions between
your non-real-time context


677
00:33:15,516 --> 00:33:17,596
and your real-time
rendering context.


678
00:33:19,286 --> 00:33:22,246
So, as an app developer,
you can host Audio Units.


679
00:33:23,296 --> 00:33:25,736
That means you can let
the user choose one,


680
00:33:25,736 --> 00:33:28,526
or you can simply
hardcode references


681
00:33:28,556 --> 00:33:30,026
to system built-in units.


682
00:33:31,016 --> 00:33:33,466
You can also build
your own Audio Units.


683
00:33:34,126 --> 00:33:36,976
You can build them as app
extensions or plug-ins.


684
00:33:37,626 --> 00:33:41,886
And you can also simply
register an Audio Unit privately


685
00:33:41,886 --> 00:33:42,876
to your application.


686
00:33:42,876 --> 00:33:46,616
And this is useful, for example,
if you've got some small piece


687
00:33:46,616 --> 00:33:48,516
of signal processing
that you want to use


688
00:33:48,516 --> 00:33:51,046
in the context of AVAudioEngine.


689
00:33:53,376 --> 00:33:56,186
So, underneath Audio Units,


690
00:33:56,186 --> 00:33:58,636
we have an even more
fundamental API


691
00:33:59,236 --> 00:34:00,836
which we call Audio Components.


692
00:34:02,076 --> 00:34:06,346
So this is a set of APIs in
the AudioToolbox framework.


693
00:34:07,006 --> 00:34:09,815
The framework maintains
a registry of all


694
00:34:09,815 --> 00:34:11,565
of the components on the system.


695
00:34:13,096 --> 00:34:16,866
Every component has a type,
subtype, and manufacturer.


696
00:34:16,866 --> 00:34:18,255
These are 4-character codes.


697
00:34:18,706 --> 00:34:19,985
And those serve as the key


698
00:34:20,076 --> 00:34:22,246
for discovering them
and registering them.


699
00:34:24,025 --> 00:34:25,856
And there are a number
of different kinds


700
00:34:25,856 --> 00:34:27,525
of Audio Components types.


701
00:34:28,126 --> 00:34:32,406
The two main categories
of types are Audio Units


702
00:34:32,406 --> 00:34:33,716
and Audio Codecs.


703
00:34:34,346 --> 00:34:37,065
But amongst the Audio Units,
we have input/output units,


704
00:34:37,096 --> 00:34:38,806
generators, effects,
instruments,


705
00:34:39,235 --> 00:34:41,916
converters, mixers as well.


706
00:34:42,315 --> 00:34:45,275
And amongst codecs, we
have encoders and decoders.


707
00:34:45,565 --> 00:34:48,206
We also have audio file
components on macOS.


708
00:34:48,835 --> 00:34:55,136
Getting into the
implementation of components,


709
00:34:55,996 --> 00:34:57,606
there are a number
of different ways


710
00:34:57,706 --> 00:34:59,266
that components are implemented.


711
00:34:59,266 --> 00:35:01,736
Some of them you'll need to know
about if you're writing them.


712
00:35:02,146 --> 00:35:03,576
And others, it's
just for background.


713
00:35:04,726 --> 00:35:08,966
The most highly-recommended
way to create a component now


714
00:35:08,966 --> 00:35:10,156
if it's an Audio Unit is


715
00:35:10,156 --> 00:35:12,616
to create an Audio Unit
application extension.


716
00:35:13,246 --> 00:35:19,116
We introduced this last year
with our 10.11 and 9.0 releases.


717
00:35:20,186 --> 00:35:21,466
So those are app extensions.


718
00:35:21,516 --> 00:35:25,566
Before that, Audio Units were
packaged in component bundles --


719
00:35:25,566 --> 00:35:27,176
as were audio codecs, et cetera.


720
00:35:28,906 --> 00:35:31,026
That goes back to
Mac OS 10.1 or so.


721
00:35:33,336 --> 00:35:34,336
Interestingly enough,


722
00:35:34,336 --> 00:35:38,916
audio components also include
inter-app audio nodes on iOS.


723
00:35:39,376 --> 00:35:41,746
Node applications
register themselves


724
00:35:42,086 --> 00:35:45,756
with a component subtype
and manufacturer key.


725
00:35:46,416 --> 00:35:49,646
And host applications
discover node applications


726
00:35:50,236 --> 00:35:51,876
through the Audio
Component Manager.


727
00:35:53,696 --> 00:35:56,326
And finally, you can register
-- as I mentioned before --


728
00:35:56,326 --> 00:35:58,556
you can register your own
components for the use


729
00:35:58,556 --> 00:35:59,656
of your own application.


730
00:36:00,506 --> 00:36:01,546
And just for completeness,


731
00:36:01,586 --> 00:36:03,876
there are some Apple
built-in components.


732
00:36:04,296 --> 00:36:06,966
On iOS, they're linked
into the AudioToolbox.


733
00:36:07,726 --> 00:36:12,376
So those are the flavors of
component implementations.


734
00:36:12,806 --> 00:36:16,336
Now I'd like to focus in on just
one kind of component here --


735
00:36:16,726 --> 00:36:18,516
the audio input/output unit.


736
00:36:18,746 --> 00:36:19,826
This is and Audio Unit.


737
00:36:21,886 --> 00:36:25,786
And it's probably the one
component that you'll use


738
00:36:25,786 --> 00:36:27,206
if you don't use any other.


739
00:36:28,086 --> 00:36:31,396
And the reason is that this
is the preferred interface


740
00:36:31,446 --> 00:36:34,496
to the system's basic
audio input/output path.


741
00:36:35,336 --> 00:36:39,826
Now, on macOS, that basic path
is in the Core Audio framework.


742
00:36:40,346 --> 00:36:41,576
We call it the Audio HAL,


743
00:36:42,366 --> 00:36:44,176
and it's a pretty
low-level interface.


744
00:36:44,606 --> 00:36:48,986
It makes its clients deal with
interesting stream typologies


745
00:36:48,986 --> 00:36:50,986
on multichannel devices
for example.


746
00:36:52,026 --> 00:36:56,266
So, it's much easier to deal
with the Audio HAL interface


747
00:36:56,626 --> 00:36:58,546
through an audio
input/output unit.


748
00:37:00,066 --> 00:37:02,416
On iOS, you don't
even have access


749
00:37:02,476 --> 00:37:04,316
to the Core Audio framework.


750
00:37:04,526 --> 00:37:05,556
It's not public there.


751
00:37:05,926 --> 00:37:07,986
You have to use an
audio input/output unit


752
00:37:08,686 --> 00:37:13,276
as your lowest-level way to get
audio in and out of the system.


753
00:37:15,336 --> 00:37:17,506
And our preferred interface now


754
00:37:18,376 --> 00:37:21,626
for audio input/output
units is AUAudioUnit


755
00:37:21,626 --> 00:37:23,266
and the AudioToolbox framework.


756
00:37:24,146 --> 00:37:26,936
If you've been working
with our APIs for a while,


757
00:37:27,306 --> 00:37:31,246
you're familiar with version
2 Audio Units that are part


758
00:37:31,246 --> 00:37:36,956
of the system AUHAL on the macOS
and AURemoteIO on iOS as well


759
00:37:36,956 --> 00:37:41,566
as Watch -- actually, I'm not
sure we have it available there.


760
00:37:41,566 --> 00:37:46,296
But in any case, AUAudioUnit
is your new modern interface


761
00:37:46,536 --> 00:37:48,916
to this low-level I/O mechanism.


762
00:37:50,136 --> 00:37:53,026
So I'd like to show
you what it looks


763
00:37:53,026 --> 00:37:58,806
like to use AUAudioUnit
to do AudioIO.


764
00:37:59,346 --> 00:38:02,026
So I've written a simple
program in Swift here


765
00:38:02,816 --> 00:38:04,356
that generates a square wave.


766
00:38:05,386 --> 00:38:08,336
And here's my signal processing.


767
00:38:08,586 --> 00:38:10,716
I mentioned earlier I
would show you what kinds


768
00:38:10,716 --> 00:38:11,996
of things you can do here.


769
00:38:12,876 --> 00:38:16,226
So this wave generator
shows you.


770
00:38:16,676 --> 00:38:21,146
You can basically read memory,
write memory, and do math.


771
00:38:21,876 --> 00:38:24,286
And that's all that's
going on here.


772
00:38:24,286 --> 00:38:27,616
It's making the simplest of all
wave forms -- the square wave --


773
00:38:27,616 --> 00:38:30,376
at least simplest from a
computational point of view.


774
00:38:31,836 --> 00:38:34,286
So that class is called
SquareWaveGenerator.


775
00:38:35,526 --> 00:38:38,176
And let's see how to play
a SqaureWaveGenerator


776
00:38:38,306 --> 00:38:39,806
from an AUAudioUnit.


777
00:38:41,706 --> 00:38:43,806
So the first thing we
do is create an audio


778
00:38:43,806 --> 00:38:45,076
component description.


779
00:38:46,076 --> 00:38:49,316
And this tells us which
component to go look for.


780
00:38:50,086 --> 00:38:51,286
The type is output.


781
00:38:51,446 --> 00:38:55,116
The subtype is something I chose
here depending on platform --


782
00:38:55,666 --> 00:38:57,436
either RemoteIO or HalOutput.


783
00:38:58,126 --> 00:39:01,546
We've got the Apple manufacturer
and some unused flags.


784
00:39:02,886 --> 00:39:06,916
Then I can create my AUAudioUnit
using my component description.


785
00:39:08,036 --> 00:39:09,786
So I'll get that
unit that I wanted.


786
00:39:11,376 --> 00:39:15,406
And now it's open and I
can start to configure it.


787
00:39:16,566 --> 00:39:19,376
So the first thing I
want to do here is find


788
00:39:19,376 --> 00:39:23,236
out how many channels of
audio are on the system.


789
00:39:23,236 --> 00:39:27,776
There are ways to do this
with AVAudioSession on iOS.


790
00:39:28,406 --> 00:39:32,336
But most simply and portably,


791
00:39:32,416 --> 00:39:36,476
you can simply query
the outputBusses


792
00:39:36,796 --> 00:39:39,316
of the input/output unit.


793
00:39:39,766 --> 00:39:45,006
And outputBus[0] is the
output-directed stream.


794
00:39:45,616 --> 00:39:46,986
So I'm going to fetch
its format,


795
00:39:46,986 --> 00:39:48,466
and that's my hardware format.


796
00:39:48,996 --> 00:39:52,716
Now this hardware format
may be something exotic.


797
00:39:52,716 --> 00:39:55,346
It may be inertly for example.


798
00:39:55,346 --> 00:39:58,456
And I don't know that I
want to deal with that.


799
00:39:58,456 --> 00:40:01,476
So I'm just going to
create a renderFormat.


800
00:40:01,796 --> 00:40:04,736
That is a standard format
with the same sample rate.


801
00:40:05,446 --> 00:40:06,816
And some number of channels.


802
00:40:07,296 --> 00:40:11,596
Just to keep things short
and simple, I'm only going


803
00:40:11,596 --> 00:40:13,386
to render two channels,


804
00:40:13,386 --> 00:40:15,436
regardless of the
hardware channel count.


805
00:40:16,136 --> 00:40:17,816
So that's my renderFormat.


806
00:40:18,386 --> 00:40:22,646
Now, I can tell the I/O unit,
"This is the format I want


807
00:40:22,646 --> 00:40:24,536
to give you on inputBus[0]."


808
00:40:24,536 --> 00:40:29,976
So, having done this, the unit
will now convert my renderFormat


809
00:40:30,186 --> 00:40:31,346
to the hardwareFormat.


810
00:40:31,946 --> 00:40:34,006
And in this case, on my MacBook,


811
00:40:34,006 --> 00:40:37,806
it's going to take this
deinterleaved floating point


812
00:40:38,426 --> 00:40:41,756
and convert it to interleaved
floating point buffers.


813
00:40:43,636 --> 00:40:45,686
OK. So, next, I'm going


814
00:40:45,686 --> 00:40:47,436
to create my square
wave generators.


815
00:40:48,276 --> 00:40:50,906
If you're a music and
math geek like me,


816
00:40:51,566 --> 00:40:54,836
you know that A440 is
there, and multiplying it


817
00:40:54,836 --> 00:40:58,206
by 1.5 will give you
a fifth above it.


818
00:40:59,676 --> 00:41:02,106
So I'm going to render
A to my left channel


819
00:41:02,106 --> 00:41:04,076
and E to my right channel.


820
00:41:05,436 --> 00:41:08,796
And here's the code that will
run in the real-time context.


821
00:41:10,406 --> 00:41:13,746
There's a lot of
parameters here,


822
00:41:13,996 --> 00:41:16,176
and I actually only
need a couple of them.


823
00:41:16,176 --> 00:41:20,096
I only need the frameCount
and the rawBufferList.


824
00:41:20,966 --> 00:41:25,416
The rawBufferList is a
difficult, low-level C structure


825
00:41:25,916 --> 00:41:32,446
which I can rewrap in Swift
using an overlay on the SDK.


826
00:41:33,006 --> 00:41:35,306
And this takes the
audio bufferList


827
00:41:35,966 --> 00:41:39,306
and makes it look something
like a vector or array.


828
00:41:40,816 --> 00:41:42,646
So having converted
the rawBufferList


829
00:41:42,646 --> 00:41:45,916
to the nice Swift wrapper,
I can query its count.


830
00:41:46,596 --> 00:41:48,636
And if I got at least
one buffer,


831
00:41:48,636 --> 00:41:50,316
then I can render
the left channel.


832
00:41:50,866 --> 00:41:53,566
If I got at least two buffers,
I can render the right channel.


833
00:41:54,326 --> 00:41:56,826
And that's all the work
I'm doing right here.


834
00:41:56,826 --> 00:41:59,206
Of course, there's more work
inside the wave generators,


835
00:41:59,486 --> 00:42:01,476
but that's all of the
real-time context work.


836
00:42:02,696 --> 00:42:03,986
So, now, I'm all setup.


837
00:42:04,056 --> 00:42:05,586
I'm ready to render.


838
00:42:05,996 --> 00:42:08,826
So I'm going to tell
the I/O unit,


839
00:42:08,826 --> 00:42:11,516
"Do any allocations you need
to do to start rendering."


840
00:42:12,116 --> 00:42:14,006
Then, I can have it
actually start the hardware,


841
00:42:14,776 --> 00:42:16,666
run for 3 seconds, and stop.


842
00:42:16,666 --> 00:42:18,236
And that's the end of
this simple program.


843
00:42:19,516 --> 00:42:22,666
[ Monotone ]


844
00:42:23,166 --> 00:42:24,656
So, that's AUAudioUnit.


845
00:42:26,066 --> 00:42:30,406
I'd like to turn next briefly to
some other kinds of Audio Units.


846
00:42:31,016 --> 00:42:34,996
We have effects which take audio
input, produce audio output.


847
00:42:35,436 --> 00:42:38,616
Instruments which take something
resembling MIDI as input


848
00:42:39,026 --> 00:42:40,606
and also produce audio output.


849
00:42:41,176 --> 00:42:44,486
And generators which
produce audio output


850
00:42:44,616 --> 00:42:48,566
without anything going in except
maybe some parametric control.


851
00:42:48,876 --> 00:42:52,866
If I were to repackage my square
wave generator as an Audio Unit,


852
00:42:52,976 --> 00:42:54,176
I would make it a generator.


853
00:42:54,906 --> 00:43:00,296
So to host these
kinds of Audio Units,


854
00:43:00,356 --> 00:43:02,446
you can also use AUAudioUnit.


855
00:43:03,566 --> 00:43:07,326
You can use a separate block
to provide input to it.


856
00:43:07,566 --> 00:43:10,186
It's very similar to the
output provider block


857
00:43:10,186 --> 00:43:12,696
that you saw on the I/O unit.


858
00:43:13,646 --> 00:43:16,126
You can chain together
these render blocks of units


859
00:43:16,196 --> 00:43:18,226
to create your own
custom typologies.


860
00:43:18,936 --> 00:43:21,876
You can control the units
using their parameters.


861
00:43:22,956 --> 00:43:26,266
And also, many units,
especially third-party units,


862
00:43:26,856 --> 00:43:28,576
have nice user interfaces.


863
00:43:28,576 --> 00:43:31,076
As a hosting application,
you can obtain


864
00:43:31,076 --> 00:43:34,776
that audio unit's view,
display it in your application,


865
00:43:34,836 --> 00:43:38,826
and let the user
interact with it.


866
00:43:39,766 --> 00:43:41,846
Now if you'd like to
write your own Audio Unit,


867
00:43:43,966 --> 00:43:46,656
the way I would start
is just building it


868
00:43:46,656 --> 00:43:48,386
within the context of an app.


869
00:43:48,676 --> 00:43:50,446
This lets you debug
without worrying


870
00:43:50,446 --> 00:43:53,436
about inter-process
communication issues.


871
00:43:53,436 --> 00:43:54,486
It's all in one process.


872
00:43:54,576 --> 00:43:57,456
So, you start by
subclassing AUAudioUnit.


873
00:43:58,036 --> 00:44:01,506
You register it as a component
using this class method


874
00:44:01,506 --> 00:44:02,226
of AUAudioUnit.


875
00:44:02,936 --> 00:44:03,886
Then, you can debug it.


876
00:44:05,256 --> 00:44:06,756
And once you've done that --


877
00:44:06,866 --> 00:44:09,086
and if you decide you'd
like to distribute it


878
00:44:09,166 --> 00:44:11,236
as an Audio Unit extension --


879
00:44:11,616 --> 00:44:14,706
you can take that same
AUAudioUnit subclass.


880
00:44:15,396 --> 00:44:17,466
You might fine-tune and
polish it some more.


881
00:44:18,286 --> 00:44:20,986
But then you have to do a
small amount of additional work


882
00:44:21,336 --> 00:44:23,716
to package this as an
Audio Unit extension.


883
00:44:24,206 --> 00:44:25,586
So you've got an extension.


884
00:44:25,586 --> 00:44:27,236
You can embed it
in an application.


885
00:44:27,486 --> 00:44:32,726
You can sell that
application on the App Store.


886
00:44:33,696 --> 00:44:35,796
So I'd like to have
my colleague, Torrey,


887
00:44:35,796 --> 00:44:39,316
now show you some of the power
of Audio Unit extensions.


888
00:44:39,686 --> 00:44:41,736
We've had some developers
doing some really cool things


889
00:44:41,736 --> 00:44:42,656
with it in the last year.


890
00:44:43,676 --> 00:44:44,446
>> How is everybody doing?


891
00:44:45,526 --> 00:44:46,726
Happy to be at WWDC?


892
00:44:47,516 --> 00:44:50,596
[ Applause ]


893
00:44:51,096 --> 00:44:52,416
Let's make some noise.


894
00:44:52,716 --> 00:44:55,256
I'm going to start here by
launching -- well, first of all,


895
00:44:55,256 --> 00:44:56,376
I have my instrument here.


896
00:44:56,676 --> 00:44:58,116
This is my iPad Pro.


897
00:44:58,116 --> 00:45:00,526
And I'm going to start by
launching Arturia iSEM --


898
00:45:01,306 --> 00:45:03,216
a very powerful synthesizer
application.


899
00:45:03,216 --> 00:45:05,956
And I have a synth trumpet
sound here that I like.


900
00:45:06,516 --> 00:45:10,546
[ Music ]


901
00:45:11,046 --> 00:45:13,806
So I like this sound
and I want to put it


902
00:45:13,806 --> 00:45:15,126
in a track that I'm working on.


903
00:45:15,426 --> 00:45:18,996
This is going to serve as our
Audio Unit plug-in application.


904
00:45:19,176 --> 00:45:21,976
And now I'm going to launch
GarageBand, which is going


905
00:45:21,976 --> 00:45:24,496
to serve as our Audio
Unit host application.


906
00:45:24,496 --> 00:45:28,466
Now, in GarageBand, I have a
sick beat I've been working


907
00:45:28,466 --> 00:45:31,516
on that I'm calling WWDC Demo.


908
00:45:31,786 --> 00:45:32,976
Let's listen to it.


909
00:45:33,516 --> 00:45:43,046
[ Music ]


910
00:45:43,546 --> 00:45:45,976
Well move into what I call
"the verse portion" next.


911
00:45:46,516 --> 00:45:54,556
[ Music ]


912
00:45:55,056 --> 00:45:57,996
And next, we're going to
work on this chorus here.


913
00:45:57,996 --> 00:45:59,986
This is supposed to be
the climax of the song.


914
00:45:59,986 --> 00:46:01,166
I want some motion.


915
00:46:01,166 --> 00:46:02,006
I want some tension.


916
00:46:02,006 --> 00:46:05,526
And let's create that by
bringing in an Audio Unit.


917
00:46:06,426 --> 00:46:07,966
I'm going to add
a new track here.


918
00:46:09,036 --> 00:46:12,056
Adding an instrument, I'll see
Audio Units is an option here.


919
00:46:12,946 --> 00:46:15,336
If I select this, then I can
see all of the Audio Units


920
00:46:15,336 --> 00:46:16,556
that are hosted here
on the system.


921
00:46:17,226 --> 00:46:21,456
Right now, I see Arturia iSEM
because I practice this at home.


922
00:46:22,776 --> 00:46:25,226
Selecting iSEM, GarageBand
is now going


923
00:46:25,226 --> 00:46:28,376
to give me an onscreen MIDI
controller that I can use here.


924
00:46:28,886 --> 00:46:32,716
It's complete with the scale
transforms and arpeggiator here


925
00:46:32,716 --> 00:46:35,306
that I'm going to make use of
because I like a lot of motion.


926
00:46:35,366 --> 00:46:39,256
Over here on the left, you
can see a Pitch/Mod Wheel.


927
00:46:39,306 --> 00:46:40,946
You can even modify
the velocity.


928
00:46:41,326 --> 00:46:44,146
And here is the view that the
Audio Unit has provided to me


929
00:46:44,146 --> 00:46:45,316
that I can actually tweak.


930
00:46:45,866 --> 00:46:48,486
For now, I'm going to record
in a little piece here


931
00:46:48,486 --> 00:46:50,546
and see what it sounds
like in context.


932
00:46:50,676 --> 00:46:51,236
So --


933
00:46:52,516 --> 00:47:03,686dle
[ Music ]


934
00:46:52,516 --> 00:47:03,686
[ Music ]


935
00:47:04,186 --> 00:47:05,076
All right, pretty good.


936
00:47:05,076 --> 00:47:08,976
Let's see what it
sounds like in context.


937
00:47:09,516 --> 00:47:13,636
[ Music ]


938
00:47:14,136 --> 00:47:14,996
There we go.


939
00:47:14,996 --> 00:47:17,006
That's the tension that I want.


940
00:47:17,006 --> 00:47:19,376
Now, let me dig in
here a little bit more


941
00:47:19,376 --> 00:47:22,366
and show you what I've done.


942
00:47:22,566 --> 00:47:23,776
I'm going to edit here.


943
00:47:23,856 --> 00:47:28,146
And I'll look into this
loop a little bit more.


944
00:47:28,146 --> 00:47:31,066
There are two observations
that I'd like you to make here.


945
00:47:31,166 --> 00:47:33,726
The first one is that
these are MIDI events.


946
00:47:34,116 --> 00:47:36,606
The difference between
using inter-app audio


947
00:47:36,606 --> 00:47:38,086
and using Audio Units


948
00:47:38,086 --> 00:47:40,566
as a plug-in is you'll
actually get MIDI notes here,


949
00:47:40,566 --> 00:47:42,636
which is much easier
to edit after the fact.


950
00:47:43,046 --> 00:47:45,826
The other observation I'd
like you to make here is


951
00:47:45,826 --> 00:47:47,956
that you see these
individual MIDI notes here


952
00:47:48,016 --> 00:47:50,926
but you saw me play one
big, fat-fingered chord.


953
00:47:51,406 --> 00:47:53,386
So, it's because
I've taken advantage


954
00:47:53,386 --> 00:47:55,626
of the arpeggiator that's
built into GarageBand


955
00:47:55,626 --> 00:47:56,996
that I've got these
individual notes.


956
00:47:57,356 --> 00:47:59,146
And I can play around
with these if I want to


957
00:47:59,146 --> 00:48:00,946
and make them sound
a bit more human.


958
00:48:01,356 --> 00:48:03,926
But I'm happy with this
recording as it is.


959
00:48:04,496 --> 00:48:08,186
The last thing that I'd actually
like to show you here is, first,


960
00:48:08,186 --> 00:48:12,046
I'm going to copy this
into the adjacent cell.


961
00:48:13,806 --> 00:48:19,476
And I told you earlier that the
Audio Unit view that's provided


962
00:48:19,476 --> 00:48:20,496
here is actually interactive.


963
00:48:20,496 --> 00:48:21,556
It's not just a pretty picture.


964
00:48:21,886 --> 00:48:24,546
So if you were adventurous,
you could even try


965
00:48:24,546 --> 00:48:26,856
to give a little
performance for your friends.


966
00:48:27,516 --> 00:48:31,566
[ Music ]


967
00:48:32,066 --> 00:48:32,786
Turn it up a little bit.


968
00:48:33,516 --> 00:49:32,546
[ Music ]


969
00:49:33,046 --> 00:49:33,686
Let's wrap it up.


970
00:49:34,516 --> 00:50:00,586
[ Music ]


971
00:50:01,086 --> 00:50:01,976
That concludes my demo.


972
00:50:02,516 --> 00:50:04,786
[ Applause ]


973
00:50:05,286 --> 00:50:07,796
I want to thank you for
your time, your attention,


974
00:50:07,796 --> 00:50:09,976
and always for making dope apps.


975
00:50:10,516 --> 00:50:14,706
[ Applause ]


976
00:50:15,206 --> 00:50:16,296
>> Thank you, Torrey.


977
00:50:16,366 --> 00:50:22,696
So, just to recap here, you can
see the session we did last year


978
00:50:22,696 --> 00:50:24,146
about Audio Unit extensions.


979
00:50:24,146 --> 00:50:27,696
It goes into a lot more detail
about the mechanics of the API.


980
00:50:28,146 --> 00:50:30,386
We just wanted to show you here
what people have been doing


981
00:50:30,386 --> 00:50:33,016
with it because it's so cool.


982
00:50:33,726 --> 00:50:38,306
So, speaking of MIDI, we saw
how GarageBand recorded Torrey's


983
00:50:38,306 --> 00:50:39,416
performance as MIDI.


984
00:50:40,666 --> 00:50:42,936
We have a number of
APIs in the system


985
00:50:42,936 --> 00:50:46,266
that communicate using MIDI,
and it's not always clear


986
00:50:46,266 --> 00:50:48,246
which ones to use when.


987
00:50:48,936 --> 00:50:53,166
So I'd like to try to help
clear that up just a little bit.


988
00:50:53,736 --> 00:50:56,706
Now, you might just have a
standard MIDI file like --


989
00:50:57,566 --> 00:51:00,276
well, an ugly cellphone
ringtone.


990
00:51:00,396 --> 00:51:03,606
But MIDI files are very
useful in music education.


991
00:51:03,796 --> 00:51:07,916
I can get a MIDI file of
a piece I want to learn.


992
00:51:07,916 --> 00:51:09,226
I can see what all
the notes are.


993
00:51:10,056 --> 00:51:12,126
So if you have a MIDI
file, you can play it back


994
00:51:12,126 --> 00:51:13,556
with AVAudioSequencer.


995
00:51:13,556 --> 00:51:17,206
And that will play it back into
the context of an AVAudioEngine.


996
00:51:17,316 --> 00:51:21,786
If you wish to control
a software synthesizer


997
00:51:22,246 --> 00:51:26,576
as we saw GarageBand doing
with iSEM, the best API to do


998
00:51:26,576 --> 00:51:28,516
that with is AUAudioUnit.


999
00:51:29,536 --> 00:51:32,046
And if you'd like your
AUAudioUnit to play back


1000
00:51:32,046 --> 00:51:35,546
into your AVAudioEngine, you
can use AVAudioMIDIInstrument.


1001
00:51:36,106 --> 00:51:40,686
Now there's the core
MIDI framework


1002
00:51:40,686 --> 00:51:42,226
which people often
think does some


1003
00:51:42,226 --> 00:51:44,546
of these other higher-level
things.


1004
00:51:44,636 --> 00:51:48,506
But it's actually a very
low-level API that's basically


1005
00:51:48,506 --> 00:51:50,546
for communicating
with MIDI hardware --


1006
00:51:50,546 --> 00:51:53,706
for example, an external
USB MIDI interface


1007
00:51:54,136 --> 00:51:55,706
or a Bluetooth MIDI keyboard.


1008
00:51:56,256 --> 00:51:58,196
We also supply a
MIDI network driver.


1009
00:51:58,996 --> 00:52:03,226
You can use that to send raw
MIDI messages between an iPad


1010
00:52:03,226 --> 00:52:04,716
and a MacBook for example.


1011
00:52:06,316 --> 00:52:09,056
You can also use the core
MIDI framework to send MIDI


1012
00:52:09,056 --> 00:52:10,926
between processes in real time.


1013
00:52:12,106 --> 00:52:15,786
Now this gets into a
gray area sometimes.


1014
00:52:15,786 --> 00:52:19,086
People wonder, "Well, should
I use core MIDI to communicate


1015
00:52:19,086 --> 00:52:23,146
between my sequencer and
my app that's listening


1016
00:52:23,146 --> 00:52:25,226
to MIDI and synthesizing?"


1017
00:52:25,786 --> 00:52:29,126
And I would say that's probably
not the right API for that case.


1018
00:52:29,126 --> 00:52:31,276
If you're using MIDI
and audio together,


1019
00:52:31,776 --> 00:52:33,496
I would use AUAudioUnit.


1020
00:52:33,936 --> 00:52:36,136
It's in the case where
you're doing pure MIDI


1021
00:52:36,136 --> 00:52:39,606
in two applications
or two entities


1022
00:52:39,606 --> 00:52:42,646
within an application --
maybe one is a static library


1023
00:52:42,646 --> 00:52:43,766
from another developer.


1024
00:52:44,706 --> 00:52:48,676
In those situations, you can
use core MIDI for inter-process


1025
00:52:48,846 --> 00:52:50,826
or inter-entity real-time MIDI.


1026
00:52:52,596 --> 00:52:53,886
So that takes us to the end


1027
00:52:53,886 --> 00:52:56,386
of our grand tour
of the audio APIs.


1028
00:52:57,326 --> 00:53:00,096
We started with applications
-- and at the bottom,


1029
00:53:00,096 --> 00:53:03,546
the CoreAudio framework
and drivers.


1030
00:53:04,456 --> 00:53:07,746
We looked at AVAudioEngine,
how you use AVAudioSession


1031
00:53:07,746 --> 00:53:11,326
to get things setup on all of
our platforms except macOS.


1032
00:53:12,036 --> 00:53:14,166
We saw how you can
use AVAudioPlayer


1033
00:53:14,166 --> 00:53:16,676
and the AVAudioRecorder
for simple playback


1034
00:53:16,676 --> 00:53:18,066
and recording from files.


1035
00:53:18,776 --> 00:53:21,806
Or if your files or network
streams involve video,


1036
00:53:21,806 --> 00:53:23,066
you can use AVPlayer.


1037
00:53:24,056 --> 00:53:27,056
AVAudioEngine is a very
good, high-level interface


1038
00:53:27,426 --> 00:53:30,176
for building complex
processing graphs


1039
00:53:30,516 --> 00:53:32,596
and will solve a
lot of problems.


1040
00:53:32,596 --> 00:53:36,216
You usually won't have to use
any of the lower-level APIs.


1041
00:53:36,216 --> 00:53:41,586
But if you do, we saw how in
AudioToolbox there's AUAudioUnit


1042
00:53:41,586 --> 00:53:45,156
that lets you communicate
directly with the I/O cycle


1043
00:53:46,256 --> 00:53:49,476
and third-party, or
your own instruments,


1044
00:53:49,476 --> 00:53:50,916
effects, and generators.


1045
00:53:51,336 --> 00:53:54,286
And finally, we took a quick
look at the core MIDI framework.


1046
00:53:54,286 --> 00:53:58,446
So that's the end
of my talk here.


1047
00:53:58,576 --> 00:54:01,936
You can visit this link
for some more information.


1048
00:54:02,696 --> 00:54:04,776
We have a number of
related sessions here.


1049
00:54:05,306 --> 00:54:05,976
Thank you very much.


1050
00:54:06,516 --> 00:54:09,500
[ Applause ]

