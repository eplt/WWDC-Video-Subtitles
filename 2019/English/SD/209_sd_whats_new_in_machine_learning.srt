1
00:00:01,516 --> 00:00:04,500
[ Music ]


2
00:00:08,516 --> 00:00:17,546
[ Applause ]


3
00:00:18,046 --> 00:00:19,896
>> Hello. Welcome everyone.


4
00:00:20,656 --> 00:00:22,786
My name is Gaurav and today


5
00:00:22,786 --> 00:00:24,306
we're going to talk about What's


6
00:00:24,396 --> 00:00:27,456
New in Machine Learning.


7
00:00:29,696 --> 00:00:31,316
Machine Learning if used by


8
00:00:31,406 --> 00:00:32,326
thousands of apps.


9
00:00:33,356 --> 00:00:35,556
The apps that you are making are


10
00:00:35,556 --> 00:00:36,436
amazing.


11
00:00:37,006 --> 00:00:38,896
They're touching every aspect of


12
00:00:38,946 --> 00:00:39,846
a user's life.


13
00:00:40,456 --> 00:00:45,086
In hospitals, doctors are using


14
00:00:45,086 --> 00:00:47,326
apps such as Butterfly iQ to do


15
00:00:47,326 --> 00:00:50,356
medical diagnostics in real


16
00:00:52,096 --> 00:00:52,206
time.


17
00:00:52,416 --> 00:00:54,866
In sports, coaches are using


18
00:00:54,866 --> 00:00:57,186
apps such as HomeCourt to train


19
00:00:57,186 --> 00:00:57,646
their players.


20
00:00:58,576 --> 00:01:02,896
In creativity, apps such as


21
00:01:02,896 --> 00:01:04,936
Pixelmator Pro are helping other


22
00:01:04,936 --> 00:01:06,366
users to augment their


23
00:01:06,366 --> 00:01:07,686
creativity using ML.


24
00:01:08,746 --> 00:01:10,366
These are just a few examples,


25
00:01:10,576 --> 00:01:12,446
and we would like all of these


26
00:01:12,446 --> 00:01:14,746
apps you guys should also make


27
00:01:14,746 --> 00:01:17,396
these kind of apps.


28
00:01:17,656 --> 00:01:19,446
So now the question is what's


29
00:01:19,486 --> 00:01:19,686
new?


30
00:01:20,556 --> 00:01:24,736
Let me begin by saying a lot.


31
00:01:25,576 --> 00:01:28,896
We have so much material that we


32
00:01:28,896 --> 00:01:30,746
can hardly cover it one session


33
00:01:30,746 --> 00:01:31,556
or two sessions.


34
00:01:32,126 --> 00:01:35,746
We need ten sessions to cover


35
00:01:35,746 --> 00:01:36,856
the entire material.


36
00:01:38,316 --> 00:01:39,606
We also have a session


37
00:01:39,776 --> 00:01:41,616
intersecting the design in


38
00:01:41,616 --> 00:01:43,216
machine learning.


39
00:01:44,556 --> 00:01:47,216
We also have Daily Labs where


40
00:01:47,216 --> 00:01:49,496
you can meet and discuss your


41
00:01:49,496 --> 00:01:51,006
ideas with Apple Machine


42
00:01:51,006 --> 00:01:51,746
Learning engineers.


43
00:01:53,116 --> 00:01:55,156
All of us are here to remove any


44
00:01:55,156 --> 00:01:56,956
roadblocks that you might be


45
00:01:56,956 --> 00:01:58,966
facing while integrating Machine


46
00:01:59,026 --> 00:02:00,326
Learning in your app.


47
00:02:02,876 --> 00:02:04,776
In all of these sessions, you


48
00:02:04,776 --> 00:02:06,366
will see that we follow simple


49
00:02:06,366 --> 00:02:07,016
principles.


50
00:02:08,356 --> 00:02:11,246
We want Machine Learning to be


51
00:02:11,246 --> 00:02:12,976
as easy to use as possible.


52
00:02:12,976 --> 00:02:14,336
We are laser focused on doing


53
00:02:15,006 --> 00:02:15,106
that.


54
00:02:15,736 --> 00:02:17,766
We want to make it flexible so


55
00:02:17,766 --> 00:02:20,466
you can do it by variety of


56
00:02:21,436 --> 00:02:22,926
tasks, and we want to make it


57
00:02:22,976 --> 00:02:25,506
powerful so you can run state of


58
00:02:25,506 --> 00:02:27,246
the art Machine Learning models


59
00:02:27,336 --> 00:02:28,136
on your devices.


60
00:02:28,876 --> 00:02:32,976
It is truly Machine Learning for


61
00:02:32,976 --> 00:02:35,036
everyone whether you are a


62
00:02:35,036 --> 00:02:36,606
researcher or someone new to


63
00:02:36,606 --> 00:02:37,286
Machine Learning.


64
00:02:38,776 --> 00:02:40,046
With create in a lab you can


65
00:02:40,046 --> 00:02:41,236
make state of the art,


66
00:02:41,236 --> 00:02:42,816
task-focused ML models.


67
00:02:44,526 --> 00:02:45,846
The next big pillar of our


68
00:02:45,846 --> 00:02:47,286
offering is Domain APIs.


69
00:02:49,256 --> 00:02:50,746
Domain APIs allow you to


70
00:02:50,746 --> 00:02:52,106
leverage Apple's built-in


71
00:02:52,106 --> 00:02:53,346
intelligence and models.


72
00:02:54,266 --> 00:02:55,506
So you don't have to worry about


73
00:02:55,506 --> 00:02:56,916
collecting the data and building


74
00:02:56,916 --> 00:02:57,426
the model.


75
00:02:57,646 --> 00:03:00,296
Simple call the API and be done.


76
00:03:02,696 --> 00:03:04,326
This year we are significantly


77
00:03:04,356 --> 00:03:05,806
expanding our Domain APIs.


78
00:03:06,256 --> 00:03:08,676
We have Domain APIs in Vision,


79
00:03:09,106 --> 00:03:13,196
Text, Speech and Sound.


80
00:03:13,406 --> 00:03:15,226
Now let's take a sneak peek of


81
00:03:15,276 --> 00:03:16,386
some of these APIs.


82
00:03:17,146 --> 00:03:19,746
Let's start with Vision.


83
00:03:20,976 --> 00:03:22,906
Vision allows you to reason


84
00:03:23,016 --> 00:03:24,716
about the content of the image.


85
00:03:26,206 --> 00:03:27,386
One of the new features this


86
00:03:27,386 --> 00:03:29,036
year is Image Saliency.


87
00:03:30,096 --> 00:03:31,576
Image Saliency can help you


88
00:03:31,576 --> 00:03:33,536
identify the most relevant


89
00:03:33,536 --> 00:03:35,256
region in your image.


90
00:03:36,106 --> 00:03:39,466
In this case, the region around


91
00:03:39,636 --> 00:03:40,146
the person.


92
00:03:41,936 --> 00:03:43,056
You can use it to generate


93
00:03:43,056 --> 00:03:44,996
thumbnails or to make image


94
00:03:44,996 --> 00:03:49,196
cropping, generate memories,


95
00:03:49,196 --> 00:03:49,976
guide camera et cetera.


96
00:03:50,046 --> 00:03:53,786
Another big feature we are


97
00:03:53,786 --> 00:03:55,406
introducing this year is Text


98
00:03:55,466 --> 00:03:56,126
Recognition.


99
00:03:57,666 --> 00:03:59,516
You can now take a picture of


100
00:03:59,516 --> 00:04:01,476
the document, perform


101
00:04:01,476 --> 00:04:03,556
perspective correction, lighting


102
00:04:03,556 --> 00:04:05,566
correction and reorganize the


103
00:04:05,616 --> 00:04:06,976
text on the device.


104
00:04:07,516 --> 00:04:13,986
[ Applause ]


105
00:04:14,486 --> 00:04:18,106
This is huge, but that's not


106
00:04:18,106 --> 00:04:18,666
all.


107
00:04:18,666 --> 00:04:19,796
We have Image inbuilt


108
00:04:19,796 --> 00:04:22,016
classifier, human detector, pet


109
00:04:22,016 --> 00:04:24,036
detector, and we are going to


110
00:04:24,036 --> 00:04:26,166
cover them in detail in our two


111
00:04:26,166 --> 00:04:27,046
Vision sessions.


112
00:04:31,036 --> 00:04:33,106
Next domain is Natural Language.


113
00:04:33,656 --> 00:04:36,356
Just like you use Vision to


114
00:04:36,356 --> 00:04:38,376
reason about images you can use


115
00:04:38,376 --> 00:04:41,046
Natural Language to reason about


116
00:04:42,056 --> 00:04:43,076
the text.


117
00:04:43,256 --> 00:04:44,906
New this year is inbuilt


118
00:04:44,906 --> 00:04:46,166
Sentiment Analysis.


119
00:04:46,166 --> 00:04:48,626
So you can use to analyze the


120
00:04:48,686 --> 00:04:50,616
sentiment of the text in real


121
00:04:50,616 --> 00:04:52,636
time on the device in a privacy


122
00:04:52,636 --> 00:04:53,216
friendly way.


123
00:04:53,836 --> 00:04:55,296
So, for example, if somebody


124
00:04:55,296 --> 00:04:56,486
types something like this I was


125
00:04:56,566 --> 00:04:57,926
so excited about the season


126
00:04:57,926 --> 00:04:59,406
finale that's a positive


127
00:05:00,256 --> 00:05:02,656
sentiment, but it was a bit


128
00:05:02,656 --> 00:05:04,386
disappointing at the end it's a


129
00:05:04,386 --> 00:05:05,406
negative sentiment.


130
00:05:06,056 --> 00:05:08,656
So you can provide this kind of


131
00:05:08,656 --> 00:05:10,006
feedback in real time.


132
00:05:10,526 --> 00:05:14,516
For the first time we are also


133
00:05:14,516 --> 00:05:16,826
exposing inbuilt Word


134
00:05:16,826 --> 00:05:17,386
Embeddings.


135
00:05:18,606 --> 00:05:20,096
Word Embeddings can allow you to


136
00:05:20,166 --> 00:05:21,666
find semantically similar words.


137
00:05:21,666 --> 00:05:23,426
So, for example, the word


138
00:05:23,426 --> 00:05:25,246
thunderstorm is very close to


139
00:05:25,246 --> 00:05:27,496
cloudy but it is very far away


140
00:05:27,496 --> 00:05:28,406
from shoes and boots.


141
00:05:28,896 --> 00:05:30,486
One of the big use case of Word


142
00:05:30,486 --> 00:05:31,486
Embeddings is semantic search


143
00:05:31,486 --> 00:05:33,636
and we are going to give you an


144
00:05:34,246 --> 00:05:35,246
example soon.


145
00:05:35,456 --> 00:05:36,546
Natural Language will be


146
00:05:36,546 --> 00:05:38,446
discussed in detail in Advances


147
00:05:38,446 --> 00:05:39,666
in Natural Language Framework


148
00:05:39,716 --> 00:05:40,086
session.


149
00:05:40,636 --> 00:05:45,836
The third way user interacts


150
00:05:45,836 --> 00:05:47,476
with your app is through speech


151
00:05:47,826 --> 00:05:48,426
and sound.


152
00:05:49,566 --> 00:05:51,076
Now we have an on-device speech


153
00:05:51,106 --> 00:05:52,496
support so you can transcribe


154
00:05:52,546 --> 00:05:54,986
the text on the device so you no


155
00:05:54,986 --> 00:05:56,226
longer have to rely on the


156
00:05:56,226 --> 00:05:59,266
network connection, and we also


157
00:05:59,266 --> 00:06:01,086
have new Voice Analytics API


158
00:06:01,436 --> 00:06:03,066
that can tell you not only what


159
00:06:03,066 --> 00:06:05,926
is spoken but how it is spoken.


160
00:06:05,926 --> 00:06:07,066
So you can differentiate between


161
00:06:07,066 --> 00:06:08,486
a normal voice and a high


162
00:06:08,576 --> 00:06:09,986
jittery voice.


163
00:06:11,016 --> 00:06:12,916
We also have a brand-new Sound


164
00:06:12,916 --> 00:06:14,396
Analysis Framework, that we will


165
00:06:14,396 --> 00:06:15,936
discuss in Create ML session.


166
00:06:16,416 --> 00:06:21,496
There's a lot more in each of


167
00:06:21,496 --> 00:06:24,146
these domains and another thing


168
00:06:24,146 --> 00:06:25,606
you can do to combine these


169
00:06:25,606 --> 00:06:27,396
domains almost seamlessly.


170
00:06:28,216 --> 00:06:29,246
Let me show you an example.


171
00:06:30,026 --> 00:06:33,176
Let's just say you want to build


172
00:06:33,176 --> 00:06:34,616
a feature that does Semantic


173
00:06:34,616 --> 00:06:35,566
Search on Images.


174
00:06:35,676 --> 00:06:36,896
That's a very complex feature.


175
00:06:36,896 --> 00:06:38,746
So if a user searches for


176
00:06:38,746 --> 00:06:41,056
thunderstorm, you want to


177
00:06:41,056 --> 00:06:42,366
provide them the results not


178
00:06:42,366 --> 00:06:43,886
only for thunderstorm but also


179
00:06:44,026 --> 00:06:46,716
for sky and cloudy.


180
00:06:46,836 --> 00:06:49,756
Now, you can combine Vision with


181
00:06:49,756 --> 00:06:51,336
Natural Language to implement


182
00:06:51,336 --> 00:06:53,376
this feature in very few lines


183
00:06:53,376 --> 00:06:53,806
of code.


184
00:06:55,176 --> 00:06:56,206
This is how you will do it.


185
00:06:56,206 --> 00:06:58,146
You will run your Image


186
00:06:58,146 --> 00:07:02,056
Classifier on images or you can


187
00:07:02,056 --> 00:07:03,606
have them use inbuilt Image


188
00:07:03,606 --> 00:07:05,286
Classifier to generate the tags.


189
00:07:06,016 --> 00:07:08,886
When the user types the word


190
00:07:08,936 --> 00:07:10,356
something like thunderstorm, you


191
00:07:10,356 --> 00:07:11,926
can use Word Embeddings to


192
00:07:11,926 --> 00:07:15,226
generate similar words and find


193
00:07:15,226 --> 00:07:16,576
the images that matches these


194
00:07:16,646 --> 00:07:16,896
tags.


195
00:07:17,626 --> 00:07:21,896
And that's not all.


196
00:07:21,896 --> 00:07:23,756
You can also combine the custom


197
00:07:23,756 --> 00:07:24,936
models that you made using


198
00:07:24,936 --> 00:07:26,746
Create ML with Domain APIs, and


199
00:07:27,036 --> 00:07:28,646
we are going to show an example


200
00:07:28,646 --> 00:07:30,686
of that in our Creating Great


201
00:07:30,686 --> 00:07:32,496
Apps Using Core ML and ARKit


202
00:07:32,606 --> 00:07:32,996
session.


203
00:07:33,546 --> 00:07:37,426
So to summarize, Domain APIs


204
00:07:37,426 --> 00:07:38,766
allow you to leverage Apple's


205
00:07:38,896 --> 00:07:40,576
built-in intelligence and model


206
00:07:40,856 --> 00:07:42,586
using API so you don't have to


207
00:07:42,586 --> 00:07:43,856
collect the data and make the


208
00:07:43,856 --> 00:07:44,266
models.


209
00:07:44,736 --> 00:07:45,776
And this year we have a


210
00:07:45,866 --> 00:07:48,046
significant expansion in our


211
00:07:48,046 --> 00:07:49,276
Vision Natural Language and


212
00:07:49,276 --> 00:07:49,976
Speech and Sound APIs.


213
00:07:56,336 --> 00:07:58,176
Now let's talk about Core ML 3,


214
00:07:58,526 --> 00:08:00,136
the third big pillar of our


215
00:08:00,846 --> 00:08:01,016
offering.


216
00:08:03,536 --> 00:08:06,106
Core ML is now supported across


217
00:08:06,196 --> 00:08:10,156
all our platforms.


218
00:08:10,216 --> 00:08:11,396
All the work is done on the


219
00:08:11,396 --> 00:08:13,046
device so user's privacy is


220
00:08:13,076 --> 00:08:13,936
maintained.


221
00:08:14,716 --> 00:08:16,686
Core ML is hardware accelerated


222
00:08:16,686 --> 00:08:18,466
so you can do, you can use it


223
00:08:18,466 --> 00:08:19,726
for realtime Machine Learning


224
00:08:20,646 --> 00:08:22,586
and you don't need a server and


225
00:08:22,586 --> 00:08:23,656
it always available.


226
00:08:24,356 --> 00:08:29,666
Core ML has always supported a


227
00:08:29,666 --> 00:08:31,056
wide variety of Machine Learning


228
00:08:31,056 --> 00:08:33,006
models ranging from classical


229
00:08:33,006 --> 00:08:34,566
generalized linear models, tree


230
00:08:34,566 --> 00:08:35,976
ensembles and support vector


231
00:08:35,976 --> 00:08:38,275
machines as well as neural


232
00:08:38,275 --> 00:08:40,395
networks such as Convolution


233
00:08:40,395 --> 00:08:41,946
Neural Network and Recurrent


234
00:08:41,946 --> 00:08:43,155
Neural Networks.


235
00:08:46,236 --> 00:08:48,596
New in Core ML is model


236
00:08:48,596 --> 00:08:50,146
flexibility and model


237
00:08:50,146 --> 00:08:51,046
personalization.


238
00:08:51,626 --> 00:08:53,196
So let's take a look at both of


239
00:08:53,926 --> 00:08:54,016
them.


240
00:08:55,956 --> 00:08:58,226
Core ML has expanded support for


241
00:08:58,226 --> 00:09:01,036
Data Neural Networks and we have


242
00:09:01,036 --> 00:09:03,686
added a support for more than


243
00:09:03,686 --> 00:09:06,106
100+ Neural Network layers.


244
00:09:07,356 --> 00:09:09,046
This means that you can bring


245
00:09:09,046 --> 00:09:11,186
almost, you can bring the most


246
00:09:11,186 --> 00:09:13,456
cutting-edge Machine Learning


247
00:09:13,456 --> 00:09:15,486
models into your app such as


248
00:09:15,486 --> 00:09:16,926
ELMo, BERT, Wavenet.


249
00:09:17,816 --> 00:09:18,586
What does that mean?


250
00:09:20,136 --> 00:09:21,716
Let's just say you have an app


251
00:09:21,716 --> 00:09:22,906
and you want to integrate a


252
00:09:22,906 --> 00:09:24,056
state of the art Question and


253
00:09:24,056 --> 00:09:26,476
Answer system in your app so


254
00:09:26,476 --> 00:09:28,156
that when a user asks a question


255
00:09:28,156 --> 00:09:29,556
how many sessions will there be


256
00:09:29,556 --> 00:09:30,606
at WWDC this year?


257
00:09:31,196 --> 00:09:32,896
You can do it by using BERT


258
00:09:32,966 --> 00:09:33,336
model.


259
00:09:34,536 --> 00:09:35,796
So the model can analyze the


260
00:09:35,796 --> 00:09:38,116
statement and gives feedback the


261
00:09:38,146 --> 00:09:43,956
result over 100 is the answer.


262
00:09:43,956 --> 00:09:45,286
Besides Natural Language you can


263
00:09:45,286 --> 00:09:47,756
also run the latest advances in


264
00:09:47,756 --> 00:09:49,166
Vision such as Instance


265
00:09:49,166 --> 00:09:51,666
Segmentation as well as latest


266
00:09:51,666 --> 00:09:53,256
advances in Audio Generation.


267
00:09:56,876 --> 00:09:58,606
And to take full advantage of


268
00:09:58,606 --> 00:10:00,216
this expanded support of Core


269
00:10:00,266 --> 00:10:03,146
ML, we are updating our


270
00:10:03,146 --> 00:10:03,556
converters.


271
00:10:03,556 --> 00:10:04,986
So we will have a brand new


272
00:10:04,986 --> 00:10:06,276
TensorFlow Core ML converter and


273
00:10:06,276 --> 00:10:08,676
ONNX support ML converter will


274
00:10:08,676 --> 00:10:09,566
be coming soon.


275
00:10:10,176 --> 00:10:14,756
We are also updating our Model


276
00:10:14,756 --> 00:10:16,326
Gallery importing some of these


277
00:10:16,366 --> 00:10:18,726
research models on our Model


278
00:10:18,726 --> 00:10:20,596
Gallery so you can start using


279
00:10:20,596 --> 00:10:21,566
them immediately.


280
00:10:22,686 --> 00:10:24,466
So Core ML 3 with this new model


281
00:10:24,466 --> 00:10:26,016
representation and embedded


282
00:10:26,056 --> 00:10:27,666
converters with Model Gallery


283
00:10:28,196 --> 00:10:29,526
can help you bring the cutting


284
00:10:29,526 --> 00:10:31,316
edge ML research into your app.


285
00:10:32,216 --> 00:10:33,476
Now a big feature, Model


286
00:10:33,476 --> 00:10:34,456
Personalization.


287
00:10:35,146 --> 00:10:36,836
So let me explain what it is.


288
00:10:37,336 --> 00:10:39,086
So up to now you have seen, you


289
00:10:39,086 --> 00:10:41,106
have used Create ML to build the


290
00:10:41,106 --> 00:10:44,006
models and Core ML to deploy the


291
00:10:44,006 --> 00:10:47,616
models, but new in Core ML 3 is


292
00:10:47,846 --> 00:10:49,806
On-Device Model Personalization.


293
00:10:50,266 --> 00:10:52,726
You can fine tune the model on


294
00:10:52,726 --> 00:10:53,236
the device.


295
00:10:53,966 --> 00:10:58,196
And we use this kind of


296
00:10:58,196 --> 00:11:00,466
technology in Face ID and


297
00:11:00,466 --> 00:11:01,276
setting Watch Face.


298
00:11:02,026 --> 00:11:05,536
So this is how it happens.


299
00:11:05,756 --> 00:11:08,506
Today you have data, you make an


300
00:11:08,566 --> 00:11:11,526
ML model, you ship it to your


301
00:11:11,736 --> 00:11:13,776
app and all of your users


302
00:11:13,776 --> 00:11:15,136
download the same model.


303
00:11:15,706 --> 00:11:19,716
And this is great because now


304
00:11:20,196 --> 00:11:21,596
users don't have to upload their


305
00:11:21,596 --> 00:11:21,986
portals.


306
00:11:22,346 --> 00:11:24,166
You can directly take photos,


307
00:11:24,646 --> 00:11:25,956
run through the model on the


308
00:11:25,956 --> 00:11:28,466
device and get inference such as


309
00:11:31,516 --> 00:11:31,616
dog.


310
00:11:31,836 --> 00:11:34,126
But what if you are trying to


311
00:11:34,126 --> 00:11:35,496
deal with a concept that is


312
00:11:35,496 --> 00:11:36,526
unique to each user?


313
00:11:37,096 --> 00:11:38,826
Say the concept of My Dog.


314
00:11:39,696 --> 00:11:41,006
You might want users to find


315
00:11:41,006 --> 00:11:42,866
pictures of their own dog in the


316
00:11:42,916 --> 00:11:44,456
photo library and not all dog


317
00:11:44,456 --> 00:11:47,216
photos they've taken.


318
00:11:47,336 --> 00:11:48,696
And each user's dog looks


319
00:11:48,696 --> 00:11:49,156
different.


320
00:11:49,156 --> 00:11:50,426
For example, someone may have


321
00:11:50,486 --> 00:11:51,706
Golden Retriever, someone may


322
00:11:52,196 --> 00:11:54,386
have this Bulldog, ah, and this


323
00:11:54,386 --> 00:11:56,976
is my crazy dog.


324
00:11:58,976 --> 00:12:01,546
So, how do we do that?


325
00:12:02,746 --> 00:12:05,106
So what we want is for user 1 we


326
00:12:05,106 --> 00:12:06,936
need image classifier that takes


327
00:12:06,936 --> 00:12:08,806
this dog and says it's their


328
00:12:08,806 --> 00:12:09,076
dog.


329
00:12:10,286 --> 00:12:12,326
For user 2, that's the English


330
00:12:12,326 --> 00:12:14,906
Bulldog and this the third dog.


331
00:12:18,576 --> 00:12:21,286
So in order to do that, one


332
00:12:21,286 --> 00:12:23,196
approach would be to use Server


333
00:12:23,196 --> 00:12:23,886
Based Approach.


334
00:12:23,916 --> 00:12:26,126
So you may ask each of your


335
00:12:26,636 --> 00:12:30,366
users to upload the photos in


336
00:12:31,836 --> 00:12:32,476
the Cloud.


337
00:12:32,476 --> 00:12:34,496
Server generates model for each


338
00:12:34,496 --> 00:12:37,526
of the users and then sends it


339
00:12:39,056 --> 00:12:39,466
back.


340
00:12:39,466 --> 00:12:41,826
Unfortunately, this approach has


341
00:12:41,826 --> 00:12:42,606
privacy concerns.


342
00:12:42,606 --> 00:12:45,006
Your user may not be very happy


343
00:12:45,266 --> 00:12:47,236
uploading their photo to your


344
00:12:47,236 --> 00:12:48,496
server and you may also not be


345
00:12:48,496 --> 00:12:49,516
okay taking their photo.


346
00:12:50,606 --> 00:12:51,866
You have to setup the server so


347
00:12:51,916 --> 00:12:53,296
there is some cost involved.


348
00:12:53,846 --> 00:12:56,346
Let us say you have a million


349
00:12:56,346 --> 00:12:58,036
users, which we sincerely hope


350
00:12:58,036 --> 00:12:59,396
you do, you have to make


351
00:12:59,396 --> 00:13:00,776
millions such models and keep


352
00:13:00,836 --> 00:13:02,966
track of them over time.


353
00:13:05,176 --> 00:13:07,356
With Core ML 3 you can do more


354
00:13:07,356 --> 00:13:09,186
personalization on the device.


355
00:13:09,396 --> 00:13:11,776
So if you have a training data


356
00:13:11,776 --> 00:13:13,886
on the device, you can just


357
00:13:13,886 --> 00:13:15,906
simply use it to fine tune the


358
00:13:15,906 --> 00:13:17,146
model on the device.


359
00:13:17,946 --> 00:13:23,366
So previously you take LMH and


360
00:13:23,366 --> 00:13:27,156
you do the inference and now you


361
00:13:27,156 --> 00:13:28,936
can take the label data feedback


362
00:13:28,936 --> 00:13:30,396
from the user, provide some kind


363
00:13:30,396 --> 00:13:32,386
of training data and fine tune


364
00:13:32,386 --> 00:13:34,276
the model on the device.


365
00:13:35,516 --> 00:13:42,226
[ Applause ]


366
00:13:42,726 --> 00:13:44,256
You have a personalized model


367
00:13:44,406 --> 00:13:46,006
for each user.


368
00:13:48,176 --> 00:13:50,826
We respect user's privacy and


369
00:13:50,826 --> 00:13:52,696
you don't need to put server.


370
00:13:53,386 --> 00:13:56,386
So Core ML 3 supports On-Device


371
00:13:56,386 --> 00:13:58,116
Personalization for Neural


372
00:13:58,936 --> 00:13:59,326
Networks.


373
00:13:59,786 --> 00:14:01,146
We're also supporting Nearest


374
00:14:01,276 --> 00:14:03,346
Neighbor and you can do it in


375
00:14:03,346 --> 00:14:05,246
the background at night.


376
00:14:08,536 --> 00:14:10,276
So to summarize and end our


377
00:14:10,356 --> 00:14:13,046
session we saw Create ML a brand


378
00:14:13,046 --> 00:14:16,006
new app to build ML models, we


379
00:14:16,006 --> 00:14:17,556
have a significant expansion in


380
00:14:17,556 --> 00:14:21,126
our Domain APIs and Core ML is


381
00:14:21,196 --> 00:14:22,586
much more flexible and now


382
00:14:22,586 --> 00:14:23,706
supports On-Device


383
00:14:23,706 --> 00:14:24,666
Personalization.


384
00:14:25,206 --> 00:14:29,016
You can find more information on


385
00:14:29,016 --> 00:14:30,796
our Developer Website Session


386
00:14:30,796 --> 00:14:33,966
209 and we hope you are as


387
00:14:33,996 --> 00:14:35,636
excited about these technologies


388
00:14:35,636 --> 00:14:36,176
as we are.


389
00:14:36,566 --> 00:14:37,876
I look forward to seeing you in


390
00:14:37,876 --> 00:14:38,226
the labs.


391
00:14:38,366 --> 00:14:38,706
Thank you.


392
00:14:39,516 --> 00:14:43,500
[ Applause ]

