1
00:00:07,516 --> 00:00:17,500
[ Music ]


2
00:00:22,516 --> 00:00:26,086
[ Applause ]


3
00:00:26,586 --> 00:00:27,746
>> So, good morning.


4
00:00:29,806 --> 00:00:31,406
Welcome to our session on What's


5
00:00:31,476 --> 00:00:34,516
New in ARKit 2.


6
00:00:34,806 --> 00:00:36,736
My name is Arsalan, and I am an


7
00:00:36,736 --> 00:00:41,276
engineer from ARKit team.


8
00:00:41,616 --> 00:00:42,906
Last year, we were really


9
00:00:42,906 --> 00:00:45,536
excited to give ARKit in your


10
00:00:45,536 --> 00:00:48,976
hands as part of iOS 11 update.


11
00:00:49,386 --> 00:00:53,496
ARKit has been deployed to


12
00:00:53,496 --> 00:00:55,266
hundreds of millions of devices,


13
00:00:56,086 --> 00:00:58,296
making iOS the biggest and most


14
00:00:58,296 --> 00:00:59,816
advanced AR platform.


15
00:01:03,226 --> 00:01:05,206
ARKit gives you simple to use


16
00:01:05,266 --> 00:01:06,986
interface for powerful set of


17
00:01:07,016 --> 00:01:07,476
features.


18
00:01:10,136 --> 00:01:12,416
We have been truly amazed by


19
00:01:12,416 --> 00:01:14,396
what you have created with ARKit


20
00:01:14,396 --> 00:01:15,286
so far.


21
00:01:15,486 --> 00:01:17,456
So let's see some examples from


22
00:01:17,456 --> 00:01:17,876
App Store.


23
00:01:20,386 --> 00:01:23,156
Civilisations is an AR app that


24
00:01:23,156 --> 00:01:25,066
brings historical artifacts in


25
00:01:25,066 --> 00:01:25,536
front of you.


26
00:01:26,296 --> 00:01:27,966
You can view them from every


27
00:01:27,966 --> 00:01:28,326
angle.


28
00:01:28,816 --> 00:01:32,206
You can also enable x-ray modes


29
00:01:32,436 --> 00:01:33,746
to have mode interaction.


30
00:01:35,206 --> 00:01:36,906
You can bring them in your


31
00:01:37,086 --> 00:01:40,226
backyard, and you can even bring


32
00:01:40,226 --> 00:01:42,336
them to life exactly they look


33
00:01:42,336 --> 00:01:43,596
like hundreds of years ago.


34
00:01:45,096 --> 00:01:46,416
So this is a great tool to


35
00:01:46,416 --> 00:01:48,016
browse historical artifacts.


36
00:01:50,356 --> 00:01:52,376
Boulevard AR is an all right app


37
00:01:52,516 --> 00:01:55,346
that lets you browse the work of


38
00:01:55,346 --> 00:01:57,446
arts in a way that's never been


39
00:01:57,446 --> 00:01:58,176
possible before.


40
00:01:58,796 --> 00:02:00,486
You can put them on ground or


41
00:02:00,486 --> 00:02:02,736
wall, and you can really go


42
00:02:02,736 --> 00:02:04,456
close to them, and you can see


43
00:02:04,456 --> 00:02:04,996
all the details.


44
00:02:04,996 --> 00:02:07,756
It's just a great way to tell


45
00:02:07,756 --> 00:02:09,386
you story of arts.


46
00:02:16,316 --> 00:02:18,216
ARKit is a fun way to educate


47
00:02:18,216 --> 00:02:19,326
everyone.


48
00:02:19,976 --> 00:02:21,936
Free reverse is an app that


49
00:02:22,126 --> 00:02:24,246
places immersive landscape in


50
00:02:24,246 --> 00:02:24,726
front of you.


51
00:02:25,586 --> 00:02:27,336
You can follow a flow of a river


52
00:02:27,336 --> 00:02:29,786
going through landscape and see


53
00:02:30,046 --> 00:02:31,276
communities and wild life.


54
00:02:32,146 --> 00:02:34,006
You can see how human activity


55
00:02:35,006 --> 00:02:37,506
impacts those communities and


56
00:02:37,836 --> 00:02:40,056
wildlife by constructions.


57
00:02:42,386 --> 00:02:43,966
So it's a great way to educate


58
00:02:43,966 --> 00:02:45,726
everyone about keeping the


59
00:02:45,726 --> 00:02:48,046
environment green and through


60
00:02:48,046 --> 00:02:48,966
sustainable development.


61
00:02:49,836 --> 00:02:51,016
So those were some of the


62
00:02:51,016 --> 00:02:51,936
examples.


63
00:02:52,246 --> 00:02:54,516
Do check a lot more examples


64
00:02:54,626 --> 00:02:57,546
from App Store.


65
00:02:57,886 --> 00:02:59,926
So some of you are new to ARKit,


66
00:03:00,446 --> 00:03:01,666
so let me give you a quick


67
00:03:02,016 --> 00:03:04,286
overview of what ARKit is.


68
00:03:08,336 --> 00:03:10,376
Tracking is the core component


69
00:03:10,376 --> 00:03:10,806
of ARKit.


70
00:03:11,586 --> 00:03:12,886
It gives you position and


71
00:03:12,886 --> 00:03:14,556
orientation of your device in


72
00:03:14,556 --> 00:03:15,326
physical world.


73
00:03:15,796 --> 00:03:22,216
It can also track objects such


74
00:03:22,216 --> 00:03:23,126
as human faces.


75
00:03:23,796 --> 00:03:29,046
Scene understanding enhances


76
00:03:29,106 --> 00:03:30,066
tracking by learning more


77
00:03:30,066 --> 00:03:31,036
attributes about the


78
00:03:31,036 --> 00:03:31,596
environment.


79
00:03:32,256 --> 00:03:35,666
So we can detect horizontal


80
00:03:35,666 --> 00:03:37,976
planes such as ground planes or


81
00:03:37,976 --> 00:03:39,006
tabletops.


82
00:03:39,736 --> 00:03:41,116
We can also detect vertical


83
00:03:41,116 --> 00:03:41,466
planes.


84
00:03:41,836 --> 00:03:43,646
So this lets you place your


85
00:03:43,646 --> 00:03:45,466
virtual objects in the scene.


86
00:03:51,366 --> 00:03:53,576
Scene understanding also learns


87
00:03:53,806 --> 00:03:56,096
about lighting conditions in the


88
00:03:56,096 --> 00:03:56,636
environment.


89
00:03:57,736 --> 00:04:00,386
So you can use lighting to


90
00:04:00,386 --> 00:04:02,156
accurately reflect the real


91
00:04:02,156 --> 00:04:03,566
environment in your virtual


92
00:04:03,646 --> 00:04:03,906
scene.


93
00:04:04,216 --> 00:04:06,736
So your objects don't look too


94
00:04:06,736 --> 00:04:07,746
bright or too dark.


95
00:04:09,736 --> 00:04:11,276
Rendering is what actually a


96
00:04:11,276 --> 00:04:12,986
user sees on the device and


97
00:04:12,986 --> 00:04:15,076
interacts with the augmented


98
00:04:15,076 --> 00:04:15,986
reality scene.


99
00:04:16,416 --> 00:04:18,926
So ARKit makes it very easy for


100
00:04:18,926 --> 00:04:21,366
you to integrate any rendering


101
00:04:21,366 --> 00:04:22,125
engine of your choice.


102
00:04:25,936 --> 00:04:29,166
ARKit offers built-in views for


103
00:04:29,336 --> 00:04:30,716
SceneKit and SpriteKit.


104
00:04:31,196 --> 00:04:35,106
In Xcode, we also have a


105
00:04:35,106 --> 00:04:36,606
Metal template for you to


106
00:04:36,606 --> 00:04:38,246
quickly get started with your


107
00:04:38,246 --> 00:04:39,666
own augmented reality


108
00:04:41,326 --> 00:04:41,706
experience.


109
00:04:41,706 --> 00:04:43,936
Note also that Unity and Unreal


110
00:04:43,936 --> 00:04:46,116
have integrated full feature set


111
00:04:46,116 --> 00:04:48,016
of ARKit into their popular


112
00:04:48,016 --> 00:04:48,606
gaming engines.


113
00:04:49,006 --> 00:04:51,006
So you have all these rendering


114
00:04:51,006 --> 00:04:53,286
technologies available to you to


115
00:04:53,286 --> 00:04:54,866
get started with ARKit.


116
00:04:58,696 --> 00:05:00,416
So let's see, what is new this


117
00:05:00,446 --> 00:05:02,316
year in ARKit 2.


118
00:05:07,836 --> 00:05:09,436
Okay. So we have saving and


119
00:05:09,436 --> 00:05:11,566
loading maps that enables


120
00:05:11,566 --> 00:05:13,156
powerful new features of


121
00:05:13,356 --> 00:05:15,426
persistence and multiuser


122
00:05:15,426 --> 00:05:16,206
experiences.


123
00:05:16,796 --> 00:05:21,026
We are also giving you


124
00:05:21,246 --> 00:05:22,906
environment texturing so you can


125
00:05:22,906 --> 00:05:24,246
realistically render your


126
00:05:24,246 --> 00:05:25,526
augmented reality scene.


127
00:05:25,986 --> 00:05:30,376
ARKit can now track 2D images in


128
00:05:30,376 --> 00:05:31,516
real-time.


129
00:05:33,116 --> 00:05:34,836
We are not limited to 2D.


130
00:05:35,076 --> 00:05:37,466
We can also detect 3D objects in


131
00:05:38,046 --> 00:05:41,316
a scene.


132
00:05:41,536 --> 00:05:43,146
And last, we have some fun


133
00:05:43,146 --> 00:05:44,756
enhancements for face tracking.


134
00:05:47,916 --> 00:05:50,136
So let's start with saving and


135
00:05:50,136 --> 00:05:51,696
loading maps.


136
00:05:56,316 --> 00:05:58,616
Saving and loading maps is part


137
00:05:58,616 --> 00:05:59,656
of world tracking.


138
00:06:00,446 --> 00:06:01,576
World tracking gives you


139
00:06:01,576 --> 00:06:03,296
position and orientation of your


140
00:06:03,296 --> 00:06:05,616
device as our six degrees of


141
00:06:05,616 --> 00:06:07,666
freedom pose in real world.


142
00:06:08,156 --> 00:06:11,476
This lets you place objects in


143
00:06:11,476 --> 00:06:14,456
the scene such as this table and


144
00:06:14,456 --> 00:06:16,226
chair you can see in this video.


145
00:06:16,736 --> 00:06:20,066
World tracking also gives you


146
00:06:20,066 --> 00:06:21,906
accurate physical scale so you


147
00:06:21,906 --> 00:06:24,946
can place your objects up to the


148
00:06:24,946 --> 00:06:25,566
correct scale.


149
00:06:26,186 --> 00:06:28,006
So your objects don't look too


150
00:06:28,006 --> 00:06:28,976
big or too small.


151
00:06:34,086 --> 00:06:36,126
This can also be used to


152
00:06:36,126 --> 00:06:38,696
implement accurate measurements,


153
00:06:39,406 --> 00:06:40,666
such as the Measure app we saw


154
00:06:40,666 --> 00:06:41,126
yesterday.


155
00:06:41,696 --> 00:06:46,296
World tracking also gives you 3D


156
00:06:46,296 --> 00:06:48,016
feature points so you can, you


157
00:06:48,096 --> 00:06:50,446
can, you know some physical


158
00:06:50,446 --> 00:06:51,636
structure of the environment,


159
00:06:52,116 --> 00:06:54,076
and this can be used to perform


160
00:06:54,076 --> 00:06:56,486
hit testing to place objects in


161
00:06:56,996 --> 00:07:00,236
the scene.


162
00:07:00,446 --> 00:07:02,886
In iOS 11.3, we introduced


163
00:07:02,886 --> 00:07:03,876
relocalization.


164
00:07:04,776 --> 00:07:07,226
This feature lets you restore


165
00:07:07,226 --> 00:07:08,816
your tracking state after AR


166
00:07:09,526 --> 00:07:11,436
session was interrupted.


167
00:07:11,946 --> 00:07:13,806
So this could happen, for


168
00:07:13,806 --> 00:07:14,766
example, your app is


169
00:07:14,866 --> 00:07:17,186
backgrounded or you're using


170
00:07:17,236 --> 00:07:23,886
picture in picture mode on iPad.


171
00:07:24,086 --> 00:07:29,326
So relocalization works with a


172
00:07:29,326 --> 00:07:32,086
map that is continuously built


173
00:07:32,086 --> 00:07:32,966
by world tracking.


174
00:07:34,066 --> 00:07:35,966
So the more we move around the


175
00:07:35,966 --> 00:07:39,566
environment, the more it is able


176
00:07:39,566 --> 00:07:41,116
to extend and learn about


177
00:07:41,286 --> 00:07:42,126
different features of the


178
00:07:42,126 --> 00:07:43,216
environment.


179
00:07:48,186 --> 00:07:51,536
So this map was only available


180
00:07:51,656 --> 00:07:53,336
as long as your AR session was


181
00:07:53,336 --> 00:07:56,306
alive, but not we are giving


182
00:07:56,306 --> 00:08:00,026
this map available to you.


183
00:08:00,256 --> 00:08:03,066
In ARKit API, this map is given


184
00:08:03,066 --> 00:08:05,396
to you as ARWorldMap object.


185
00:08:14,426 --> 00:08:16,406
ARWorldMap represents mapping of


186
00:08:16,446 --> 00:08:19,346
physical 3D space, similar to


187
00:08:19,346 --> 00:08:22,036
what we see in this, on the


188
00:08:22,036 --> 00:08:25,676
visual, on the right.


189
00:08:25,886 --> 00:08:27,436
We also know that anchors are


190
00:08:27,436 --> 00:08:29,046
important points in physical


191
00:08:29,076 --> 00:08:29,376
space.


192
00:08:30,066 --> 00:08:31,286
So these are the places where


193
00:08:31,286 --> 00:08:33,306
you want to place your virtual


194
00:08:33,306 --> 00:08:33,666
objects.


195
00:08:34,576 --> 00:08:36,535
So we have also included plain


196
00:08:36,535 --> 00:08:38,566
anchors by default in


197
00:08:38,566 --> 00:08:39,746
ARWorldMap.


198
00:08:40,716 --> 00:08:42,895
Moreover, you can also add your


199
00:08:42,895 --> 00:08:44,646
custom anchors to this list


200
00:08:45,276 --> 00:08:46,546
since it is mutable list.


201
00:08:46,546 --> 00:08:48,866
So you can create your custom


202
00:08:48,866 --> 00:08:50,016
anchors in the scene and add


203
00:08:50,016 --> 00:08:51,726
them to World Map.


204
00:08:57,546 --> 00:08:59,236
For your visualization and


205
00:08:59,236 --> 00:09:01,776
debugging, World Map also give


206
00:09:01,776 --> 00:09:03,596
you raw feature points and


207
00:09:03,596 --> 00:09:06,116
extend, so you know the real


208
00:09:06,216 --> 00:09:12,856
physical space you just scanned.


209
00:09:12,856 --> 00:09:15,636
More importantly, World Map is a


210
00:09:15,636 --> 00:09:18,056
serializable object, so it can


211
00:09:18,056 --> 00:09:20,356
be serialized to any data stream


212
00:09:20,356 --> 00:09:22,546
of your choice, such as file on


213
00:09:22,546 --> 00:09:25,756
local system or to a shared


214
00:09:25,806 --> 00:09:27,126
network place.


215
00:09:28,336 --> 00:09:33,636
So this ARWorldMap object enable


216
00:09:33,636 --> 00:09:34,976
two powerful set of new


217
00:09:34,976 --> 00:09:37,116
experiences in ARKit.


218
00:09:37,116 --> 00:09:41,376
The first is persistence.


219
00:09:44,076 --> 00:09:46,526
So just to show you an example


220
00:09:46,526 --> 00:09:49,816
how it works, we have a user


221
00:09:49,816 --> 00:09:52,376
starting world tracking, and he


222
00:09:52,426 --> 00:09:53,956
places an object in the scene


223
00:09:54,626 --> 00:09:56,126
through ARKit hit testing.


224
00:09:56,646 --> 00:10:00,956
And before leaves the scene, he


225
00:10:00,956 --> 00:10:03,896
will save World Map on the


226
00:10:04,436 --> 00:10:04,686
device.


227
00:10:09,046 --> 00:10:12,066
So some point, sometime later,


228
00:10:12,066 --> 00:10:15,866
the user comes back, and he is


229
00:10:15,866 --> 00:10:17,846
able to load the same World Map,


230
00:10:17,846 --> 00:10:20,686
and he will find the same


231
00:10:20,726 --> 00:10:22,086
augmented reality experience.


232
00:10:22,696 --> 00:10:24,616
So he can repeat this experience


233
00:10:24,616 --> 00:10:26,946
as many times he wants, and he


234
00:10:27,026 --> 00:10:28,546
will find these objects on the


235
00:10:28,586 --> 00:10:30,386
table every time he will start


236
00:10:30,386 --> 00:10:31,006
his experience.


237
00:10:31,566 --> 00:10:34,376
So this is persistence in world


238
00:10:34,376 --> 00:10:34,716
tracking


239
00:10:36,016 --> 00:10:37,626
[applause].


240
00:10:37,626 --> 00:10:37,956
Thank you.


241
00:10:38,508 --> 00:10:40,508
[applause]


242
00:10:44,346 --> 00:10:45,946
ARWorldMap also enables


243
00:10:45,946 --> 00:10:47,506
multiuser experiences.


244
00:10:49,336 --> 00:10:51,016
Now your augmented reality


245
00:10:51,016 --> 00:10:52,236
experience is not limited to a


246
00:10:52,306 --> 00:10:53,616
single device or single user.


247
00:10:54,936 --> 00:10:56,956
It can be shared with many


248
00:10:56,956 --> 00:10:57,426
users.


249
00:10:59,996 --> 00:11:03,766
One user can create World Map


250
00:11:03,826 --> 00:11:05,986
and share with one or more


251
00:11:06,186 --> 00:11:06,716
users.


252
00:11:07,226 --> 00:11:10,866
Note that World Map represents a


253
00:11:10,916 --> 00:11:13,876
single coordinate system in real


254
00:11:13,876 --> 00:11:14,176
world.


255
00:11:15,126 --> 00:11:17,226
So what it means that every user


256
00:11:17,606 --> 00:11:18,946
will share the same working


257
00:11:19,006 --> 00:11:19,356
space.


258
00:11:20,426 --> 00:11:22,326
They are able to experience the


259
00:11:22,396 --> 00:11:23,506
same augmented reality


260
00:11:23,506 --> 00:11:25,416
experience from different point


261
00:11:25,936 --> 00:11:27,436
of view.


262
00:11:27,676 --> 00:11:29,106
So this is a great new feature.


263
00:11:29,716 --> 00:11:33,346
You can use World Map to enable


264
00:11:33,346 --> 00:11:36,476
multiuser games, such as the one


265
00:11:36,476 --> 00:11:37,406
we saw yesterday.


266
00:11:37,976 --> 00:11:42,756
We can also use ARWorldMap to


267
00:11:42,756 --> 00:11:44,426
create multiuser shared


268
00:11:44,426 --> 00:11:45,626
educational experiences.


269
00:11:49,856 --> 00:11:51,536
Note that we are giving


270
00:11:51,536 --> 00:11:53,366
ARWorldMap object in your hands,


271
00:11:53,906 --> 00:11:55,856
so you are free to choose any


272
00:11:56,636 --> 00:12:00,076
technology to share with every


273
00:12:00,076 --> 00:12:00,436
user.


274
00:12:02,156 --> 00:12:04,336
For example, for sharing you can


275
00:12:04,406 --> 00:12:06,706
use air drop or multipeer


276
00:12:06,706 --> 00:12:08,336
connectivity that relies on


277
00:12:08,336 --> 00:12:10,136
local Bluetooth or WiFi


278
00:12:10,136 --> 00:12:10,636
connection.


279
00:12:10,636 --> 00:12:13,186
So it means that you don't


280
00:12:13,186 --> 00:12:14,296
really need an internet


281
00:12:14,296 --> 00:12:16,226
connection for this feature to


282
00:12:16,746 --> 00:12:16,846
work.


283
00:12:22,416 --> 00:12:24,226
:15 So let's see how ARKit API


284
00:12:24,226 --> 00:12:25,856
makes it very easy for you to


285
00:12:26,406 --> 00:12:28,606
retrieve and load World Map.


286
00:12:30,736 --> 00:12:33,516
On your AR session object, you


287
00:12:33,516 --> 00:12:35,576
will need to call get current


288
00:12:35,576 --> 00:12:39,666
world map at any point in time.


289
00:12:39,836 --> 00:12:41,146
This method comes with a


290
00:12:41,146 --> 00:12:43,846
completion handler in which it


291
00:12:43,846 --> 00:12:46,586
will return you and ARWorldMap


292
00:12:48,446 --> 00:12:48,656
object.


293
00:12:49,006 --> 00:12:50,676
Note also that it can also


294
00:12:50,676 --> 00:12:52,746
return and other in case World


295
00:12:52,746 --> 00:12:53,846
Map is not available.


296
00:12:54,186 --> 00:12:55,676
So it's important to handle this


297
00:12:55,676 --> 00:12:57,486
error in your application code.


298
00:12:59,116 --> 00:13:03,146
So once you have ARWorldMap, you


299
00:13:03,146 --> 00:13:08,946
can simply set initial World Map


300
00:13:08,946 --> 00:13:10,686
property in world tracking


301
00:13:10,686 --> 00:13:13,596
configuration and run your


302
00:13:13,596 --> 00:13:13,966
session.


303
00:13:14,566 --> 00:13:17,356
Note that this can be


304
00:13:17,356 --> 00:13:18,976
dynamically changed as well, so


305
00:13:18,976 --> 00:13:20,646
you can also reconfigure AR


306
00:13:20,986 --> 00:13:22,466
session by running a new


307
00:13:22,466 --> 00:13:23,046
configuration.


308
00:13:23,626 --> 00:13:27,606
So once AR session is started


309
00:13:27,696 --> 00:13:31,266
with ARWorldMap, it will follow


310
00:13:31,266 --> 00:13:33,086
the exact same behavior of


311
00:13:33,086 --> 00:13:34,526
relocalization that we


312
00:13:34,526 --> 00:13:36,976
introduced in iOS 11.3.


313
00:13:44,086 --> 00:13:46,276
It is important for your


314
00:13:46,906 --> 00:13:49,946
experience that relocalization


315
00:13:49,946 --> 00:13:50,836
works reliably.


316
00:13:50,836 --> 00:13:54,076
So it is good to, it is


317
00:13:54,076 --> 00:13:55,296
important to acquire good World


318
00:13:55,296 --> 00:13:55,626
Maps.


319
00:13:56,406 --> 00:13:57,996
Note that you can call get a


320
00:13:57,996 --> 00:13:59,906
current world map at any point


321
00:14:00,646 --> 00:14:02,526
in time.


322
00:14:02,526 --> 00:14:04,976
So, it's important to scan your


323
00:14:04,976 --> 00:14:07,626
physical space from multiple


324
00:14:07,666 --> 00:14:09,226
point of views.


325
00:14:09,256 --> 00:14:11,266
So tracking system can really


326
00:14:11,266 --> 00:14:12,516
learn about physical structure


327
00:14:12,516 --> 00:14:13,316
of the environment.


328
00:14:14,956 --> 00:14:17,996
The environment should be static


329
00:14:17,996 --> 00:14:19,976
and well textured so we can


330
00:14:19,976 --> 00:14:22,326
learn, extract more features of


331
00:14:22,326 --> 00:14:23,936
it and learn more about the


332
00:14:23,936 --> 00:14:24,376
environment.


333
00:14:27,596 --> 00:14:30,116
And also, it's important to have


334
00:14:30,116 --> 00:14:31,986
dense feature points on the map,


335
00:14:32,296 --> 00:14:34,936
so it can reliably relocalize.


336
00:14:37,076 --> 00:14:38,716
But you don't have to worry


337
00:14:38,716 --> 00:14:40,256
about all those points.


338
00:14:41,676 --> 00:14:43,916
In ARKit, API really makes


339
00:14:43,976 --> 00:14:46,076
things easier for you by giving


340
00:14:46,076 --> 00:14:47,776
you WorldMappingStatus on


341
00:14:47,776 --> 00:14:48,186
ARFrame.


342
00:14:49,366 --> 00:14:51,516
WorldMappingStatus is updated in


343
00:14:51,516 --> 00:14:53,856
every ARFrame and can be


344
00:14:53,856 --> 00:14:55,776
retrieved by WorldMappingStatus


345
00:14:55,956 --> 00:14:56,376
property.


346
00:14:56,896 --> 00:14:58,456
So let's see how this works.


347
00:14:59,106 --> 00:15:02,866
So when we start world tracking,


348
00:15:02,866 --> 00:15:04,186
WorldMappingStatus will be not


349
00:15:04,186 --> 00:15:04,666
available.


350
00:15:05,056 --> 00:15:06,936
As soon as we start scanning the


351
00:15:06,936 --> 00:15:08,156
physical space, it will be


352
00:15:08,266 --> 00:15:08,786
limited.


353
00:15:10,356 --> 00:15:12,786
The more we move in the physical


354
00:15:12,786 --> 00:15:14,526
world, world tracking will


355
00:15:14,526 --> 00:15:19,406
continue to extend the map.


356
00:15:19,546 --> 00:15:21,196
And if we have scanned enough


357
00:15:21,336 --> 00:15:23,656
physical world from current


358
00:15:23,656 --> 00:15:24,206
point of view,


359
00:15:24,366 --> 00:15:25,586
WorldMappingStatus will be


360
00:15:25,756 --> 00:15:26,096
mapped.


361
00:15:34,236 --> 00:15:36,486
So note that if you point away


362
00:15:36,486 --> 00:15:39,356
from a map physical space,


363
00:15:39,926 --> 00:15:41,766
WorldMappingStatus may go back


364
00:15:41,766 --> 00:15:42,526
to limited.


365
00:15:43,126 --> 00:15:44,756
So it will start to learn more


366
00:15:44,756 --> 00:15:46,996
about the new environment that


367
00:15:46,996 --> 00:15:51,076
we are starting to see.


368
00:15:51,306 --> 00:15:52,196
So how you can use


369
00:15:52,196 --> 00:15:53,386
WorldMappingStatus in your


370
00:15:53,386 --> 00:15:54,096
application code.


371
00:15:54,586 --> 00:15:58,076
Let's say you have an app that


372
00:15:58,076 --> 00:15:59,806
lets you share your World Map


373
00:15:59,876 --> 00:16:02,346
with another user, and you have


374
00:16:02,346 --> 00:16:04,906
a shared map button on your user


375
00:16:04,906 --> 00:16:05,356
interface.


376
00:16:05,896 --> 00:16:09,096
It's a good practice to disable


377
00:16:09,096 --> 00:16:10,046
this button when


378
00:16:10,046 --> 00:16:11,146
WorldMappingStatus is not


379
00:16:11,146 --> 00:16:12,576
available or limited.


380
00:16:13,096 --> 00:16:16,166
And when WorldMappingStatus is


381
00:16:16,196 --> 00:16:19,386
extending, you may want to show


382
00:16:19,386 --> 00:16:21,446
an activity indicator on UI.


383
00:16:22,046 --> 00:16:23,936
So this encourages your end user


384
00:16:24,426 --> 00:16:25,786
to continue moving in the


385
00:16:25,786 --> 00:16:27,476
physical world and continue


386
00:16:27,546 --> 00:16:29,526
scanning it and extending the


387
00:16:29,526 --> 00:16:31,446
map, because you need that for


388
00:16:31,446 --> 00:16:32,146
relocalization.


389
00:16:36,256 --> 00:16:38,416
Once WorldMappingStatus is fully


390
00:16:38,446 --> 00:16:42,476
mapped, you may enable your


391
00:16:42,476 --> 00:16:44,346
share map button and hide your


392
00:16:44,466 --> 00:16:45,366
activity indicator.


393
00:16:46,096 --> 00:16:47,886
So this will let your user to


394
00:16:47,886 --> 00:16:49,046
share the map.


395
00:16:53,716 --> 00:16:55,676
So let's see a demo of saving


396
00:16:55,676 --> 00:16:57,146
and loading World Map.


397
00:16:58,516 --> 00:17:06,546
[ Applause ]


398
00:17:07,046 --> 00:17:11,036
Okay. So can we switch to AR 1.


399
00:17:13,306 --> 00:17:15,976
Okay. So for this demo, I have


400
00:17:16,116 --> 00:17:16,955
two apps.


401
00:17:17,665 --> 00:17:20,586
In one app I will retrieve and


402
00:17:20,616 --> 00:17:22,576
save World Map to a local file,


403
00:17:23,366 --> 00:17:26,046
and in my second app, I will


404
00:17:26,046 --> 00:17:28,406
load the same World Map to


405
00:17:28,406 --> 00:17:30,096
restore the same augmented


406
00:17:30,096 --> 00:17:31,146
reality experience.


407
00:17:31,626 --> 00:17:32,266
So let's start.


408
00:17:32,266 --> 00:17:36,896
So as you can see,


409
00:17:36,896 --> 00:17:38,886
WorldMappingStatus on top right


410
00:17:38,886 --> 00:17:39,376
corner.


411
00:17:39,876 --> 00:17:41,466
It was not available.


412
00:17:41,826 --> 00:17:43,476
As soon as I start to move in


413
00:17:43,476 --> 00:17:46,416
the environment, it is now


414
00:17:46,416 --> 00:17:47,846
extending my World Map.


415
00:17:47,846 --> 00:17:50,666
So if I continue to map and move


416
00:17:50,666 --> 00:17:53,476
in this environment, the


417
00:17:53,476 --> 00:17:55,336
WorldMappingStatus will go


418
00:17:55,336 --> 00:17:56,016
mapped.


419
00:17:57,206 --> 00:17:58,956
So it means that it has seen


420
00:17:58,956 --> 00:18:01,356
enough features from this point


421
00:18:01,356 --> 00:18:03,216
of view for relocalization to


422
00:18:03,216 --> 00:18:03,446
work.


423
00:18:03,876 --> 00:18:06,406
So it is a good time to retrieve


424
00:18:06,486 --> 00:18:08,326
and serialize World Map object.


425
00:18:09,956 --> 00:18:12,546
But let's make this augmented


426
00:18:12,546 --> 00:18:14,576
reality scene more interesting


427
00:18:14,636 --> 00:18:16,116
by placing a custom anchor.


428
00:18:17,176 --> 00:18:19,466
So through hit testing, I have


429
00:18:19,466 --> 00:18:21,816
created a custom anchor, and I


430
00:18:21,816 --> 00:18:23,926
am overlaying this object,


431
00:18:23,926 --> 00:18:27,266
basically it's a old TV.


432
00:18:27,936 --> 00:18:29,116
I think most of you may have


433
00:18:29,166 --> 00:18:30,586
seen in the past.


434
00:18:33,136 --> 00:18:34,806
So of course I can still


435
00:18:34,806 --> 00:18:38,156
continue mapping the world, and


436
00:18:38,156 --> 00:18:40,276
let's save the World Map.


437
00:18:41,426 --> 00:18:44,246
So when I saved my World Map, I


438
00:18:44,246 --> 00:18:45,916
could also show raw feature


439
00:18:45,916 --> 00:18:47,136
points that belongs to this


440
00:18:47,136 --> 00:18:47,586
World Map.


441
00:18:47,866 --> 00:18:49,416
So those blue dots that you see,


442
00:18:49,756 --> 00:18:52,126
they are all part of my World


443
00:18:52,736 --> 00:18:52,866
Map.


444
00:18:55,236 --> 00:18:58,926
And also as a good practice, I


445
00:18:58,926 --> 00:19:02,036
saved a screen shot of my point


446
00:19:02,036 --> 00:19:04,346
of view where I saved World Map.


447
00:19:07,096 --> 00:19:09,696
So now we have serialized World


448
00:19:09,696 --> 00:19:11,276
Map to our file.


449
00:19:12,216 --> 00:19:13,986
We can now restore the same


450
00:19:13,986 --> 00:19:15,636
augmented reality experience in


451
00:19:15,696 --> 00:19:16,226
another app.


452
00:19:17,096 --> 00:19:18,096
So let's try that.


453
00:19:18,836 --> 00:19:20,126
I will start this app from a


454
00:19:20,126 --> 00:19:24,516
different position, and you can


455
00:19:24,516 --> 00:19:26,806
see this is my world origin.


456
00:19:26,806 --> 00:19:28,526
It is defined on this side of


457
00:19:28,526 --> 00:19:30,606
the table, and my world tracking


458
00:19:30,606 --> 00:19:32,506
is now in relocalizing state.


459
00:19:33,216 --> 00:19:34,526
So this is the same opening


460
00:19:34,526 --> 00:19:36,366
relocalization behavior that we


461
00:19:36,426 --> 00:19:40,296
introduced in iOS 11.3.


462
00:19:40,296 --> 00:19:44,366
So let me point my device to the


463
00:19:44,446 --> 00:19:46,496
physical place where I created


464
00:19:46,496 --> 00:19:47,526
World Map.


465
00:19:48,416 --> 00:19:50,706
So as soon as I point to that


466
00:19:50,776 --> 00:19:53,126
same space, it restored my world


467
00:19:53,126 --> 00:19:57,016
origin back to where it was, and


468
00:19:57,016 --> 00:19:59,306
at the same time it also


469
00:19:59,306 --> 00:20:00,636
restored my custom anchor.


470
00:20:00,986 --> 00:20:02,596
So I have the exact same AR


471
00:20:02,596 --> 00:20:03,026
experience.


472
00:20:04,016 --> 00:20:08,916
[ Applause ]


473
00:20:09,416 --> 00:20:09,916
Thank you.


474
00:20:11,586 --> 00:20:13,176
So now note that I can start


475
00:20:13,176 --> 00:20:15,276
this app as many times I want,


476
00:20:15,776 --> 00:20:17,456
and it will show me the same


477
00:20:17,456 --> 00:20:18,936
experience every time I start.


478
00:20:19,556 --> 00:20:20,696
So this is persistence.


479
00:20:21,306 --> 00:20:23,566
And of course, this can be


480
00:20:23,566 --> 00:20:25,146
shared with another device.


481
00:20:25,826 --> 00:20:28,756
So back to slides.


482
00:20:34,196 --> 00:20:36,616
So this was saving and loading


483
00:20:36,746 --> 00:20:37,046
map.


484
00:20:38,036 --> 00:20:39,726
It's a powerful new feature in


485
00:20:39,726 --> 00:20:43,006
ARKit 2 that enables persistence


486
00:20:43,516 --> 00:20:44,766
and multiuser shared


487
00:20:44,766 --> 00:20:45,856
experiences.


488
00:20:49,766 --> 00:20:51,426
In ARKit 2, we have faster


489
00:20:51,426 --> 00:20:53,326
initialization and plane


490
00:20:53,326 --> 00:20:53,806
detection.


491
00:20:54,296 --> 00:20:58,226
World tracking is now more


492
00:20:58,226 --> 00:21:00,536
robust, and we can detect planes


493
00:21:00,656 --> 00:21:02,276
in more difficult environments.


494
00:21:07,396 --> 00:21:09,286
Both horizontal and vertical


495
00:21:09,286 --> 00:21:11,506
planes have more accurate extent


496
00:21:11,506 --> 00:21:13,416
and boundaries, so it means that


497
00:21:13,416 --> 00:21:14,816
you can accurately place your


498
00:21:14,816 --> 00:21:15,846
objects in the scene.


499
00:21:19,916 --> 00:21:22,576
In iOS 11.3, we introduced


500
00:21:22,776 --> 00:21:25,436
continuous autofocus for your


501
00:21:25,436 --> 00:21:26,866
augmented reality experiences.


502
00:21:27,346 --> 00:21:31,326
IOS 12 comes with even more


503
00:21:31,326 --> 00:21:33,816
optimizations specifically for


504
00:21:33,816 --> 00:21:35,146
augmented reality experiences.


505
00:21:38,816 --> 00:21:41,466
We are also introducing 4 by 3


506
00:21:41,466 --> 00:21:44,176
video formats in ARKit.


507
00:21:44,606 --> 00:21:49,346
Four by three is a -angle video


508
00:21:49,346 --> 00:21:51,896
format that greatly enhances


509
00:21:51,896 --> 00:21:54,766
your visualization on iPad


510
00:21:54,766 --> 00:21:56,496
because iPad also have 4 by 3


511
00:21:56,626 --> 00:21:57,826
display aspect ratios.


512
00:21:58,346 --> 00:22:01,676
Note that 4 by 3 video format


513
00:22:01,676 --> 00:22:03,606
will be the default video format


514
00:22:03,606 --> 00:22:06,556
in ARKit 2.


515
00:22:06,816 --> 00:22:08,326
So all of these enhancements,


516
00:22:08,966 --> 00:22:10,896
they will be applied to all


517
00:22:10,896 --> 00:22:12,486
existing apps in the App Store


518
00:22:13,176 --> 00:22:15,406
except 4 by 3 video format.


519
00:22:15,536 --> 00:22:18,176
For that, you will have to build


520
00:22:18,176 --> 00:22:20,086
your app with the new STK.


521
00:22:21,776 --> 00:22:26,506
So coming back to improving


522
00:22:26,506 --> 00:22:30,636
end-user experience, we are


523
00:22:30,876 --> 00:22:32,316
introducing environment


524
00:22:32,316 --> 00:22:32,806
texturing.


525
00:22:33,876 --> 00:22:36,116
So this greatly enhances your


526
00:22:36,116 --> 00:22:38,176
rendering for end-user


527
00:22:38,176 --> 00:22:38,876
experiences.


528
00:22:40,976 --> 00:22:43,826
So let's say your designer have


529
00:22:44,016 --> 00:22:45,496
worked really hard to create


530
00:22:45,496 --> 00:22:47,486
these virtual objects for you,


531
00:22:47,776 --> 00:22:49,156
for your augmented reality


532
00:22:49,696 --> 00:22:49,786
scene.


533
00:22:50,616 --> 00:22:53,466
This looks really great, but you


534
00:22:53,466 --> 00:22:56,226
need to do more for your


535
00:22:56,226 --> 00:22:57,316
augmented reality scene.


536
00:22:57,316 --> 00:23:03,046
You need to have position and


537
00:23:03,046 --> 00:23:05,026
orientation correct in your AR


538
00:23:05,436 --> 00:23:08,376
scene so that object really


539
00:23:08,376 --> 00:23:10,716
looks like it is placed in the


540
00:23:10,716 --> 00:23:11,746
real world.


541
00:23:13,366 --> 00:23:15,256
It is also important to get the


542
00:23:15,256 --> 00:23:17,106
scale right so your object is


543
00:23:17,106 --> 00:23:18,666
not too big or not too small.


544
00:23:19,176 --> 00:23:21,236
So ARKit helps you by giving you


545
00:23:21,236 --> 00:23:22,876
the correct transform in world


546
00:23:23,106 --> 00:23:24,606
tracking.


547
00:23:28,456 --> 00:23:30,386
For realistic rendering, it is


548
00:23:30,386 --> 00:23:31,886
important to also consider


549
00:23:32,346 --> 00:23:33,496
lighting in the environment.


550
00:23:33,916 --> 00:23:38,836
ARKit gives you ambient light


551
00:23:38,836 --> 00:23:40,296
estimator that you can use in


552
00:23:40,296 --> 00:23:43,576
your rendering to correct the


553
00:23:43,576 --> 00:23:45,066
brightness of your objects.


554
00:23:45,436 --> 00:23:47,136
So your objects don't look too


555
00:23:47,136 --> 00:23:48,776
bright or too dark.


556
00:23:49,386 --> 00:23:51,746
They just blend into the


557
00:23:54,676 --> 00:23:55,056
environment.


558
00:23:55,056 --> 00:23:56,966
If you are placing your objects


559
00:23:56,966 --> 00:23:58,716
on physical surfaces such as


560
00:23:58,716 --> 00:24:01,076
horizontal planes, it is also


561
00:24:01,076 --> 00:24:03,416
important to add a shadow for


562
00:24:03,416 --> 00:24:03,866
the object.


563
00:24:04,506 --> 00:24:06,916
So this greatly improves human


564
00:24:06,916 --> 00:24:07,776
visual perception.


565
00:24:08,036 --> 00:24:09,106
They can really perceive that


566
00:24:09,106 --> 00:24:13,256
objection is on the surface.


567
00:24:13,426 --> 00:24:16,876
And last, in case of reflective


568
00:24:16,876 --> 00:24:20,156
objects, humans wants to see


569
00:24:20,916 --> 00:24:22,576
reflection of the environment


570
00:24:22,716 --> 00:24:24,376
from the surface of the virtual


571
00:24:24,376 --> 00:24:24,696
objects.


572
00:24:25,886 --> 00:24:28,166
So this is what environment


573
00:24:28,166 --> 00:24:28,946
texturing enables.


574
00:24:29,006 --> 00:24:32,496
So let's see how this object


575
00:24:32,496 --> 00:24:34,866
looks like in an augmented


576
00:24:34,866 --> 00:24:37,976
reality scene.


577
00:24:38,196 --> 00:24:39,426
So I created this scene


578
00:24:39,426 --> 00:24:41,076
yesterday evening when I was


579
00:24:41,156 --> 00:24:42,696
preparing for this presentation.


580
00:24:43,246 --> 00:24:47,186
So while eating those fruits, I


581
00:24:47,186 --> 00:24:48,346
also wanted to place this


582
00:24:48,346 --> 00:24:49,606
virtual object.


583
00:24:49,606 --> 00:24:54,886
And you can see, it is correct


584
00:24:54,996 --> 00:24:57,486
to scale, and you can see more


585
00:24:57,486 --> 00:24:59,356
importantly you can see a


586
00:24:59,356 --> 00:25:00,956
reflection of the environment in


587
00:25:01,476 --> 00:25:02,336
the object.


588
00:25:02,496 --> 00:25:04,326
On your right side of this


589
00:25:04,326 --> 00:25:06,766
object, you can see this yellow


590
00:25:06,766 --> 00:25:09,376
and orange reflection of those


591
00:25:09,376 --> 00:25:11,386
fruits on the right, and on the


592
00:25:11,386 --> 00:25:13,646
left, you can notice the green


593
00:25:13,646 --> 00:25:14,596
texture from the leaves.


594
00:25:15,906 --> 00:25:17,196
And in the middle, you can also


595
00:25:17,196 --> 00:25:19,336
see reflection of the surface of


596
00:25:20,136 --> 00:25:21,386
the bench.


597
00:25:21,616 --> 00:25:23,286
So this is enabled by


598
00:25:23,286 --> 00:25:25,366
environment texturing in ARKit


599
00:25:25,906 --> 00:25:25,976
2.


600
00:25:29,336 --> 00:25:29,606
Thank you.


601
00:25:30,476 --> 00:25:33,636
[ Applause ]


602
00:25:34,136 --> 00:25:35,826
So environment texturing gathers


603
00:25:35,906 --> 00:25:37,456
scene texture information.


604
00:25:40,616 --> 00:25:43,176
Usually it is represented as a


605
00:25:43,176 --> 00:25:44,666
cube map, but there are other


606
00:25:44,666 --> 00:25:48,956
representations as well.


607
00:25:49,036 --> 00:25:51,426
Environment texture or this cube


608
00:25:51,426 --> 00:25:53,746
map can be used as a reflection


609
00:25:53,746 --> 00:25:56,146
probe in your rendering engines.


610
00:25:58,636 --> 00:26:01,696
This reflection probe can apply


611
00:26:01,696 --> 00:26:03,646
this as texture information onto


612
00:26:03,646 --> 00:26:05,426
virtual objects, such as the one


613
00:26:05,426 --> 00:26:06,476
we saw in the last slide.


614
00:26:07,286 --> 00:26:10,146
So it greatly improves


615
00:26:11,326 --> 00:26:13,156
visualization of reflective


616
00:26:13,156 --> 00:26:13,626
objects.


617
00:26:14,936 --> 00:26:16,996
So let's see how this works in


618
00:26:18,136 --> 00:26:20,026
this short video clip.


619
00:26:22,076 --> 00:26:24,796
So ARKit, while running world


620
00:26:24,796 --> 00:26:25,816
tracking and scene


621
00:26:25,816 --> 00:26:27,596
understanding, continues to


622
00:26:27,596 --> 00:26:28,506
learn more about the


623
00:26:28,506 --> 00:26:29,036
environment.


624
00:26:30,226 --> 00:26:32,706
Using computer vision, it can


625
00:26:33,066 --> 00:26:35,676
extract textured information and


626
00:26:35,676 --> 00:26:37,646
start to fill this cube map.


627
00:26:38,136 --> 00:26:41,676
And this cube map is accurately


628
00:26:41,676 --> 00:26:42,636
placed in the scene.


629
00:26:43,166 --> 00:26:46,456
Note that this cube map is


630
00:26:46,456 --> 00:26:50,456
partially filled, and to set up


631
00:26:50,506 --> 00:26:52,186
reflection probes, we need to


632
00:26:52,186 --> 00:26:53,926
have a fully completed cube map.


633
00:26:54,486 --> 00:26:58,756
To have a fully completed cube


634
00:26:58,756 --> 00:27:01,396
map, you will need to scan your


635
00:27:01,466 --> 00:27:03,966
full physical space, something


636
00:27:03,966 --> 00:27:06,286
like a 360-degree scan you do


637
00:27:06,286 --> 00:27:07,376
with your panoramas.


638
00:27:08,586 --> 00:27:10,856
But this is not practical for


639
00:27:10,956 --> 00:27:11,516
end-users.


640
00:27:13,296 --> 00:27:15,526
So ARKit makes it very easy for


641
00:27:15,526 --> 00:27:18,626
you by automatically completing


642
00:27:18,626 --> 00:27:20,756
this cube map using advanced


643
00:27:20,756 --> 00:27:21,946
machine learning algorithms.


644
00:27:24,016 --> 00:27:29,460
[ Applause ]


645
00:27:31,536 --> 00:27:33,446
Note also that all of this


646
00:27:33,446 --> 00:27:35,296
processing happens locally on


647
00:27:35,296 --> 00:27:37,416
your device in real-time.


648
00:27:39,556 --> 00:27:41,666
So once we have a cube map, we


649
00:27:41,666 --> 00:27:44,116
can set up reflection probe and


650
00:27:44,116 --> 00:27:46,066
as soon as we place virtual


651
00:27:46,066 --> 00:27:48,186
objects in the scene, they start


652
00:27:48,186 --> 00:27:49,506
to reflect the real environment.


653
00:27:50,006 --> 00:27:52,726
So this was a quick overview of


654
00:27:52,726 --> 00:27:54,416
how this environment texturing


655
00:27:54,416 --> 00:27:55,116
process works.


656
00:27:55,116 --> 00:28:00,286
Let's see how ARKit API makes it


657
00:28:00,286 --> 00:28:03,446
very easy for you to enable this


658
00:28:07,516 --> 00:28:07,686
feature.


659
00:28:07,836 --> 00:28:09,966
So all you have to do in your


660
00:28:09,966 --> 00:28:12,516
world tracking configuration to


661
00:28:12,516 --> 00:28:14,006
set environment texturing


662
00:28:14,006 --> 00:28:17,776
property to automatic and run


663
00:28:17,776 --> 00:28:18,206
the session.


664
00:28:19,196 --> 00:28:21,206
So this is as simple as this.


665
00:28:22,516 --> 00:28:28,576
[ Applause ]


666
00:28:29,076 --> 00:28:31,186
AR session will automatically


667
00:28:31,186 --> 00:28:32,946
run this environment texturing


668
00:28:32,946 --> 00:28:34,976
process in the background and


669
00:28:34,976 --> 00:28:36,326
will give you environment


670
00:28:36,546 --> 00:28:39,316
texture as an environment probe


671
00:28:39,316 --> 00:28:39,616
anchor.


672
00:28:40,996 --> 00:28:43,416
AREnvironmentProbeAnchor is an


673
00:28:43,416 --> 00:28:45,226
extension of AR anchor, so it


674
00:28:45,226 --> 00:28:46,916
means it has a six degrees of


675
00:28:46,916 --> 00:28:48,496
freedom position and orientation


676
00:28:48,496 --> 00:28:49,066
transform.


677
00:28:50,716 --> 00:28:53,346
Moreover, it has a cube map in


678
00:28:53,346 --> 00:28:58,596
the form of metal texture.


679
00:28:58,596 --> 00:29:00,336
ARKit also gives you physical


680
00:29:00,336 --> 00:29:02,356
extent of the cube map.


681
00:29:02,846 --> 00:29:04,546
So this is area of the influence


682
00:29:04,836 --> 00:29:08,176
of the reflection probe, and it


683
00:29:08,176 --> 00:29:10,786
can be used by rendering agents


684
00:29:11,246 --> 00:29:12,846
to correct for parallels.


685
00:29:12,846 --> 00:29:14,176
So such as in case your object


686
00:29:14,176 --> 00:29:16,186
is moving in the scene, it will


687
00:29:16,186 --> 00:29:18,636
automatically adapt to new


688
00:29:18,636 --> 00:29:21,226
position and new texture will be


689
00:29:21,226 --> 00:29:22,226
reflected in the environment.


690
00:29:22,686 --> 00:29:26,666
Note that this follows same


691
00:29:26,666 --> 00:29:28,476
lifecycle as every other anchor


692
00:29:29,036 --> 00:29:31,166
such as AR plane anchor or AR


693
00:29:31,166 --> 00:29:31,676
image anchor.


694
00:29:36,676 --> 00:29:38,396
Furthermore, it is fully


695
00:29:38,396 --> 00:29:40,566
integrated into ARSCNView.


696
00:29:41,016 --> 00:29:42,396
So in case you are using


697
00:29:42,646 --> 00:29:44,216
SceneKit as your rendering


698
00:29:44,216 --> 00:29:47,136
technology, you just need to


699
00:29:47,136 --> 00:29:49,586
enable this feature in world


700
00:29:49,586 --> 00:29:50,486
tracking configuration.


701
00:29:51,056 --> 00:29:52,706
The rest is done automatically


702
00:29:52,796 --> 00:29:54,076
by ARSCNView.


703
00:29:58,996 --> 00:30:02,146
Note that for advanced use


704
00:30:02,146 --> 00:30:04,496
cases, you may want to place


705
00:30:04,496 --> 00:30:05,666
environment probe anchors


706
00:30:05,876 --> 00:30:11,406
manually in the scene.


707
00:30:11,726 --> 00:30:13,246
So for this, you will need to


708
00:30:13,246 --> 00:30:14,796
set environment texturing mode


709
00:30:14,856 --> 00:30:18,626
to manual, and then you can


710
00:30:18,626 --> 00:30:20,366
create environment probe anchors


711
00:30:21,086 --> 00:30:23,066
at your desired position and


712
00:30:23,066 --> 00:30:24,836
orientation and add them to AR


713
00:30:24,836 --> 00:30:25,476
session object.


714
00:30:25,886 --> 00:30:30,306
Note that this only enables you


715
00:30:30,306 --> 00:30:32,936
to place the probe anchors in


716
00:30:33,516 --> 00:30:34,006
the scene.


717
00:30:34,006 --> 00:30:35,666
AR session will automatically


718
00:30:35,666 --> 00:30:37,606
update its texture as soon as it


719
00:30:37,656 --> 00:30:39,596
gets more information about the


720
00:30:39,596 --> 00:30:39,996
environment.


721
00:30:42,016 --> 00:30:45,906
So you may use this mode in case


722
00:30:45,906 --> 00:30:48,306
your augmented reality scene has


723
00:30:48,306 --> 00:30:49,136
a single object.


724
00:30:49,296 --> 00:30:51,176
You don't want to overload


725
00:30:51,576 --> 00:30:53,346
system with too many environment


726
00:30:53,916 --> 00:30:57,046
probe anchors.


727
00:30:57,246 --> 00:30:59,306
So let's see a quick demo of


728
00:30:59,306 --> 00:31:00,926
environment texturing and see


729
00:31:01,426 --> 00:31:03,506
how we can realistically render


730
00:31:03,866 --> 00:31:05,806
augmented reality scene.


731
00:31:06,516 --> 00:31:10,500
[ Applause ]


732
00:31:16,316 --> 00:31:18,646
So we can switch to AR 1.


733
00:31:23,626 --> 00:31:25,916
Okay. So for this demo, I am


734
00:31:26,116 --> 00:31:27,856
running world tracking


735
00:31:27,856 --> 00:31:29,986
configuration without


736
00:31:29,986 --> 00:31:31,516
environment texturing feature


737
00:31:31,516 --> 00:31:31,896
enabled.


738
00:31:33,176 --> 00:31:37,656
So as you can see on the bottom


739
00:31:37,706 --> 00:31:39,626
switch controller, it's just


740
00:31:39,626 --> 00:31:41,026
using ambient light estimate.


741
00:31:41,456 --> 00:31:44,146
And let's place the same object


742
00:31:44,146 --> 00:31:45,856
that we have seen before.


743
00:31:47,116 --> 00:31:52,376
And you can see it is, it looks


744
00:31:52,376 --> 00:31:52,746
okay.


745
00:31:52,746 --> 00:31:53,766
I mean you can see this on the


746
00:31:53,766 --> 00:31:54,056
table.


747
00:31:54,056 --> 00:31:55,656
You can see the shadow of it,


748
00:31:56,096 --> 00:31:57,886
and it looks like a really good


749
00:31:57,886 --> 00:31:58,356
AR scene.


750
00:31:59,686 --> 00:32:01,536
But what we are missing is that


751
00:32:01,536 --> 00:32:03,776
it does not reflect wooden


752
00:32:03,776 --> 00:32:06,786
surface of the table.


753
00:32:06,916 --> 00:32:08,286
So moreover, if I place


754
00:32:08,286 --> 00:32:11,526
something in the scene such as


755
00:32:11,526 --> 00:32:17,516
real fruit, we don't see a


756
00:32:17,516 --> 00:32:20,516
reflection of it in the virtual


757
00:32:20,936 --> 00:32:21,126
object.


758
00:32:21,656 --> 00:32:23,616
So let's enable environment


759
00:32:23,616 --> 00:32:25,896
texturing and see how it can


760
00:32:26,206 --> 00:32:29,156
realistically represent this


761
00:32:29,216 --> 00:32:29,576
texture.


762
00:32:30,636 --> 00:32:32,326
So as you can see, as soon as I


763
00:32:32,326 --> 00:32:33,766
enable environment texturing,


764
00:32:34,526 --> 00:32:36,366
the object started to reflect


765
00:32:36,366 --> 00:32:38,616
the wooden surface of the table


766
00:32:39,016 --> 00:32:41,686
as well as the texture from this


767
00:32:41,746 --> 00:32:42,176
banana.


768
00:32:43,516 --> 00:32:50,546
[ Applause ]


769
00:32:51,046 --> 00:32:51,266
Thank you.


770
00:32:52,626 --> 00:32:55,326
So this greatly enhances you


771
00:32:55,326 --> 00:32:56,706
augmented reality scene.


772
00:32:57,346 --> 00:32:59,586
So it looks as real as possible,


773
00:33:00,776 --> 00:33:04,726
as if it is really on the table.


774
00:33:05,056 --> 00:33:06,796
Okay. Back to slides.


775
00:33:12,806 --> 00:33:14,086
So this was environment


776
00:33:14,206 --> 00:33:14,686
texturing.


777
00:33:14,866 --> 00:33:17,116
It's a powerful new feature in


778
00:33:17,116 --> 00:33:20,616
ARKit 2 that lets you create


779
00:33:20,616 --> 00:33:22,436
your augmented reality scene as


780
00:33:22,436 --> 00:33:26,256
realistic as possible.


781
00:33:26,256 --> 00:33:28,116
Now, to continue with the rest


782
00:33:28,116 --> 00:33:30,296
of the great new features, I


783
00:33:30,296 --> 00:33:32,376
will invite Reinhard on stage.


784
00:33:34,476 --> 00:33:38,660
[ Applause ]


785
00:33:42,226 --> 00:33:42,866
>> It's working?


786
00:33:43,206 --> 00:33:43,856
Oh, okay, great.


787
00:33:46,066 --> 00:33:46,496
Good morning.


788
00:33:47,376 --> 00:33:48,936
My name is Reinhard, and I'm an


789
00:33:48,936 --> 00:33:50,336
engineer on the ARKit team.


790
00:33:50,746 --> 00:33:52,446
So next let's talk about image


791
00:33:52,446 --> 00:33:52,816
tracking.


792
00:33:52,816 --> 00:33:56,566
In iOS 11.3, we introduced image


793
00:33:56,566 --> 00:33:58,116
detection as part of world


794
00:33:58,116 --> 00:33:58,536
tracking.


795
00:33:59,646 --> 00:34:01,326
Image detection searches for


796
00:34:01,326 --> 00:34:03,086
known 2D images in the scene.


797
00:34:03,776 --> 00:34:05,836
The term detection here implies


798
00:34:05,996 --> 00:34:07,996
that these images are static and


799
00:34:07,996 --> 00:34:09,126
are therefore not supposed to


800
00:34:09,126 --> 00:34:09,396
move.


801
00:34:10,326 --> 00:34:11,626
Great examples for such images


802
00:34:11,626 --> 00:34:13,576
could be movie posters or


803
00:34:13,576 --> 00:34:14,746
paintings in a museum.


804
00:34:16,416 --> 00:34:18,536
ARKit will estimate the position


805
00:34:18,636 --> 00:34:20,156
and orientation of such an image


806
00:34:20,466 --> 00:34:22,216
in six degrees of freedom once


807
00:34:22,216 --> 00:34:23,416
an image has been detected.


808
00:34:24,536 --> 00:34:26,446
This pose can be used to trigger


809
00:34:26,446 --> 00:34:28,005
content in your rendered scene.


810
00:34:28,525 --> 00:34:31,576
As I mentioned earlier, all this


811
00:34:31,626 --> 00:34:33,065
is fully integrated world


812
00:34:33,065 --> 00:34:33,505
tracking.


813
00:34:34,065 --> 00:34:35,065
So all you need to do is set


814
00:34:35,065 --> 00:34:37,626
once in your property to set it


815
00:34:38,666 --> 00:34:38,746
up.


816
00:34:39,025 --> 00:34:40,255
In order to load images to be


817
00:34:40,255 --> 00:34:42,386
used for image detection, you


818
00:34:42,386 --> 00:34:44,275
made load them from file or use


819
00:34:44,346 --> 00:34:46,196
Xcode's asset catalog, which


820
00:34:46,196 --> 00:34:47,946
also gives you the detection


821
00:34:47,946 --> 00:34:49,056
quality for an image.


822
00:34:50,406 --> 00:34:51,496
So image detection is already


823
00:34:51,496 --> 00:34:54,275
great, but now in iOS 12, we can


824
00:34:54,275 --> 00:34:54,735
do better.


825
00:34:54,926 --> 00:34:56,636
So let's talk about image


826
00:34:56,636 --> 00:34:56,976
tracking.


827
00:34:57,846 --> 00:34:59,636
Image tracking is an extension


828
00:34:59,636 --> 00:35:01,546
to image detection with a big


829
00:35:01,546 --> 00:35:03,776
advantage that images no longer


830
00:35:03,776 --> 00:35:07,406
need to be static and may move.


831
00:35:07,616 --> 00:35:09,036
ARKit will now estimate the


832
00:35:09,036 --> 00:35:10,806
position and orientation for


833
00:35:10,806 --> 00:35:12,516
every frame at 60 frames per


834
00:35:12,516 --> 00:35:12,886
second.


835
00:35:13,706 --> 00:35:15,206
This allows you to accurately


836
00:35:15,466 --> 00:35:18,316
augment 2D images, say


837
00:35:18,906 --> 00:35:20,626
magazines, board games, or


838
00:35:20,626 --> 00:35:21,856
pretty much anything that


839
00:35:21,856 --> 00:35:23,396
features a real image.


840
00:35:24,986 --> 00:35:27,136
And ARKit can also track


841
00:35:27,516 --> 00:35:29,066
multiple images simultaneously.


842
00:35:30,886 --> 00:35:32,356
By default it only selects 1,


843
00:35:32,646 --> 00:35:34,596
but in cases, for example, the


844
00:35:34,596 --> 00:35:36,166
cover of a magazine, you may


845
00:35:36,166 --> 00:35:39,156
want to keep this set to 1, or


846
00:35:39,156 --> 00:35:41,046
in case of a double-page


847
00:35:41,046 --> 00:35:42,866
magazine inside a magazine, you


848
00:35:42,866 --> 00:35:43,716
want to set this to 2.


849
00:35:45,416 --> 00:35:49,496
And in ARKit 2 on iOS 12, we


850
00:35:49,736 --> 00:35:51,086
have a brand-new configuration


851
00:35:51,086 --> 00:35:52,556
called the AR Image Tracking


852
00:35:52,556 --> 00:35:54,626
Configuration that lets you do


853
00:35:55,076 --> 00:35:56,546
stand-alone image tracking.


854
00:35:57,356 --> 00:35:58,596
So let's see how to set it up.


855
00:36:00,016 --> 00:36:01,566
We start by loading a set of


856
00:36:01,656 --> 00:36:02,736
reference images, either from


857
00:36:02,736 --> 00:36:04,456
file or from the asset catalog.


858
00:36:05,676 --> 00:36:07,246
Once I'm done loading such a set


859
00:36:07,246 --> 00:36:09,276
of reference images, I use this


860
00:36:09,966 --> 00:36:11,596
to set up my session that can be


861
00:36:11,676 --> 00:36:13,726
of type world tracking by


862
00:36:13,806 --> 00:36:15,226
specifying its detection images


863
00:36:15,286 --> 00:36:17,026
property or of type


864
00:36:17,026 --> 00:36:19,386
ARImageTrackingConfiguration by


865
00:36:19,386 --> 00:36:20,786
specifying the tracking images


866
00:36:20,826 --> 00:36:20,986
one.


867
00:36:22,406 --> 00:36:24,036
Once I'm done setting up my


868
00:36:24,036 --> 00:36:26,316
configuration, I use this to run


869
00:36:26,316 --> 00:36:26,846
my session.


870
00:36:28,296 --> 00:36:30,276
And just as usual, once the


871
00:36:30,326 --> 00:36:31,966
session is running, I'll get an


872
00:36:31,966 --> 00:36:33,496
ARFrame at every update.


873
00:36:34,476 --> 00:36:35,776
And such and ARFrame will


874
00:36:35,776 --> 00:36:37,486
continue an object of type


875
00:36:37,676 --> 00:36:40,116
ARImageAnchor, once an image has


876
00:36:40,156 --> 00:36:40,676
been detected.


877
00:36:41,036 --> 00:36:44,086
Such an ARImageAnchor is now a


878
00:36:44,086 --> 00:36:45,066
trackable object.


879
00:36:45,406 --> 00:36:46,836
I can see this by conforming to


880
00:36:46,836 --> 00:36:48,086
the AR trackable protocol.


881
00:36:48,786 --> 00:36:51,346
This means it comes as a boolean


882
00:36:51,546 --> 00:36:55,036
isTracked, which informs you about


883
00:36:55,036 --> 00:36:56,466
the tracking state of the image.


884
00:36:56,976 --> 00:36:58,566
It's true if it's tracked and


885
00:36:58,566 --> 00:36:59,246
false otherwise.


886
00:37:00,516 --> 00:37:01,956
It also informs about which


887
00:37:01,956 --> 00:37:03,566
image has been detected and


888
00:37:03,566 --> 00:37:06,176
where it is by giving me it's


889
00:37:06,406 --> 00:37:08,536
position and orientation as a 4


890
00:37:08,536 --> 00:37:09,956
by 4 matrix.


891
00:37:11,496 --> 00:37:13,326
So in order to get such image


892
00:37:13,326 --> 00:37:14,896
anchors, it all starts by


893
00:37:14,896 --> 00:37:15,646
loading images.


894
00:37:15,726 --> 00:37:16,676
So that's good.


895
00:37:16,906 --> 00:37:18,176
Let's have a look at what good


896
00:37:18,216 --> 00:37:19,096
images could be.


897
00:37:19,846 --> 00:37:21,426
This image here could be found


898
00:37:21,486 --> 00:37:23,476
in a book for children, and in


899
00:37:23,476 --> 00:37:25,686
fact, it works great for image


900
00:37:25,686 --> 00:37:26,066
tracking.


901
00:37:26,436 --> 00:37:27,976
It has a lot of distinct visual


902
00:37:27,976 --> 00:37:28,496
features.


903
00:37:28,806 --> 00:37:30,346
It's we'll textured and shows


904
00:37:30,346 --> 00:37:31,096
really good contrast.


905
00:37:32,566 --> 00:37:34,236
On the other hand, an image like


906
00:37:34,236 --> 00:37:36,076
this, which could also be found


907
00:37:36,076 --> 00:37:39,446
in a textbook for kids, is not


908
00:37:39,446 --> 00:37:39,986
recommended.


909
00:37:40,756 --> 00:37:41,646
It has a lot of repetitive


910
00:37:41,646 --> 00:37:43,326
structures, uniform color


911
00:37:43,326 --> 00:37:44,876
regions, and a pretty narrow


912
00:37:44,876 --> 00:37:46,666
histogram once converted to gray


913
00:37:46,666 --> 00:37:46,946
scale.


914
00:37:48,436 --> 00:37:49,586
But you don't have to identify


915
00:37:49,586 --> 00:37:50,966
these statistics yourself as


916
00:37:51,066 --> 00:37:52,066
Xcode is here to help.


917
00:37:52,926 --> 00:37:54,716
So if I import these two images


918
00:37:54,716 --> 00:37:57,286
to Xcode, I see the sea life one


919
00:37:57,586 --> 00:37:59,216
without any warning, which means


920
00:37:59,216 --> 00:38:01,296
it's recommended, and the one


921
00:38:01,296 --> 00:38:02,346
about the three kids reading


922
00:38:02,346 --> 00:38:03,676
showing a warning icon, meaning


923
00:38:03,886 --> 00:38:04,776
it's not recommended.


924
00:38:06,096 --> 00:38:07,756
If I click this icon, I get a


925
00:38:07,756 --> 00:38:09,856
precise description why this


926
00:38:09,886 --> 00:38:13,046
image is not recommended to use


927
00:38:13,046 --> 00:38:13,676
image tracking.


928
00:38:14,186 --> 00:38:15,556
I get information about the


929
00:38:15,556 --> 00:38:17,236
histogram, uniform color


930
00:38:17,236 --> 00:38:19,206
regions, as well as the


931
00:38:19,676 --> 00:38:19,946
histogram.


932
00:38:21,136 --> 00:38:23,866
So once I'm done loading images,


933
00:38:23,866 --> 00:38:25,326
I'm left with two choices of


934
00:38:25,376 --> 00:38:26,226
configurations.


935
00:38:26,766 --> 00:38:27,646
First one is


936
00:38:27,646 --> 00:38:29,416
ARWorldTrackingConfiguration.


937
00:38:29,596 --> 00:38:31,826
So let's talk about that.


938
00:38:32,816 --> 00:38:34,656
When we use image tracking with


939
00:38:34,656 --> 00:38:36,726
world tracking, image anchors


940
00:38:36,726 --> 00:38:38,056
are represented in a world


941
00:38:38,056 --> 00:38:38,816
coordinates system.


942
00:38:39,346 --> 00:38:41,426
This means that image anchors


943
00:38:41,476 --> 00:38:43,016
optionally plane anchors.


944
00:38:44,186 --> 00:38:45,946
The camera and the world origin


945
00:38:46,116 --> 00:38:48,516
itself all appear in the same


946
00:38:48,516 --> 00:38:49,276
coordinate system.


947
00:38:49,826 --> 00:38:51,256
This makes their interaction


948
00:38:51,346 --> 00:38:54,286
very easy and intuitive, and


949
00:38:54,286 --> 00:38:57,676
what's new in iOS 12 now images


950
00:38:57,676 --> 00:38:58,666
that could previously only be


951
00:38:58,666 --> 00:39:00,286
detected can now be tracked.


952
00:39:00,726 --> 00:39:03,626
And we have a new configuration,


953
00:39:03,626 --> 00:39:03,746
the


954
00:39:03,746 --> 00:39:05,486
ARImageTrackingConfiguration,


955
00:39:05,846 --> 00:39:07,456
which performs stand-alone image


956
00:39:07,456 --> 00:39:07,836
tracking.


957
00:39:09,086 --> 00:39:10,396
This means it's independent from


958
00:39:10,396 --> 00:39:12,636
world tracking and does not rely


959
00:39:12,706 --> 00:39:14,906
on the motion sensor to perform


960
00:39:14,906 --> 00:39:15,406
the tracking.


961
00:39:16,206 --> 00:39:18,386
This means this configuration is


962
00:39:18,386 --> 00:39:20,966
not to initialize before it


963
00:39:20,966 --> 00:39:23,306
starts to identify images and


964
00:39:23,306 --> 00:39:25,046
could also succeed in scenarios


965
00:39:25,046 --> 00:39:26,696
in which world tracking fails,


966
00:39:26,856 --> 00:39:28,776
such as moving platform like an


967
00:39:28,816 --> 00:39:30,076
elevator or a train.


968
00:39:31,336 --> 00:39:33,766
I think in this case ARKit would


969
00:39:33,816 --> 00:39:35,276
estimate the position and


970
00:39:35,276 --> 00:39:36,946
orientation for every frame at


971
00:39:37,036 --> 00:39:39,286
60 frames per second.


972
00:39:39,406 --> 00:39:40,696
And implementing this can be


973
00:39:40,696 --> 00:39:42,546
done in four simple lines of


974
00:39:42,586 --> 00:39:43,126
code.


975
00:39:44,096 --> 00:39:46,026
So all you need to do, I create


976
00:39:46,026 --> 00:39:46,916
a configuration type


977
00:39:47,096 --> 00:39:49,566
ARImageTrackingConfiguration and


978
00:39:49,566 --> 00:39:52,006
specify a set of images I'd like


979
00:39:52,006 --> 00:39:52,396
to track.


980
00:39:53,146 --> 00:39:54,836
In this case, I specified a


981
00:39:54,836 --> 00:39:56,806
photo of a cat, a dog, and a


982
00:39:56,806 --> 00:39:57,076
bird.


983
00:39:59,556 --> 00:40:00,816
I tell the configuration how


984
00:40:00,816 --> 00:40:02,426
many images I'd like to track.


985
00:40:03,016 --> 00:40:04,236
In this case, I specified this


986
00:40:04,276 --> 00:40:04,816
to be 2.


987
00:40:05,576 --> 00:40:08,126
In my use case, I imagined only


988
00:40:08,126 --> 00:40:10,676
2 images will interact but not 3


989
00:40:10,676 --> 00:40:11,496
at the same time.


990
00:40:12,526 --> 00:40:14,116
Note that if I'm tracking 2


991
00:40:14,226 --> 00:40:16,076
images and a third comes into


992
00:40:16,246 --> 00:40:19,076
its view, it won't be tracked,


993
00:40:19,076 --> 00:40:20,856
but it will still get a


994
00:40:20,856 --> 00:40:21,666
detection update.


995
00:40:23,096 --> 00:40:24,266
And then I use this


996
00:40:24,266 --> 00:40:26,016
configuration to run my session.


997
00:40:26,596 --> 00:40:28,926
And as I mentioned earlier, you


998
00:40:29,146 --> 00:40:30,906
can also do this using world


999
00:40:30,906 --> 00:40:32,796
tracking by simply switching out


1000
00:40:33,136 --> 00:40:33,786
these two lines.


1001
00:40:34,596 --> 00:40:36,216
The only difference between


1002
00:40:36,216 --> 00:40:38,266
image detection and tracking is


1003
00:40:38,266 --> 00:40:39,466
the maximum number of tracking


1004
00:40:39,466 --> 00:40:39,946
images.


1005
00:40:40,786 --> 00:40:44,616
So if you have an app that uses


1006
00:40:44,616 --> 00:40:46,066
image detection, you could


1007
00:40:46,066 --> 00:40:47,856
simply add this, recompile, and


1008
00:40:47,856 --> 00:40:49,686
your app may use tracking.


1009
00:40:51,026 --> 00:40:52,866
So in order to show you how easy


1010
00:40:52,866 --> 00:40:54,376
this really is, let's do a demo


1011
00:40:54,486 --> 00:40:55,046
in Xcode.


1012
00:40:56,676 --> 00:41:02,236
[ Applause ]


1013
00:41:02,736 --> 00:41:04,056
Can we go to AR 2?


1014
00:41:04,266 --> 00:41:07,346
Yes. So for this demo I'd like


1015
00:41:07,416 --> 00:41:10,656
to build a AR photo frame, and


1016
00:41:10,656 --> 00:41:12,176
for this, I brought a photo of


1017
00:41:12,176 --> 00:41:13,036
my cat from home.


1018
00:41:13,846 --> 00:41:15,346
So let's build this using Xcode.


1019
00:41:16,256 --> 00:41:19,996
So I started by creating a iOS


1020
00:41:20,936 --> 00:41:22,666
app template using Xcode.


1021
00:41:23,056 --> 00:41:24,796
As you can see by now it's


1022
00:41:24,796 --> 00:41:26,396
pretty empty.


1023
00:41:26,776 --> 00:41:28,496
Next, I need to specify which


1024
00:41:28,526 --> 00:41:29,736
image I'd like to attach.


1025
00:41:30,596 --> 00:41:32,266
For this I imported the photo of


1026
00:41:32,266 --> 00:41:33,456
my cat, Daisy.


1027
00:41:33,456 --> 00:41:36,436
Let's open her up here.


1028
00:41:37,056 --> 00:41:37,606
That's my cat.


1029
00:41:37,606 --> 00:41:41,176
I need to specify a name.


1030
00:41:41,256 --> 00:41:43,576
I give it the name Daisy, which


1031
00:41:43,576 --> 00:41:45,016
is the name of my cat, and I


1032
00:41:45,016 --> 00:41:47,816
specify here the physical width


1033
00:41:48,176 --> 00:41:49,576
of the image found in the real


1034
00:41:49,576 --> 00:41:50,756
world, which is my photo frame.


1035
00:41:52,296 --> 00:41:55,156
I also loaded a movie of my cat.


1036
00:41:55,956 --> 00:41:56,986
So let's bring this all


1037
00:41:56,986 --> 00:41:57,326
together.


1038
00:41:58,456 --> 00:42:00,696
First, I will create a


1039
00:42:00,696 --> 00:42:03,176
configuration, which will be a


1040
00:42:03,176 --> 00:42:04,166
configuration of type


1041
00:42:04,756 --> 00:42:06,406
ARImageTrackingConfiguration.


1042
00:42:07,426 --> 00:42:08,976
I load a set of tracking images


1043
00:42:08,976 --> 00:42:11,426
from the asset catalog by using


1044
00:42:11,556 --> 00:42:13,176
the group name photos.


1045
00:42:13,836 --> 00:42:15,086
This will contain only one


1046
00:42:15,086 --> 00:42:17,146
image, which is the photo of my


1047
00:42:17,146 --> 00:42:17,736
cat, Daisy.


1048
00:42:18,936 --> 00:42:22,066
Next, I set up the configuration


1049
00:42:22,066 --> 00:42:23,616
image tracking by specifying the


1050
00:42:23,616 --> 00:42:25,486
tracking images property here,


1051
00:42:26,236 --> 00:42:28,106
and I specify the max number of


1052
00:42:28,156 --> 00:42:30,066
tracked images that we want.


1053
00:42:30,446 --> 00:42:31,556
At this point, the app will


1054
00:42:31,556 --> 00:42:34,126
already start an AR session and


1055
00:42:34,126 --> 00:42:35,356
provide you with image anchors


1056
00:42:35,466 --> 00:42:36,596
once an image has been detected.


1057
00:42:37,096 --> 00:42:38,106
But let's add some content.


1058
00:42:38,936 --> 00:42:40,716
I will load the video


1059
00:42:41,916 --> 00:42:45,586
by trading a AV player from the


1060
00:42:46,106 --> 00:42:47,046
video by loading it from the


1061
00:42:47,046 --> 00:42:47,656
resource panel.


1062
00:42:49,136 --> 00:42:51,016
Now let's add it on top of the


1063
00:42:51,016 --> 00:42:52,016
real image.


1064
00:42:52,586 --> 00:42:56,296
So for this, I'm checking


1065
00:42:56,296 --> 00:42:57,636
whether the anchor is of type


1066
00:42:57,686 --> 00:43:00,616
image anchor, and I create an


1067
00:43:00,716 --> 00:43:02,296
SCN plane having the same


1068
00:43:02,366 --> 00:43:04,106
physical dimension as the image


1069
00:43:04,166 --> 00:43:04,876
found in the scene.


1070
00:43:06,256 --> 00:43:08,136
I assign the video player as the


1071
00:43:08,176 --> 00:43:10,796
texture to my plane, and I start


1072
00:43:10,796 --> 00:43:11,766
playing my video player.


1073
00:43:12,356 --> 00:43:15,446
I create an SCN note from


1074
00:43:15,446 --> 00:43:17,356
geometry, and I counter rotate


1075
00:43:17,666 --> 00:43:20,646
to match the anchor's coordinate


1076
00:43:20,866 --> 00:43:21,116
system.


1077
00:43:21,906 --> 00:43:22,896
So that's it.


1078
00:43:22,896 --> 00:43:24,026
This will run.


1079
00:43:24,446 --> 00:43:26,086
Let's see it live.


1080
00:43:27,716 --> 00:43:32,086
So once I bring the frame of my


1081
00:43:32,086 --> 00:43:33,936
cat into the camera's view, the


1082
00:43:34,146 --> 00:43:36,036
video starts playing, and I can


1083
00:43:36,096 --> 00:43:37,126
see my cat interact.


1084
00:43:37,676 --> 00:43:43,946
[ Applause ]


1085
00:43:44,446 --> 00:43:46,516
Since ARKit estimates position


1086
00:43:46,516 --> 00:43:48,186
in real-time, I can move my


1087
00:43:48,186 --> 00:43:50,156
device freely, or I can move the


1088
00:43:50,156 --> 00:43:50,616
object.


1089
00:43:50,616 --> 00:43:52,546
So I can really see that there's


1090
00:43:52,546 --> 00:43:54,056
an update at every frame.


1091
00:43:54,056 --> 00:43:56,626
Oh, oh, she just left.


1092
00:43:56,966 --> 00:43:59,176
I guess it's the end of the


1093
00:43:59,176 --> 00:44:00,316
demo, let's go back to the


1094
00:44:00,316 --> 00:44:00,676
slides.


1095
00:44:02,016 --> 00:44:07,936
[ Applause ]


1096
00:44:08,436 --> 00:44:09,876
So as you can see, it's really


1097
00:44:09,876 --> 00:44:12,106
easy to use image tracking in


1098
00:44:12,106 --> 00:44:12,546
ARKit.


1099
00:44:13,216 --> 00:44:14,626
In fact, it's much harder to


1100
00:44:14,626 --> 00:44:15,696
make a video of your cat.


1101
00:44:18,136 --> 00:44:20,306
So image tracking is great at


1102
00:44:20,306 --> 00:44:22,476
interacting with 2D objects, but


1103
00:44:22,476 --> 00:44:24,476
we're not limited to plainer 2D


1104
00:44:24,476 --> 00:44:28,286
objects, so let's talk next


1105
00:44:28,856 --> 00:44:31,326
about object detection.


1106
00:44:32,036 --> 00:44:35,576
Object detection can be used to


1107
00:44:35,576 --> 00:44:38,236
detect known 3D objects in the


1108
00:44:38,346 --> 00:44:38,636
scene.


1109
00:44:39,916 --> 00:44:41,216
Just like image detection here,


1110
00:44:41,216 --> 00:44:42,906
the term detection means that


1111
00:44:42,906 --> 00:44:44,716
this object needs to be static


1112
00:44:44,906 --> 00:44:46,176
and can therefore, or should


1113
00:44:46,176 --> 00:44:47,186
therefore not move.


1114
00:44:48,366 --> 00:44:50,156
Great examples of such objects


1115
00:44:50,156 --> 00:44:51,946
could be exhibits in a museum,


1116
00:44:52,196 --> 00:44:53,496
certain toys, or household


1117
00:44:53,496 --> 00:44:53,836
items.


1118
00:44:56,586 --> 00:44:57,656
And like image detection,


1119
00:44:58,096 --> 00:44:59,636
objects need to be scanned first


1120
00:44:59,636 --> 00:45:01,756
using an iOS app running ARKit.


1121
00:45:02,686 --> 00:45:05,016
For this we offered the full


1122
00:45:05,016 --> 00:45:06,666
source code of a full-featured


1123
00:45:07,036 --> 00:45:09,256
iOS app that allows you to scan


1124
00:45:09,256 --> 00:45:10,296
your own 3D objects.


1125
00:45:11,436 --> 00:45:13,006
Such objects have a few


1126
00:45:13,096 --> 00:45:15,236
properties such that they need


1127
00:45:15,236 --> 00:45:17,336
to be well textured, rigid, and


1128
00:45:17,376 --> 00:45:18,056
nonreflective.


1129
00:45:18,576 --> 00:45:20,396
And they need to have roughly


1130
00:45:20,396 --> 00:45:23,866
the size of a tabletop.


1131
00:45:23,866 --> 00:45:25,586
ARKit can be used to estimate


1132
00:45:25,586 --> 00:45:27,356
the position and orientation of


1133
00:45:27,356 --> 00:45:28,796
such objects in six degrees of


1134
00:45:28,796 --> 00:45:29,166
freedom.


1135
00:45:29,726 --> 00:45:33,716
And all of this is fully


1136
00:45:33,716 --> 00:45:34,936
integrated into world tracking.


1137
00:45:35,396 --> 00:45:37,026
So all you need to do is set one


1138
00:45:37,026 --> 00:45:38,726
single property to get started


1139
00:45:38,726 --> 00:45:39,526
with object detection.


1140
00:45:40,296 --> 00:45:41,976
So let's have a look how it can


1141
00:45:41,976 --> 00:45:42,356
be set up.


1142
00:45:42,406 --> 00:45:45,206
I load a set of AR reference


1143
00:45:45,296 --> 00:45:47,486
images from file or from Xcode


1144
00:45:47,486 --> 00:45:48,166
asset catalog.


1145
00:45:49,066 --> 00:45:50,726
I will talk about the reference


1146
00:45:50,726 --> 00:45:51,506
objects in a second.


1147
00:45:52,496 --> 00:45:53,586
Once I'm done loading these


1148
00:45:53,586 --> 00:45:54,926
reference objects, I used them


1149
00:45:55,776 --> 00:45:59,716
to set up my configuration of


1150
00:45:59,716 --> 00:46:00,056
type


1151
00:46:00,226 --> 00:46:02,096
ARWorldTrackingConfiguration by


1152
00:46:02,096 --> 00:46:03,776
specifying the detection objects


1153
00:46:03,806 --> 00:46:04,196
property.


1154
00:46:04,706 --> 00:46:07,166
When I'm done setting up my


1155
00:46:07,236 --> 00:46:09,706
configuration, again I run my


1156
00:46:09,786 --> 00:46:10,346
session with it.


1157
00:46:11,406 --> 00:46:12,696
And just as image detection,


1158
00:46:13,326 --> 00:46:14,466
once the AR session is running,


1159
00:46:14,466 --> 00:46:15,496
I get an ARFrame with every


1160
00:46:15,496 --> 00:46:18,936
update, and in this case, once


1161
00:46:18,936 --> 00:46:20,386
an object has been detected in


1162
00:46:20,386 --> 00:46:22,606
the scene, I will find an AR


1163
00:46:23,106 --> 00:46:25,966
object anchor as part of my AR


1164
00:46:25,966 --> 00:46:26,336
frame.


1165
00:46:28,926 --> 00:46:30,866
Such an AR object is a simple


1166
00:46:30,866 --> 00:46:32,506
subclass of AR anchor.


1167
00:46:33,156 --> 00:46:34,336
So it comes with a transform,


1168
00:46:34,336 --> 00:46:35,606
which represents its position


1169
00:46:35,796 --> 00:46:37,016
and orientation and six degrees


1170
00:46:37,016 --> 00:46:39,656
of freedom as well as it tells


1171
00:46:39,656 --> 00:46:41,706
me which objects has been


1172
00:46:41,706 --> 00:46:43,056
detected by giving me a


1173
00:46:43,056 --> 00:46:44,296
reference to the AR reference


1174
00:46:44,296 --> 00:46:44,676
object.


1175
00:46:46,326 --> 00:46:49,596
And implementing this can be


1176
00:46:49,596 --> 00:46:51,866
done with three simple lines of


1177
00:46:51,936 --> 00:46:52,186
code.


1178
00:46:52,976 --> 00:46:54,286
I create a configuration of type


1179
00:46:54,486 --> 00:46:56,876
ARWorldTrackingConfiguration and


1180
00:46:56,876 --> 00:46:59,116
specify a set of objects I'd


1181
00:46:59,116 --> 00:46:59,756
like to detect.


1182
00:47:00,596 --> 00:47:01,936
In this case, I envision to


1183
00:47:01,936 --> 00:47:04,896
build a simple AR museum app by


1184
00:47:04,896 --> 00:47:07,016
detecting an ancient bust and a


1185
00:47:07,016 --> 00:47:07,546
clay pot.


1186
00:47:07,546 --> 00:47:10,726
And I use this to run my


1187
00:47:10,806 --> 00:47:11,126
session.


1188
00:47:12,436 --> 00:47:14,696
So in fact, at the office, we


1189
00:47:14,696 --> 00:47:16,616
build a very simple AR museum


1190
00:47:16,616 --> 00:47:18,556
app, so let's have a look.


1191
00:47:19,126 --> 00:47:23,166
So once this bust gets into view


1192
00:47:23,166 --> 00:47:25,456
of my iOS app, I get a six


1193
00:47:25,456 --> 00:47:26,996
degrees of freedom pose and can


1194
00:47:26,996 --> 00:47:29,426
use this to show the scene, very


1195
00:47:29,426 --> 00:47:31,246
simple infographics floating


1196
00:47:31,246 --> 00:47:32,026
about the statue.


1197
00:47:32,906 --> 00:47:33,926
In this case, we have simply


1198
00:47:33,926 --> 00:47:36,366
added date of birth, the name of


1199
00:47:36,366 --> 00:47:37,496
this Egyptian queen, which was


1200
00:47:37,496 --> 00:47:39,906
Nefertiti, but you could add any


1201
00:47:39,906 --> 00:47:40,956
content that your rendering


1202
00:47:40,956 --> 00:47:43,576
engine allows you to use.


1203
00:47:43,956 --> 00:47:45,846
In order to build this app, I


1204
00:47:45,896 --> 00:47:47,386
had to scan the object first.


1205
00:47:48,066 --> 00:47:49,266
So let's talk about object


1206
00:47:49,266 --> 00:47:49,756
scanning.


1207
00:47:50,236 --> 00:47:53,306
Object scanning extracts


1208
00:47:53,426 --> 00:47:54,956
accumulated scene information


1209
00:47:55,216 --> 00:47:55,736
from the world.


1210
00:47:56,636 --> 00:47:58,506
This is very much related to


1211
00:47:58,506 --> 00:48:00,736
plane estimation in which we use


1212
00:48:00,776 --> 00:48:02,186
accumulated scene information to


1213
00:48:02,516 --> 00:48:03,566
estimate the position of a


1214
00:48:03,566 --> 00:48:05,106
horizontal or vertical plane.


1215
00:48:06,046 --> 00:48:07,556
In this case, we use this


1216
00:48:08,326 --> 00:48:11,606
information to gather


1217
00:48:11,606 --> 00:48:13,286
information about the 3D object.


1218
00:48:13,776 --> 00:48:17,556
In order to specify which area


1219
00:48:17,556 --> 00:48:19,086
to look for the object, I just


1220
00:48:19,086 --> 00:48:21,326
specify a transform and extend


1221
00:48:21,366 --> 00:48:21,986
in the center.


1222
00:48:22,796 --> 00:48:24,036
This is essentially a bounding


1223
00:48:24,036 --> 00:48:25,746
box around the object just to


1224
00:48:25,846 --> 00:48:29,696
define where it is in the scene.


1225
00:48:29,876 --> 00:48:31,266
Extracted objects are fully


1226
00:48:31,266 --> 00:48:33,126
supported by Xcode's asset


1227
00:48:33,126 --> 00:48:34,736
catalog, so it makes it really


1228
00:48:34,736 --> 00:48:38,486
easy to port them to a new app


1229
00:48:38,556 --> 00:48:40,756
and reuse them as many times as


1230
00:48:40,756 --> 00:48:41,116
you want.


1231
00:48:42,606 --> 00:48:43,986
And for scanning, we added a new


1232
00:48:43,986 --> 00:48:45,266
configuration, the


1233
00:48:45,266 --> 00:48:46,696
ARObjectScanningConfiguration.


1234
00:48:48,156 --> 00:48:49,896
But you do not need to go ahead


1235
00:48:49,896 --> 00:48:51,246
and implement your own scanning


1236
00:48:51,246 --> 00:48:53,826
app as the full sample code is


1237
00:48:53,826 --> 00:48:55,386
available for full-featured


1238
00:48:55,486 --> 00:48:57,616
scanning app called Scanning and


1239
00:48:57,616 --> 00:48:58,746
Detecting 3D Objects.


1240
00:49:00,186 --> 00:49:01,356
So let's have a look how this


1241
00:49:01,396 --> 00:49:02,066
app works.


1242
00:49:02,756 --> 00:49:04,396
I start by creating a bounding


1243
00:49:04,396 --> 00:49:05,786
box around the object of


1244
00:49:05,786 --> 00:49:07,136
interest, in this case, the


1245
00:49:07,136 --> 00:49:07,976
statue of Nefertiti.


1246
00:49:07,976 --> 00:49:10,486
Note that the bounding box does


1247
00:49:10,486 --> 00:49:11,856
not need to be really strict


1248
00:49:11,856 --> 00:49:12,616
around the object.


1249
00:49:12,896 --> 00:49:14,806
All we care is that the most


1250
00:49:14,806 --> 00:49:16,626
important feature points are


1251
00:49:16,766 --> 00:49:17,716
within its bounds.


1252
00:49:19,226 --> 00:49:20,166
When I'm satisfied with the


1253
00:49:20,166 --> 00:49:23,366
bounding box, I can click press


1254
00:49:23,516 --> 00:49:26,056
scan, and we start scanning the


1255
00:49:26,056 --> 00:49:26,416
object.


1256
00:49:27,106 --> 00:49:28,326
I can see the progress going up


1257
00:49:28,326 --> 00:49:30,216
and this tile representation


1258
00:49:30,916 --> 00:49:32,446
indicating how much of the


1259
00:49:32,446 --> 00:49:33,976
object has been scanned in a


1260
00:49:33,976 --> 00:49:34,736
spatial manner.


1261
00:49:35,946 --> 00:49:37,066
Note that you do not have to


1262
00:49:37,166 --> 00:49:38,566
scan the object from all sides.


1263
00:49:39,546 --> 00:49:41,336
For example, if you know that a


1264
00:49:41,436 --> 00:49:43,496
statue will be facing a wall in


1265
00:49:43,496 --> 00:49:46,426
a museum, and there is no way


1266
00:49:46,426 --> 00:49:47,626
that you could detect it from


1267
00:49:47,626 --> 00:49:49,546
one specific viewpoint, you do


1268
00:49:49,546 --> 00:49:51,626
not need to scan it from that


1269
00:49:52,636 --> 00:49:52,756
side.


1270
00:49:53,426 --> 00:49:55,156
Once you're satisfied with the


1271
00:49:55,226 --> 00:50:00,126
scan, you can adjust the center


1272
00:50:00,226 --> 00:50:02,186
of the extent which corresponds


1273
00:50:02,186 --> 00:50:03,336
to the origin of the object.


1274
00:50:04,346 --> 00:50:05,516
The only requirement here is


1275
00:50:05,516 --> 00:50:07,446
that the center stays within the


1276
00:50:07,446 --> 00:50:08,336
object's extent.


1277
00:50:09,006 --> 00:50:12,026
And lastly, the scanning app


1278
00:50:12,026 --> 00:50:13,736
lets you perform detection


1279
00:50:13,736 --> 00:50:13,886
tests.


1280
00:50:14,216 --> 00:50:16,696
So in this case, detection was


1281
00:50:16,696 --> 00:50:18,286
successful from various


1282
00:50:18,286 --> 00:50:19,906
viewpoints, which means it's a


1283
00:50:19,906 --> 00:50:20,406
good scan.


1284
00:50:21,766 --> 00:50:22,796
And our recommendation here is


1285
00:50:22,796 --> 00:50:24,796
also to move the object to


1286
00:50:24,796 --> 00:50:27,186
different location to test


1287
00:50:27,186 --> 00:50:29,676
whether detection works with


1288
00:50:29,676 --> 00:50:31,046
different texture and under


1289
00:50:31,046 --> 00:50:32,046
different lighting conditions.


1290
00:50:34,476 --> 00:50:35,926
Once you're done scanning, you


1291
00:50:35,926 --> 00:50:37,186
will obtain an object of type


1292
00:50:37,296 --> 00:50:39,926
ARReferenceObject, which we have


1293
00:50:40,286 --> 00:50:41,306
seen earlier in the diagram.


1294
00:50:42,296 --> 00:50:44,816
This object can be serialized to


1295
00:50:44,816 --> 00:50:46,286
usually and AR object file


1296
00:50:46,286 --> 00:50:46,956
extension type.


1297
00:50:47,636 --> 00:50:49,306
It has a name, which will also


1298
00:50:49,306 --> 00:50:51,856
be visible in your asset catalog


1299
00:50:52,386 --> 00:50:53,576
as well as the center and the


1300
00:50:53,576 --> 00:50:55,236
extent used for scanning it.


1301
00:50:56,146 --> 00:50:57,666
And you will also get all the


1302
00:50:57,666 --> 00:51:00,016
raw feature points found within


1303
00:51:00,016 --> 00:51:02,236
the area when you performed your


1304
00:51:02,326 --> 00:51:02,486
scan.


1305
00:51:05,386 --> 00:51:06,686
So this was object detection.


1306
00:51:07,256 --> 00:51:08,896
Keep in mind, before detection


1307
00:51:08,896 --> 00:51:10,616
object, you need to scan them,


1308
00:51:10,616 --> 00:51:11,956
but there is the full source


1309
00:51:11,956 --> 00:51:13,196
code available for you to


1310
00:51:13,196 --> 00:51:16,286
download it right now.


1311
00:51:16,406 --> 00:51:18,926
So let's talk next about face


1312
00:51:18,926 --> 00:51:19,266
tracking.


1313
00:51:24,546 --> 00:51:26,296
When we released the iPhone X


1314
00:51:26,296 --> 00:51:28,166
last year, we added robust face


1315
00:51:28,166 --> 00:51:29,776
detection and tracking to ARKit.


1316
00:51:30,746 --> 00:51:32,276
Here, ARKit estimates the


1317
00:51:32,276 --> 00:51:33,816
position and orientation of a


1318
00:51:33,876 --> 00:51:36,186
face for every frame at 60


1319
00:51:36,186 --> 00:51:37,006
frames per second.


1320
00:51:37,826 --> 00:51:39,586
Here we can use the, this pose


1321
00:51:39,586 --> 00:51:41,576
can be used to augment a user's


1322
00:51:41,576 --> 00:51:44,206
face by adding masks, hats, or


1323
00:51:44,206 --> 00:51:45,746
replace the full texture of a


1324
00:51:46,776 --> 00:51:46,906
face.


1325
00:51:47,816 --> 00:51:48,956
ARKit also provides you with a


1326
00:51:48,956 --> 00:51:50,536
fitted triangle mesh coming to


1327
00:51:50,536 --> 00:51:52,016
form of the ARFaceGeometry.


1328
00:51:52,566 --> 00:51:56,136
This type, the ARFaceGeometry,


1329
00:51:56,136 --> 00:51:57,636
contains all the information


1330
00:51:57,636 --> 00:51:59,326
needed to render this facial


1331
00:51:59,326 --> 00:52:01,916
mesh, and it comes in the form


1332
00:52:01,976 --> 00:52:05,566
of all vertices, triangles, as


1333
00:52:05,566 --> 00:52:07,296
well as detection coordinates.


1334
00:52:08,556 --> 00:52:10,356
The main anchor type of face


1335
00:52:10,356 --> 00:52:12,186
tracking is ARFaceAnchor, which


1336
00:52:12,186 --> 00:52:13,876
contains all information needed


1337
00:52:13,876 --> 00:52:15,266
to perform face tracking.


1338
00:52:16,666 --> 00:52:18,486
And in order to render such


1339
00:52:18,486 --> 00:52:20,886
geometry realistically, we added


1340
00:52:20,956 --> 00:52:22,506
a directional light estimate.


1341
00:52:23,616 --> 00:52:25,426
Here, ARKit uses your light as a


1342
00:52:25,426 --> 00:52:27,786
light probe and estimates this


1343
00:52:28,616 --> 00:52:30,986
ARDirectionLightEstimate, which


1344
00:52:31,316 --> 00:52:33,026
consists of the light intensity,


1345
00:52:33,176 --> 00:52:34,696
direction, as well as the color


1346
00:52:34,696 --> 00:52:35,096
temperature.


1347
00:52:36,086 --> 00:52:39,066
This estimate will be sufficient


1348
00:52:39,346 --> 00:52:41,406
to make most apps already look


1349
00:52:41,506 --> 00:52:43,456
great, but if your app has more


1350
00:52:43,456 --> 00:52:45,896
sophisticated needs, we also


1351
00:52:45,896 --> 00:52:47,686
provide the second-degree


1352
00:52:47,686 --> 00:52:48,986
spherical harmonics coefficients


1353
00:52:49,396 --> 00:52:51,046
that gather lighting conditions


1354
00:52:51,086 --> 00:52:53,266
throughout the entire scene for


1355
00:52:53,356 --> 00:52:55,606
you to make your content even


1356
00:52:56,216 --> 00:52:57,726
look better.


1357
00:52:57,926 --> 00:52:59,106
And ARKit can also track


1358
00:52:59,106 --> 00:53:00,426
expressions in real-time.


1359
00:53:01,176 --> 00:53:02,436
These expressions are so-called


1360
00:53:02,806 --> 00:53:05,236
blend shapes, and there's 50 or


1361
00:53:05,306 --> 00:53:06,616
more of them.


1362
00:53:08,186 --> 00:53:09,296
Such a blend shape assume a


1363
00:53:09,296 --> 00:53:10,516
value between 0 and 1.


1364
00:53:11,236 --> 00:53:12,356
One means there's full


1365
00:53:12,356 --> 00:53:13,116
activation.


1366
00:53:13,116 --> 00:53:13,856
Zero means there is none.


1367
00:53:14,386 --> 00:53:15,666
For example, the jaw


1368
00:53:15,666 --> 00:53:17,836
open coefficient will assume a


1369
00:53:17,836 --> 00:53:19,156
value close to 1 if I open my


1370
00:53:19,156 --> 00:53:20,796
mouth and a value close to 0 if


1371
00:53:20,796 --> 00:53:21,236
I close it.


1372
00:53:22,106 --> 00:53:24,456
And this is great to animate


1373
00:53:24,456 --> 00:53:25,416
your own virtual character.


1374
00:53:26,186 --> 00:53:27,706
This example here, I've used the


1375
00:53:27,706 --> 00:53:29,466
jaw open and eye blink left and


1376
00:53:29,466 --> 00:53:31,236
eye blink right to animate this


1377
00:53:31,236 --> 00:53:32,276
really simple box face


1378
00:53:32,276 --> 00:53:32,616
character.


1379
00:53:34,156 --> 00:53:35,276
But it can do better than that.


1380
00:53:36,286 --> 00:53:38,156
In fact, when we built Animoji,


1381
00:53:38,156 --> 00:53:39,856
we used a handful more of such


1382
00:53:39,856 --> 00:53:40,406
blend shapes.


1383
00:53:41,176 --> 00:53:42,256
So all the blue bars you see


1384
00:53:42,256 --> 00:53:44,446
moving here were used to get


1385
00:53:44,446 --> 00:53:46,106
over the head post to map my


1386
00:53:46,106 --> 00:53:47,856
facial expressions on the panda


1387
00:53:47,856 --> 00:53:48,076
bear.


1388
00:53:49,706 --> 00:53:50,986
Note that ARKit offers


1389
00:53:51,056 --> 00:53:52,616
everything needed for you to


1390
00:53:52,616 --> 00:53:54,896
animate your own character just


1391
00:53:54,896 --> 00:53:56,086
like we did with Animoji.


1392
00:53:56,696 --> 00:53:58,036
Thank you.


1393
00:53:59,516 --> 00:54:04,276
[ Applause ]


1394
00:54:04,776 --> 00:54:06,196
So let's see what's new for face


1395
00:54:06,196 --> 00:54:07,736
tracking in ARKit 2.


1396
00:54:08,736 --> 00:54:11,106
We added gaze tracking that will


1397
00:54:11,106 --> 00:54:12,826
track the left and the right eye


1398
00:54:12,966 --> 00:54:15,326
both in six degrees of freedom.


1399
00:54:16,516 --> 00:54:21,046
[ Applause ]


1400
00:54:21,546 --> 00:54:22,646
You will find these properties


1401
00:54:22,646 --> 00:54:24,956
as members of ARFaceAnchor as


1402
00:54:24,956 --> 00:54:26,516
well as a look-at point, this


1403
00:54:26,516 --> 00:54:28,346
corresponds to reintersection of


1404
00:54:28,346 --> 00:54:29,316
the two gaze directions.


1405
00:54:30,326 --> 00:54:32,256
You may use this information to


1406
00:54:32,296 --> 00:54:34,076
animate again your own character


1407
00:54:34,496 --> 00:54:36,456
or of any other form of input to


1408
00:54:36,456 --> 00:54:36,796
your app.


1409
00:54:36,796 --> 00:54:38,446
And there's more.


1410
00:54:39,716 --> 00:54:40,816
We added support for tongue,


1411
00:54:42,176 --> 00:54:43,186
which comes in the form of a new


1412
00:54:43,186 --> 00:54:43,746
blend shape.


1413
00:54:44,636 --> 00:54:45,566
This blend shape will assume a


1414
00:54:45,566 --> 00:54:47,786
value of 1 if my tongue is out 0


1415
00:54:48,096 --> 00:54:48,516
if not.


1416
00:54:49,516 --> 00:54:51,276
Again, you could use this to


1417
00:54:51,276 --> 00:54:53,086
animate your own character or


1418
00:54:53,086 --> 00:54:55,896
use this as a form of input to


1419
00:54:55,896 --> 00:54:56,946
your app.


1420
00:55:00,636 --> 00:55:01,086
Thank you.


1421
00:55:01,821 --> 00:55:03,821
[ Applause ]


1422
00:55:04,126 --> 00:55:05,216
So seeing myself sticking my


1423
00:55:05,216 --> 00:55:06,766
tongue out over and over is a


1424
00:55:06,766 --> 00:55:07,936
good time for summary.


1425
00:55:09,086 --> 00:55:11,796
So, what's new in ARKit 2.


1426
00:55:11,796 --> 00:55:12,966
Let's have a look.


1427
00:55:13,816 --> 00:55:15,306
We've seen saving and loading


1428
00:55:15,306 --> 00:55:16,606
maps, which are powerful new


1429
00:55:16,606 --> 00:55:18,016
features for persistence and


1430
00:55:18,206 --> 00:55:20,436
multiuser collaboration.


1431
00:55:21,726 --> 00:55:23,086
World tracking enhancements


1432
00:55:23,506 --> 00:55:25,236
simply shows better and fasting


1433
00:55:25,236 --> 00:55:27,746
plane detection as well as new


1434
00:55:27,746 --> 00:55:29,356
video formats.


1435
00:55:29,566 --> 00:55:31,056
And with environment texturing,


1436
00:55:31,726 --> 00:55:34,256
we can make the content really


1437
00:55:34,506 --> 00:55:35,876
look as if it was really in the


1438
00:55:35,976 --> 00:55:38,526
scene by gathering texture of


1439
00:55:38,526 --> 00:55:39,876
the scene and applying it as a


1440
00:55:39,906 --> 00:55:40,616
textured object.


1441
00:55:41,846 --> 00:55:44,136
And with image tracking, with


1442
00:55:44,836 --> 00:55:48,196
image tracking, we are now able


1443
00:55:48,196 --> 00:55:50,516
to track 2D objects in the form


1444
00:55:50,516 --> 00:55:50,986
of images.


1445
00:55:51,506 --> 00:55:53,236
But ARKit can also detect 3D


1446
00:55:53,236 --> 00:55:53,586
objects.


1447
00:55:54,576 --> 00:55:56,386
And for face tracking, we have


1448
00:55:56,916 --> 00:55:59,116
gaze and tongue.


1449
00:55:59,926 --> 00:56:01,226
All of this is made available


1450
00:56:01,226 --> 00:56:02,736
for you in the form of the


1451
00:56:02,736 --> 00:56:04,256
building blocks of ARKit.


1452
00:56:05,336 --> 00:56:08,016
In iOS 12, ARKit features five


1453
00:56:08,066 --> 00:56:09,456
different configurations with


1454
00:56:09,456 --> 00:56:10,876
two new additions, the


1455
00:56:10,876 --> 00:56:12,556
ARImageTrackingConfiguration for


1456
00:56:12,556 --> 00:56:15,116
stand-alone image tracking and


1457
00:56:15,196 --> 00:56:15,306
the


1458
00:56:15,306 --> 00:56:16,766
ARObjectScanningConfiguration.


1459
00:56:18,056 --> 00:56:19,146
And there's a series of


1460
00:56:19,746 --> 00:56:21,456
supplementary types used to


1461
00:56:21,456 --> 00:56:22,716
interact with the AR session.


1462
00:56:23,426 --> 00:56:25,036
The ARFrame, the ARCamera, for


1463
00:56:25,166 --> 00:56:25,486
example.


1464
00:56:26,686 --> 00:56:27,906
And this got two new additions,


1465
00:56:27,996 --> 00:56:29,546
the ARReferenceObject for object


1466
00:56:29,546 --> 00:56:31,706
detection and the ARWorldMap for


1467
00:56:31,766 --> 00:56:33,366
persistence and multiuser.


1468
00:56:33,896 --> 00:56:36,306
And the AR anchors, which


1469
00:56:36,306 --> 00:56:37,836
represent positions in the real


1470
00:56:37,836 --> 00:56:39,276
world, the anchor types.


1471
00:56:39,586 --> 00:56:41,166
Got two new additions, the


1472
00:56:41,166 --> 00:56:43,016
ARObjectAnchor and the


1473
00:56:43,016 --> 00:56:44,176
AREnvironmentProbeAnchor.


1474
00:56:45,236 --> 00:56:46,976
I'm really excited to see what


1475
00:56:46,976 --> 00:56:48,156
you guys will build with all


1476
00:56:48,156 --> 00:56:49,226
these building blocks in the


1477
00:56:49,226 --> 00:56:51,886
ARKit available in iOS 12 as of


1478
00:56:51,936 --> 00:56:52,176
today.


1479
00:56:54,516 --> 00:57:00,506
[ Applause ]


1480
00:57:01,006 --> 00:57:01,846
There's another real cool


1481
00:57:01,906 --> 00:57:03,286
session about integrating


1482
00:57:03,286 --> 00:57:04,656
ARQuickLook into your own


1483
00:57:04,656 --> 00:57:06,596
application to make your content


1484
00:57:06,596 --> 00:57:07,396
look simply great.


1485
00:57:09,026 --> 00:57:11,396
With this, thanks a lot, and


1486
00:57:11,396 --> 00:57:11,906
enjoy the rest of the


1487
00:57:11,906 --> 00:57:12,256
conference.


1488
00:57:13,508 --> 00:57:15,508
[ Applause ]

