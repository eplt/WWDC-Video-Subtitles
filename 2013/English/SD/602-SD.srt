1
00:00:00,506 --> 00:00:10,706
[ Silence ]


2
00:00:11,206 --> 00:00:11,886
>> Good morning everyone.


3
00:00:12,496 --> 00:00:14,316
My name is Tony Guetta
and I'm the Manager


4
00:00:14,316 --> 00:00:15,576
in the Core Audio
group at Apple.


5
00:00:15,716 --> 00:00:17,186
And today, I'm going
to talk to you about,


6
00:00:17,186 --> 00:00:21,356
What's New in Core
Audio for iOS.


7
00:00:21,476 --> 00:00:23,706
We're going to begin with a
very high level overview of some


8
00:00:23,756 --> 00:00:25,466
of the new audio
features in iOS 7.


9
00:00:25,626 --> 00:00:28,106
And for the majority of this
session, we're going to talk--


10
00:00:28,106 --> 00:00:30,556
spend our time focused on one
new technology in particular


11
00:00:30,766 --> 00:00:32,305
that we think you're going
to be very excited about.


12
00:00:32,625 --> 00:00:34,896
So, let's dive in to the
list of new features.


13
00:00:35,906 --> 00:00:39,756
First is Audio Input Selection
and with input selection,


14
00:00:39,886 --> 00:00:41,986
your application now has
the ability to specify


15
00:00:41,986 --> 00:00:44,696
which audio input it would like
to use in certain situations.


16
00:00:44,736 --> 00:00:48,506
So for example, if the user had
a wired headset plugged into his


17
00:00:48,506 --> 00:00:50,706
or her device, but your
app wanted to continue


18
00:00:50,706 --> 00:00:52,396
to use the built-in
microphone for input,


19
00:00:52,636 --> 00:00:55,506
you now have the
capability to control that.


20
00:00:56,166 --> 00:00:58,436
With input selection, you can
also choose which microphone


21
00:00:58,436 --> 00:01:00,456
that you'd like to use on
our multi-mic platforms


22
00:01:00,996 --> 00:01:03,426
and on devices that support
it such as the iPhone 5.


23
00:01:03,846 --> 00:01:04,616
You can take advantage


24
00:01:04,616 --> 00:01:06,136
of microphone beam
forming processing


25
00:01:06,636 --> 00:01:08,846
to set an effective
microphone directivity


26
00:01:08,976 --> 00:01:12,096
by specifying a polar pattern
such as cardioid or subcardioid.


27
00:01:14,716 --> 00:01:17,176
We've made some enhancements
to multichannel audio on iOS 7.


28
00:01:17,556 --> 00:01:19,536
And through the use of
the AVAudioSession API,


29
00:01:20,236 --> 00:01:22,266
you can now discover the
maximum number of input


30
00:01:22,266 --> 00:01:23,446
and output channels
that are supported


31
00:01:23,446 --> 00:01:25,656
by the current audio route
as well as being able


32
00:01:25,656 --> 00:01:29,096
to specify your preferred number
of input and output channels.


33
00:01:29,096 --> 00:01:31,006
For audio outputs
supported such as HDMI,


34
00:01:31,906 --> 00:01:33,786
you can obtain audio
channel labels


35
00:01:33,786 --> 00:01:35,806
which associate a
particular audio channel


36
00:01:36,126 --> 00:01:38,196
with the description of a
physical speaker location


37
00:01:38,196 --> 00:01:41,486
such as front left, front
right, center and so on.


38
00:01:43,236 --> 00:01:44,916
We've added some
extensions to Open AL


39
00:01:44,916 --> 00:01:47,886
to enhance the gaming audio
experience in iOS 7 starting


40
00:01:47,886 --> 00:01:51,036
with the ability to specify a
spatialization rendering quality


41
00:01:51,086 --> 00:01:52,376
on a per-sound source basis.


42
00:01:53,126 --> 00:01:54,046
Now, you might use this


43
00:01:54,046 --> 00:01:56,196
to specify a very high
quality rendering algorithm


44
00:01:56,196 --> 00:01:58,146
for the important sound
sources in your game.


45
00:01:58,396 --> 00:02:00,066
But a less CPU intensive
algorithm


46
00:02:00,066 --> 00:02:02,036
for those less importance
sound sources in your game.


47
00:02:03,576 --> 00:02:04,816
We've also made some
improvements


48
00:02:04,816 --> 00:02:06,996
to our high quality
spatialization


49
00:02:06,996 --> 00:02:07,676
rendering algorithm.


50
00:02:08,326 --> 00:02:10,366
And also added the ability
to support rendering


51
00:02:10,366 --> 00:02:12,356
to multichannel output
hardware when it's available.


52
00:02:13,326 --> 00:02:15,016
Finally, we've added
an extension


53
00:02:15,016 --> 00:02:16,116
to allow capturing the output


54
00:02:16,116 --> 00:02:18,166
of the current Open AL
3D rendering context.


55
00:02:18,666 --> 00:02:22,666
We've added time-pitch
capabilities to Audio Queue.


56
00:02:22,916 --> 00:02:25,496
So your application can now
control the speed up and slow


57
00:02:25,496 --> 00:02:27,266
down of Audio Queue
playback both in terms


58
00:02:27,266 --> 00:02:28,576
of time and in frequency.


59
00:02:31,276 --> 00:02:34,176
We've enhanced the security
around audio recording in iOS 7


60
00:02:34,546 --> 00:02:37,456
and we now require explicit user
approval before your application


61
00:02:37,456 --> 00:02:38,336
can do audio input.


62
00:02:38,966 --> 00:02:40,166
Now, the reason for
doing this is


63
00:02:40,166 --> 00:02:42,146
to prevent a malicious
application from being able


64
00:02:42,146 --> 00:02:43,876
to record a user without
him or her knowing it.


65
00:02:45,046 --> 00:02:46,976
The way that this works
is very similar to the way


66
00:02:46,976 --> 00:02:49,876
that the location service's
permission mechanism works.


67
00:02:49,876 --> 00:02:50,976
In that the user is presented


68
00:02:50,976 --> 00:02:53,206
with a model dialog
requesting his or her permission


69
00:02:53,206 --> 00:02:54,096
to use the audio input.


70
00:02:54,486 --> 00:02:58,006
The decision is made on
per-application basis


71
00:02:58,076 --> 00:02:59,196
and it is a one-time decision.


72
00:02:59,726 --> 00:03:01,976
However, if you'd like to
go in and change your mind


73
00:03:01,976 --> 00:03:03,206
at a later time,
you can always go


74
00:03:03,206 --> 00:03:04,816
into the Settings
application to do that.


75
00:03:06,666 --> 00:03:08,776
Until the user has given
your application permission


76
00:03:08,776 --> 00:03:11,186
to use audio input, you
will get silence so you need


77
00:03:11,186 --> 00:03:12,276
to be prepared to handle that.


78
00:03:12,276 --> 00:03:15,876
Now, what actually triggers
the dialog from being presented


79
00:03:15,876 --> 00:03:17,976
to the user is an attempt
by your application


80
00:03:17,976 --> 00:03:20,626
to use an audio session
category that would enable input


81
00:03:20,626 --> 00:03:22,546
such as the record
category or play and record.


82
00:03:23,596 --> 00:03:26,076
However, if you'd
like to have control


83
00:03:26,076 --> 00:03:27,856
over when the user is
presented with his dialog


84
00:03:28,016 --> 00:03:29,966
so that it can happen
at a more opportune time


85
00:03:29,966 --> 00:03:32,256
for your application,
we've added some API


86
00:03:32,446 --> 00:03:35,426
and AVAudioSession for
you to be able to do that.


87
00:03:38,226 --> 00:03:39,976
Finally, just a note on
the AudioSession API.


88
00:03:40,426 --> 00:03:42,766
As we mentioned at last year's
conference, the C version


89
00:03:42,766 --> 00:03:45,316
of the AudioSession API is
officially being deprecated


90
00:03:45,316 --> 00:03:46,086
in iOS 7.


91
00:03:46,326 --> 00:03:48,386
So, we hope that over the
course of the past year,


92
00:03:48,576 --> 00:03:50,416
you've all had the opportunity
to move your applications


93
00:03:50,416 --> 00:03:52,286
over to using the
AVAudioSession API.


94
00:03:55,696 --> 00:03:57,976
So, here is a summary of the
features that we just discussed.


95
00:03:57,976 --> 00:04:00,116
We're not going to spend anymore
time today going over any


96
00:04:00,116 --> 00:04:01,446
of these topics in
any more detail.


97
00:04:01,616 --> 00:04:03,326
So, if you have any
questions about these


98
00:04:03,416 --> 00:04:05,636
or like a more detailed
overview of any of these items,


99
00:04:06,026 --> 00:04:08,036
we encourage you to come by
our labs either later today


100
00:04:08,316 --> 00:04:10,106
or tomorrow morning and
we'd be happy to discuss


101
00:04:10,106 --> 00:04:10,936
with you in more detail.


102
00:04:11,666 --> 00:04:14,046
I'd also encourage you to have
a look at the documentation


103
00:04:14,046 --> 00:04:16,046
in the various header files
that I outlined in the course


104
00:04:16,046 --> 00:04:17,276
of going through
each of these topics.


105
00:04:18,276 --> 00:04:21,526
So for the remainder of this
session, we're going to focus


106
00:04:21,526 --> 00:04:23,546
on one new technology in
particular that again,


107
00:04:23,856 --> 00:04:25,406
we think you're going
to be very excited about


108
00:04:25,406 --> 00:04:28,046
and that's Inter-App Audio.


109
00:04:28,516 --> 00:04:29,926
So what is Inter-App Audio?


110
00:04:30,286 --> 00:04:32,856
Well, as the name implies,


111
00:04:32,856 --> 00:04:34,996
Inter-App Audio provides
the ability to stream audio


112
00:04:34,996 --> 00:04:36,406
between applications
in real-time.


113
00:04:36,756 --> 00:04:39,326
So, if you have a really cool
effects application and you want


114
00:04:39,326 --> 00:04:40,806
to integrate that into
your DAW application,


115
00:04:40,906 --> 00:04:42,726
you now have the
ability to do that.


116
00:04:43,266 --> 00:04:46,146
We've built Inter-App Audio on
top of existing Core Audio APIs


117
00:04:46,146 --> 00:04:47,986
so it should be very
easy for you to integrate


118
00:04:47,986 --> 00:04:49,776
into your existing applications


119
00:04:49,776 --> 00:04:52,616
and deploy quickly
to the app store.


120
00:04:52,616 --> 00:04:54,326
Because it's built into
the operating system,


121
00:04:54,526 --> 00:04:56,776
the solution is very efficient
with zero additional latency


122
00:04:57,296 --> 00:05:00,026
and should provide for a stable
platform for the evolution


123
00:05:00,026 --> 00:05:00,796
of the feature over time.


124
00:05:01,706 --> 00:05:03,476
Now, before we get into any
of the technical details


125
00:05:03,476 --> 00:05:06,406
of how Inter-App Audio works,
I'd like to invite up Alec


126
00:05:06,636 --> 00:05:09,106
from the GarageBand
team to give you a demo.


127
00:05:09,181 --> 00:05:11,181
[ Applause ]


128
00:05:11,256 --> 00:05:16,566
>> Thanks Tony.


129
00:05:17,056 --> 00:05:17,476
Am I up?


130
00:05:17,766 --> 00:05:17,986
>> Yeah.


131
00:05:18,416 --> 00:05:22,996
>> My name is Alec, I am a
product designer for GarageBand


132
00:05:22,996 --> 00:05:26,296
and Logic and I'm going to
switch over here to my iPad.


133
00:05:27,106 --> 00:05:30,496
So, what I want to do today
is give a quick demonstration


134
00:05:30,546 --> 00:05:33,876
about how we have been working
with the development version,


135
00:05:33,876 --> 00:05:36,016
kind of a sneak peek into
a development version


136
00:05:36,016 --> 00:05:38,976
of GarageBand and how we're
doing some experiments


137
00:05:39,096 --> 00:05:40,096
with Inter-App Audio.


138
00:05:40,936 --> 00:05:43,946
So, what I have up here is
just a simple FourTrack song


139
00:05:43,946 --> 00:05:45,736
in GarageBand, I'm going
to play a little bit


140
00:05:45,736 --> 00:05:47,396
so you can get an idea
of what it sounds like.


141
00:05:47,396 --> 00:05:47,936
[ Music ]


142
00:05:47,936 --> 00:05:56,386
OK. So the first thing
I want to do is I want


143
00:05:56,386 --> 00:05:58,556
to add a little keyboard
part to this.


144
00:05:59,056 --> 00:06:01,736
But instead of using one
of the built-in instruments


145
00:06:01,736 --> 00:06:05,946
in GarageBand, I want to use
an instrument, on the system


146
00:06:05,946 --> 00:06:07,216
that is not part of GarageBand.


147
00:06:07,216 --> 00:06:08,806
So to do that, I'm going to go


148
00:06:08,806 --> 00:06:10,666
out to the GarageBand
instrument browser.


149
00:06:10,776 --> 00:06:13,356
Now, what we see here are
the instruments that ship


150
00:06:13,356 --> 00:06:15,096
with GarageBand,
part of GarageBand.


151
00:06:15,576 --> 00:06:17,976
And then we have a new
icon here, Music Apps.


152
00:06:18,266 --> 00:06:22,536
I'm going to tap on that and
we see the icons of other apps


153
00:06:22,536 --> 00:06:26,466
on the system which
are audio apps.


154
00:06:26,516 --> 00:06:30,116
So, I'm going to click on
sampler one here and we'll see


155
00:06:30,116 --> 00:06:32,556
that the sampler launches
in the background.


156
00:06:33,126 --> 00:06:36,976
Now here it with the UI in the
foreground and we can hear it.


157
00:06:37,936 --> 00:06:39,906
Now, you see there's a
transport here and that's,


158
00:06:39,906 --> 00:06:42,626
this transport is remotely
controlling the transport


159
00:06:42,626 --> 00:06:43,386
of GarageBand.


160
00:06:43,556 --> 00:06:45,456
So, when I press the record
button, what we're going


161
00:06:45,456 --> 00:06:48,716
to hear is a count off from
GarageBand and then the track


162
00:06:48,716 --> 00:06:53,856
that I just played and I'll
record over the top of it.


163
00:06:54,356 --> 00:07:01,376ddle
[ Music ]


164
00:06:54,356 --> 00:07:01,376
[ Music ]


165
00:07:01,876 --> 00:07:03,236
Brilliant musical passage.


166
00:07:04,816 --> 00:07:07,996
So now, if I-- if you look up at
this transport again you'll see


167
00:07:07,996 --> 00:07:09,736
that there's a GarageBand icon.


168
00:07:10,366 --> 00:07:12,916
When I tap on that
icon, I switch back


169
00:07:12,916 --> 00:07:16,756
to the GarageBand application
and now, in the tracks view,


170
00:07:17,126 --> 00:07:20,456
a new track has been added
with this little keyboard part


171
00:07:20,456 --> 00:07:26,846
that I played we
can listen to it.


172
00:07:26,936 --> 00:07:30,346
[Background Music] And
add some keyboard to it.


173
00:07:30,346 --> 00:07:30,896
[ Music ]


174
00:07:30,896 --> 00:07:33,746
So, that was bringing audio
from another application,


175
00:07:33,746 --> 00:07:35,986
controlling that
application in its interface,


176
00:07:35,986 --> 00:07:37,486
and recording that
in GarageBand.


177
00:07:38,096 --> 00:07:39,626
The next thing I
want to do is I want


178
00:07:39,626 --> 00:07:43,066
to process an input
from GarageBand.


179
00:07:43,066 --> 00:07:47,736
So, I'm going to put on my
little guitar here and we'll go


180
00:07:47,736 --> 00:07:50,896
to the guitar amp in GarageBand.


181
00:07:51,426 --> 00:07:53,516
Now, this guitar
amp is part of--


182
00:07:53,516 --> 00:07:57,466
one of the instruments built
in the GarageBand and I'm going


183
00:07:57,466 --> 00:07:59,856
to turn on input monitoring
so I can hear myself.


184
00:08:00,616 --> 00:08:04,886
[Background Music] You
guys got it out there?


185
00:08:07,456 --> 00:08:10,076
It's a little phase switch.


186
00:08:13,456 --> 00:08:15,586
OK. Or we're going to--


187
00:08:15,586 --> 00:08:18,176
there you go, that's
more rock and roll.


188
00:08:18,686 --> 00:08:21,186
OK. So, that's a
good sound right?


189
00:08:21,186 --> 00:08:23,316
That's using the guitar
app from GarageBand.


190
00:08:23,316 --> 00:08:25,456
What I want to do though
is I want to process it


191
00:08:25,626 --> 00:08:27,476
with another effect
on my system.


192
00:08:27,926 --> 00:08:31,426
So again, I'm going to go into
the input settings in GarageBand


193
00:08:31,426 --> 00:08:32,416
and if you see about halfway


194
00:08:32,416 --> 00:08:34,466
down this list, it
says Effect App.


195
00:08:34,466 --> 00:08:38,076
I'm going to tap on that and
we can see a list of apps


196
00:08:38,076 --> 00:08:40,525
on my system that are
effects so I'm going


197
00:08:40,525 --> 00:08:42,046
to click on this Audio Delay.


198
00:08:42,905 --> 00:08:45,606
[Music] So, there is the delay


199
00:08:45,656 --> 00:08:47,366
but it's not really
the settings I want.


200
00:08:47,496 --> 00:08:51,276
So, I'm going to tap on
the Effect icon and switch


201
00:08:51,276 --> 00:08:52,546
to the Effects Interface.


202
00:08:53,086 --> 00:08:54,636
I'm going to take the feedback


203
00:08:54,636 --> 00:08:58,356
down here a little
bit and the mix.


204
00:08:59,016 --> 00:08:59,196
[Music] OK.


205
00:09:00,266 --> 00:09:04,836
So that's a little bit better.


206
00:09:04,836 --> 00:09:06,726
So now, what I'm doing
is I'm taking the input


207
00:09:07,296 --> 00:09:09,316
through GarageBand, sending
it out to this effect


208
00:09:09,686 --> 00:09:11,476
and bringing it back
on to GarageBand.


209
00:09:12,226 --> 00:09:13,676
Then I can hit record.


210
00:09:13,676 --> 00:09:14,256
[ Music ]


211
00:09:14,256 --> 00:09:54,026
Now, if we switch back
to the tracks view,


212
00:09:55,626 --> 00:09:57,406
we can see that new
region has been recorded


213
00:09:57,406 --> 00:09:59,606
in GarageBand and if I play.


214
00:10:00,106 --> 00:10:03,576
[ Music ]


215
00:10:04,076 --> 00:10:08,566
There is the source
with the delay added


216
00:10:08,676 --> 00:10:10,676
to it recorded in
the GarageBand.


217
00:10:14,816 --> 00:10:17,626
So that's just the
quick overview


218
00:10:17,626 --> 00:10:19,536
of how we're doing some
experiments inside this


219
00:10:19,536 --> 00:10:21,096
development version
of GarageBand


220
00:10:21,186 --> 00:10:24,056
with the new Inter-App
Audio APIs.


221
00:10:24,056 --> 00:10:25,976
And next, we're going
to bring up Doug


222
00:10:25,976 --> 00:10:27,996
to give you a little more
detail about how some


223
00:10:28,136 --> 00:10:36,726
of the stuff works
under the hood.


224
00:10:36,726 --> 00:10:36,793
[ Applause ]


225
00:10:36,793 --> 00:10:39,286
>> Thank you Alec.


226
00:10:39,286 --> 00:10:42,096
Hi, my name is Doug Wyatt, I'm a
plumber in the Core Audio group.


227
00:10:42,686 --> 00:10:44,896
I'd like to present to
you some of the details


228
00:10:44,896 --> 00:10:46,776
of the Inter-App Audio APIs.


229
00:10:48,376 --> 00:10:52,956
So, conceptually here, we
have two kinds of applications


230
00:10:52,956 --> 00:10:54,746
which we call the
host application


231
00:10:55,146 --> 00:10:56,476
and the node application.


232
00:10:57,296 --> 00:10:58,536
The fundamental distinction


233
00:10:58,536 --> 00:11:01,556
between these two
applications is that the host is


234
00:11:01,556 --> 00:11:04,066
where we ultimately
want the audio coming


235
00:11:04,066 --> 00:11:06,296
from the node application
to end up.


236
00:11:06,296 --> 00:11:09,766
So, GarageBand in this
example was a host application.


237
00:11:09,766 --> 00:11:12,626
It was receiving audio from
the sampler application


238
00:11:13,066 --> 00:11:15,256
and from the delay
effect application.


239
00:11:15,816 --> 00:11:18,396
So, given these two
kinds of applications,


240
00:11:18,916 --> 00:11:20,256
we're going to look at APIs


241
00:11:20,256 --> 00:11:22,776
for how node applications
can register themselves


242
00:11:22,776 --> 00:11:26,306
with the system and how host
applications can discover those


243
00:11:26,306 --> 00:11:27,916
registered node applications.


244
00:11:28,816 --> 00:11:32,206
We'll look at how
host applications can


245
00:11:32,206 --> 00:11:35,766
of initiate connections through
the system to node applications.


246
00:11:35,766 --> 00:11:38,476
And once those connections
are established,


247
00:11:38,846 --> 00:11:41,476
the two applications can stream
audio between each other.


248
00:11:41,846 --> 00:11:45,986
But, again, primarily the
destination has to be the host


249
00:11:46,036 --> 00:11:47,826
that could optionally
send the audio to the node


250
00:11:47,826 --> 00:11:49,886
if the node is providing
an effect.


251
00:11:50,506 --> 00:11:54,266
We'll look at how furthermore
host applications can send MIDI


252
00:11:54,266 --> 00:11:57,576
events to node applications to
control their audio rendering.


253
00:11:58,106 --> 00:12:01,656
So for example, with
that sampler application,


254
00:12:01,656 --> 00:12:04,106
the host could have been
actually sending the MIDI nodes


255
00:12:04,556 --> 00:12:08,946
to the sampler and receiving
the rendered audio back.


256
00:12:09,556 --> 00:12:11,766
We'll look at some interfaces


257
00:12:11,766 --> 00:12:14,226
where the host can
express information


258
00:12:14,226 --> 00:12:17,016
about its transport
controls and transport state


259
00:12:17,016 --> 00:12:19,826
and timeline position
to node applications.


260
00:12:20,036 --> 00:12:21,306
And finally, we'll look


261
00:12:21,306 --> 00:12:23,866
at how node applications
can remotely control


262
00:12:23,866 --> 00:12:24,976
host applications.


263
00:12:24,976 --> 00:12:28,416
So, let's look inside
host applications.


264
00:12:28,956 --> 00:12:31,636
So, this is your
basic standalone music


265
00:12:31,636 --> 00:12:33,516
or audio application on iOS.


266
00:12:34,106 --> 00:12:38,366
We have AURemoteIO audio unit
and its function is to connect


267
00:12:38,466 --> 00:12:41,606
to the audio input and
output system with zero--


268
00:12:41,606 --> 00:12:45,876
well, very low latency using
pretty much the same mechanisms


269
00:12:45,876 --> 00:12:49,426
as on the desktop but always
through the RemoteIO audio unit.


270
00:12:49,786 --> 00:12:53,506
So, feeding the audio unit, we
have the host's audio engine


271
00:12:53,506 --> 00:12:55,696
and that can be constructed
in a number of ways,


272
00:12:55,696 --> 00:12:57,316
we'll show some examples later.


273
00:12:58,016 --> 00:13:00,196
But for the-- the purposes
of Inter-App Audio,


274
00:13:01,176 --> 00:13:04,206
the host engine connects
to a node application


275
00:13:04,786 --> 00:13:06,806
by instantiating
a node audio unit.


276
00:13:07,216 --> 00:13:10,976
This is another Apple supplied
audio unit that, in effect,


277
00:13:11,426 --> 00:13:14,546
creates the bridge to the
remote node application


278
00:13:14,606 --> 00:13:16,546
to communicate audio with it.


279
00:13:17,916 --> 00:13:21,256
Now, on the node side, a
node application is also


280
00:13:21,686 --> 00:13:25,796
by default a normal application
with an AURemoteIO that can play


281
00:13:25,796 --> 00:13:29,366
and record as always and it's
got its own audio engine.


282
00:13:30,386 --> 00:13:33,086
What's a little different here
is in the Inter-App scenario,


283
00:13:33,086 --> 00:13:36,976
the node application has its
input and output redirected


284
00:13:36,976 --> 00:13:40,236
from the mic and speaker
to the host application.


285
00:13:40,866 --> 00:13:44,646
So, that's the node application.


286
00:13:45,636 --> 00:13:48,996
So, you see then we're
implementing this API


287
00:13:48,996 --> 00:13:50,966
as a series of extensions


288
00:13:50,966 --> 00:13:54,276
to the existing
AudioUnit.framework APIs.


289
00:13:54,276 --> 00:13:57,496
The host sees the node
application as an audio unit


290
00:13:57,496 --> 00:13:58,546
that it communicates with


291
00:13:59,206 --> 00:14:04,536
and the nodes AURemoteIO unit
gets redirected to the host


292
00:14:04,596 --> 00:14:06,226
so the node's communication


293
00:14:06,226 --> 00:14:09,226
to the host application
is through that IO unit.


294
00:14:11,696 --> 00:14:15,486
So, to express the capabilities
of these node applications


295
00:14:15,486 --> 00:14:17,346
and to distinguish them a bit


296
00:14:17,406 --> 00:14:20,246
from the existing
audio unit types,


297
00:14:20,336 --> 00:14:22,126
we have these four new types.


298
00:14:23,306 --> 00:14:26,196
They all are the same in that
they produce audio output


299
00:14:27,046 --> 00:14:30,136
but they differ in what input
they receive from the host.


300
00:14:30,546 --> 00:14:33,116
We have remote generators
which require no input.


301
00:14:33,796 --> 00:14:36,456
We have remote instruments
which take MIDI input


302
00:14:36,456 --> 00:14:38,436
to produce output, audio output.


303
00:14:39,156 --> 00:14:41,306
We have effects which
are audio in and out.


304
00:14:41,776 --> 00:14:45,086
And finally, we have music
effects which take both audio


305
00:14:45,086 --> 00:14:47,546
and MIDI input and
produce audio.


306
00:14:48,876 --> 00:14:52,546
So, node applications
use these component types


307
00:14:52,546 --> 00:14:54,286
to describe their capabilities.


308
00:14:54,726 --> 00:14:57,776
And furthermore one node
application may actually have


309
00:14:58,316 --> 00:15:01,966
multiple sets of capabilities
and may wish to present itself


310
00:15:02,096 --> 00:15:03,996
in multiple ways to hosts.


311
00:15:04,156 --> 00:15:05,486
As a simple example,


312
00:15:05,486 --> 00:15:09,106
a node application may produce
audio just fine on its own,


313
00:15:09,576 --> 00:15:10,876
in which case, it's a generator.


314
00:15:11,316 --> 00:15:15,006
It may optionally be
able to respond to MIDI


315
00:15:15,006 --> 00:15:16,936
in producing that audio.


316
00:15:17,616 --> 00:15:19,566
So, it can also be a generator.


317
00:15:19,566 --> 00:15:22,436
So, such a node application
could publish itself


318
00:15:22,436 --> 00:15:24,326
as two different
audio components


319
00:15:24,566 --> 00:15:25,996
with separate capabilities.


320
00:15:26,856 --> 00:15:29,036
Another example of
that is an application


321
00:15:29,036 --> 00:15:33,336
like a guitar amp simulator
where the application appears


322
00:15:33,336 --> 00:15:36,816
to the user as an effect
because audio is going in,


323
00:15:37,136 --> 00:15:39,636
it's being processed in some
way and then it comes out.


324
00:15:40,206 --> 00:15:41,746
But from the host's
point of view,


325
00:15:42,156 --> 00:15:45,616
this application can appear
either as a generator an effect


326
00:15:45,776 --> 00:15:48,056
and the node can publish
itself either way.


327
00:15:48,566 --> 00:15:52,676
For example, if a node says I'm
a generator, it can continue


328
00:15:52,676 --> 00:15:57,906
to receive microphone or a line
input from a guitar directly


329
00:15:57,976 --> 00:16:02,736
from the underlying AURemoteIO
while only sending the audio


330
00:16:02,736 --> 00:16:03,736
output to the host.


331
00:16:04,066 --> 00:16:05,446
So again, that's generator mode.


332
00:16:05,986 --> 00:16:07,906
Or if a host application


333
00:16:07,906 --> 00:16:10,686
like GarageBand might have
a prerecorded guitar track


334
00:16:11,176 --> 00:16:13,846
and want to process that through
the guitar amp simulator.


335
00:16:14,306 --> 00:16:17,226
The guitar amp simulator can
function fully as an effect,


336
00:16:17,786 --> 00:16:20,036
not communicate with the
audio hardware at all,


337
00:16:20,556 --> 00:16:22,896
and just communicate
the two audio streams


338
00:16:23,276 --> 00:16:27,366
between itself and the host.


339
00:16:27,966 --> 00:16:30,416
Let's move on and look at
some of the requirements


340
00:16:30,416 --> 00:16:33,276
for the Inter-App Audio
feature, it's available


341
00:16:33,276 --> 00:16:35,826
on most iOS 7 compatible
devices,


342
00:16:35,866 --> 00:16:37,886
the exception being
the iPhone 4.


343
00:16:38,196 --> 00:16:41,476
And on the iPhone 4, you
don't really have to deal


344
00:16:41,476 --> 00:16:43,786
with this specially because
what will happen is node


345
00:16:43,786 --> 00:16:46,916
applications, if they
attempt to register themselves


346
00:16:46,916 --> 00:16:50,276
with the system, those calls
will just fail silently,


347
00:16:50,276 --> 00:16:51,396
the system will ignore them.


348
00:16:51,396 --> 00:16:55,776
And on the host side, the
host will simply see no node


349
00:16:55,776 --> 00:16:58,626
applications on the system.


350
00:16:58,626 --> 00:17:00,676
Both host and node
applications need


351
00:17:00,676 --> 00:17:04,185
to have a new entitlement called
"inter-app-audio" and this--


352
00:17:04,656 --> 00:17:06,566
you can set this
for your application


353
00:17:06,566 --> 00:17:08,396
in the Xcode Capabilities tab.


354
00:17:10,156 --> 00:17:15,296
Furthermore, most applications
will want to have audio


355
00:17:15,296 --> 00:17:17,076
in their UIBackgroundModes.


356
00:17:17,806 --> 00:17:21,036
Most especially hosts
for obvious reasons


357
00:17:21,086 --> 00:17:23,925
because hosts will keep
running their engines


358
00:17:23,925 --> 00:17:25,226
when nodes are on
the foreground.


359
00:17:25,736 --> 00:17:29,436
Also, nodes like the guitar amp
simulator I just mentioned may


360
00:17:29,436 --> 00:17:32,206
want to continue accessing the
mic and to be able to do that,


361
00:17:32,256 --> 00:17:34,786
they too need to have the
audio background mode.


362
00:17:35,766 --> 00:17:37,686
One final requirement for nodes


363
00:17:37,686 --> 00:17:39,846
in particular is
the MixWithOthers


364
00:17:39,846 --> 00:17:41,616
AudioSessionCategoryOption.


365
00:17:42,276 --> 00:17:45,556
Hosts can go either
way on this one.


366
00:17:45,666 --> 00:17:47,576
We'll get into that
in more detail later.


367
00:17:48,176 --> 00:17:52,086
OK. Getting in to the nuts
and bolts of the APIs here,


368
00:17:52,086 --> 00:17:54,276
let's look at how node
applications can register


369
00:17:54,276 --> 00:17:57,876
themselves with the system.


370
00:17:57,876 --> 00:18:01,656
So, there's two pieces
of registering one's self


371
00:18:01,656 --> 00:18:02,766
for a node application.


372
00:18:03,246 --> 00:18:07,186
The first is an Info.plist
entry called AudioComponents.


373
00:18:07,626 --> 00:18:11,306
So, the presence of this
Info.plist entry makes the app


374
00:18:11,586 --> 00:18:15,096
discoverable and
launchable to the system.


375
00:18:15,396 --> 00:18:16,896
The system knows,
oh I've got one


376
00:18:16,896 --> 00:18:18,706
of this node applications
installed.


377
00:18:19,226 --> 00:18:22,866
The second part of registration
is for the node application


378
00:18:22,866 --> 00:18:26,286
to call AudioOutputUnitPublish
which checks


379
00:18:26,286 --> 00:18:30,106
in that registration that it
advertised in its Info.plist.


380
00:18:30,286 --> 00:18:33,146
It says, I've been launched and
here I am ready to communicate.


381
00:18:33,366 --> 00:18:36,316
So let's look at those two
pieces in a little detail.


382
00:18:37,096 --> 00:18:41,286
So here is the AudioComponents
entry in the Info.plist.


383
00:18:41,286 --> 00:18:43,756
Its value is an array
and in that array,


384
00:18:43,756 --> 00:18:46,436
there is a dictionary
for every AudioComponent


385
00:18:46,436 --> 00:18:47,836
that the node wants to register.


386
00:18:48,576 --> 00:18:50,986
And in that dictionary,
if you're familiar


387
00:18:50,986 --> 00:18:53,056
with AudioComponentDescriptions
already,


388
00:18:53,056 --> 00:18:55,276
you'll see some familiar
fields there.


389
00:18:55,276 --> 00:18:59,946
There is the type, subtype and
manufacturer along with the name


390
00:18:59,946 --> 00:19:01,056
and the version number.


391
00:19:01,656 --> 00:19:04,186
So, that completely
describes the AudioComponent


392
00:19:04,566 --> 00:19:07,376
that the node application
is advertising.


393
00:19:07,976 --> 00:19:13,026
So, moving on to the second
part of the registration here,


394
00:19:13,636 --> 00:19:16,306
this is when the node
application launches,


395
00:19:16,806 --> 00:19:21,676
the first piece of code here
is the node's normal process


396
00:19:21,676 --> 00:19:24,416
for creating its
AURemoteIO when it launches.


397
00:19:24,846 --> 00:19:26,856
It creates an
AudioComponentDescription


398
00:19:26,856 --> 00:19:30,176
describing the Apple
AURemoteIO instance,


399
00:19:30,176 --> 00:19:32,146
it uses AudioComponentFindNext


400
00:19:32,146 --> 00:19:35,476
to go find the AudioComponent
for the AURemoteIO.


401
00:19:35,996 --> 00:19:38,896
And finally, it creates an
instance of the AURemoteIO


402
00:19:38,896 --> 00:19:43,326
and this is something just about
every audio and music app on--


403
00:19:43,556 --> 00:19:47,586
today will do for creating
a low latency IO channel.


404
00:19:48,106 --> 00:19:52,166
What's new is that the node
application, to participate


405
00:19:52,166 --> 00:19:56,696
in Inter-App Audio is now
going to connect that IO Unit


406
00:19:56,736 --> 00:20:00,086
that it just created with
the component description


407
00:20:00,086 --> 00:20:02,356
that was published in
the Info.plist entry.


408
00:20:02,976 --> 00:20:06,126
So, to do that, we're
seeing this code here


409
00:20:06,126 --> 00:20:08,506
that that node creates an
AudioComponentDescription


410
00:20:08,506 --> 00:20:11,736
which matches the one in the
Info.plist we saw a moment ago.


411
00:20:12,356 --> 00:20:15,736
It supplies the name and version
number and passes all that along


412
00:20:15,736 --> 00:20:17,456
with the AURemoteIO instance


413
00:20:17,456 --> 00:20:20,806
to a new API called
AudioOutputUnitPublish.


414
00:20:20,976 --> 00:20:24,476
So again, that connects what
was advertised in the Info.plist


415
00:20:25,126 --> 00:20:29,476
with the actual RemoteIO
instance in the application


416
00:20:29,806 --> 00:20:32,106
to which the host
application will connect


417
00:20:32,106 --> 00:20:34,216
as we'll see in a little bit.


418
00:20:34,216 --> 00:20:36,556
So, to make this all
work, a requirement


419
00:20:36,556 --> 00:20:40,726
of the node application is
to publish that RemoteIO unit


420
00:20:40,836 --> 00:20:44,426
when it launches because the
node application is going


421
00:20:44,426 --> 00:20:46,986
to get launched by host
applications when--


422
00:20:46,986 --> 00:20:48,896
at times when the user
what's to use them.


423
00:20:49,936 --> 00:20:53,366
And so, the node application
basically has to acknowledge,


424
00:20:53,366 --> 00:20:54,526
I'm here, I've been launched.


425
00:20:55,666 --> 00:21:00,306
And so, you can see then why the
Info.plist entry and the call


426
00:21:00,306 --> 00:21:04,636
to AudioOutputUnitPublish
must have the same component


427
00:21:04,636 --> 00:21:06,426
descriptions, names,
and versions.


428
00:21:07,426 --> 00:21:09,416
One note here is
that by convention,


429
00:21:09,826 --> 00:21:13,106
the component name should
contain your manufacture name


430
00:21:13,106 --> 00:21:17,236
and application name and that
lets host applications sort the


431
00:21:17,236 --> 00:21:20,416
available node applications by
manufacture name if they like.


432
00:21:20,416 --> 00:21:26,516
So, that's the registration
process for node applications,


433
00:21:26,516 --> 00:21:29,416
let's look at how host
applications can discover


434
00:21:29,416 --> 00:21:30,556
those registrations.


435
00:21:33,836 --> 00:21:37,426
So again, if you've used the
AudioComponent calls before,


436
00:21:37,626 --> 00:21:39,346
this should look
fairly familiar.


437
00:21:39,726 --> 00:21:43,456
What we here-- have here is a
loop where we want to iterate


438
00:21:43,526 --> 00:21:46,126
through all of the components on
the system because we're looking


439
00:21:46,126 --> 00:21:48,656
for nodes and there
are multiple types.


440
00:21:48,786 --> 00:21:50,286
So, the simplest
way to do that is


441
00:21:50,286 --> 00:21:52,506
to create a wild card
component description


442
00:21:52,506 --> 00:21:54,916
and that's the searchDesc,
it's full of zeros.


443
00:21:55,586 --> 00:21:58,906
And so then, this loop will
call AudioComponentFindNext


444
00:21:58,906 --> 00:22:02,936
repeatedly and that
will yield in turn each


445
00:22:02,936 --> 00:22:05,166
of the AudioComponents
on the system which are


446
00:22:05,526 --> 00:22:06,886
in the local variable comp.


447
00:22:07,516 --> 00:22:10,486
When we find null, then we've
gotten to the end of the list


448
00:22:10,486 --> 00:22:14,006
of all the components in
the system and we're done


449
00:22:14,006 --> 00:22:15,286
with our loop, we'll
have found them all.


450
00:22:16,126 --> 00:22:18,256
Now, for each component on
the system, what we want


451
00:22:18,256 --> 00:22:20,716
to do is call
AudioComponentGetDescription


452
00:22:21,596 --> 00:22:24,486
and this will supply to us
the AudioComponent description


453
00:22:24,576 --> 00:22:27,446
of the actual unit as
opposed to that wild card


454
00:22:27,646 --> 00:22:28,786
that we used for searching.


455
00:22:29,436 --> 00:22:32,816
So now, in foundDesc, we can
look at its component type


456
00:22:32,816 --> 00:22:37,706
and see if it's one of the
four inter-app audio unit types


457
00:22:37,706 --> 00:22:40,466
that we're interested in, the
RemoteEffects, RemoteGenerator,


458
00:22:40,466 --> 00:22:42,196
RemoteInstrument, and
RemoteMusicEffect.


459
00:22:42,676 --> 00:22:46,596
If we see one of those, then
we know we found the node.


460
00:22:47,136 --> 00:22:49,336
OK. So the host has
found a node.


461
00:22:49,926 --> 00:22:52,496
So now, I'm going to
walk through a little bit


462
00:22:52,496 --> 00:22:54,746
of code here from one
of our sample apps.


463
00:22:55,296 --> 00:22:59,126
It creates an objective C
object of its own just as a way


464
00:22:59,126 --> 00:23:01,486
of storing information about
the nodes that it's found.


465
00:23:02,436 --> 00:23:05,686
And it calls this class
RemoteAU and it stores away


466
00:23:05,686 --> 00:23:08,266
into it the component
description that was found


467
00:23:08,806 --> 00:23:10,866
and the AudioComponent
that was found.


468
00:23:11,886 --> 00:23:14,516
It also fetches the
component's name and stores


469
00:23:14,516 --> 00:23:17,806
that in the field of
the RemoteAU object.


470
00:23:18,696 --> 00:23:22,806
It sets the image from
AudioComponentGetIcon


471
00:23:22,806 --> 00:23:25,536
which is a new API call which
works with inter-app audio.


472
00:23:26,186 --> 00:23:28,046
This gives you the
application of--


473
00:23:28,376 --> 00:23:30,966
I'm sorry, the icon of
the node application.


474
00:23:32,836 --> 00:23:36,996
We can also discover the time at
which the user last interacted


475
00:23:37,086 --> 00:23:41,656
with the node app and this
can be useful if we want


476
00:23:41,696 --> 00:23:46,286
to sort a list of available
node applications by time


477
00:23:46,286 --> 00:23:47,926
of when they were
most recently used,


478
00:23:48,336 --> 00:23:50,016
the way the home screen does.


479
00:23:50,686 --> 00:23:52,936
So we've gathered up
all this information


480
00:23:52,936 --> 00:23:56,146
about the node application,
and now we've built an array


481
00:23:56,146 --> 00:24:00,216
from which we can drive a
table view and present the user


482
00:24:00,216 --> 00:24:05,306
with a choice of node
applications to deal with.


483
00:24:06,046 --> 00:24:09,606
One wrinkle though is having
cached all that information


484
00:24:09,606 --> 00:24:13,986
in an array, it can become stale
and out of sync with the system.


485
00:24:14,436 --> 00:24:17,606
Most notably, when apps
are installed and deleted


486
00:24:17,606 --> 00:24:21,656
so if you find yourself caching
list of components like this,


487
00:24:21,656 --> 00:24:25,456
you should probably also
listen to this new notification


488
00:24:25,456 --> 00:24:28,966
that we supply, its name is
AudioComponentRegistrations


489
00:24:28,966 --> 00:24:30,196
ChangedNotification.


490
00:24:30,586 --> 00:24:35,916
So, you can pass that
to NSNotification center


491
00:24:35,916 --> 00:24:37,846
to register for a notification.


492
00:24:38,136 --> 00:24:42,216
In this example, we're supplying
a block to be called and then


493
00:24:42,216 --> 00:24:45,426
that block which is called when
the notification or rather,


494
00:24:45,426 --> 00:24:48,166
when the registration has
changed, we can refresh


495
00:24:48,236 --> 00:24:50,366
that cached list of
audio units we built.


496
00:24:51,016 --> 00:24:55,346
So, that's the process of
discovering node apps for host.


497
00:24:55,696 --> 00:24:57,656
So now, we've built up a table


498
00:24:58,256 --> 00:25:01,656
and maybe the user has
selected one of them


499
00:25:01,656 --> 00:25:03,246
and in the host application now,


500
00:25:03,246 --> 00:25:05,116
we want to actually
establish a connection


501
00:25:05,516 --> 00:25:08,836
to the node application so
let's look at how that works.


502
00:25:09,886 --> 00:25:13,716
The first step is very
simple because we held


503
00:25:13,716 --> 00:25:16,646
on to the AudioComponent
of that node.


504
00:25:17,476 --> 00:25:20,356
Now, all we have to do is create
an instance of that component


505
00:25:20,436 --> 00:25:22,266
and now, we have an audio unit


506
00:25:22,386 --> 00:25:24,296
through which we can
communicate with the node.


507
00:25:24,756 --> 00:25:27,746
It's worth mentioning
that this is the moment


508
00:25:27,746 --> 00:25:30,556
at which the node
application will get launched


509
00:25:30,556 --> 00:25:32,506
into the background if it
it's not already running.


510
00:25:33,796 --> 00:25:36,006
And we'll look it all the
mechanics of what happens


511
00:25:36,006 --> 00:25:37,486
on the node side of that later.


512
00:25:37,826 --> 00:25:39,736
Right now, we're just going
to focus on the host side.


513
00:25:40,446 --> 00:25:44,826
So, the host has to do a fair--
a few steps here to get ready


514
00:25:44,906 --> 00:25:48,046
to steam audio between
itself and the node.


515
00:25:49,396 --> 00:25:53,406
Most importantly, the
host must be communicating


516
00:25:53,406 --> 00:25:57,376
with the node using the same
hardware sample rate as--


517
00:25:57,466 --> 00:25:59,146
or the same sample
rate as the hardware.


518
00:25:59,806 --> 00:26:04,036
So, to be absolutely sure the
hardware sample rate is what


519
00:26:04,036 --> 00:26:07,156
it's supposed to be, we should
be making our audio session


520
00:26:07,156 --> 00:26:08,666
active if we haven't already.


521
00:26:09,996 --> 00:26:11,876
So, once having done that,


522
00:26:11,876 --> 00:26:14,786
then we can specify the audio
stream basic description


523
00:26:15,236 --> 00:26:19,516
which is a detailed
description of the audio format


524
00:26:20,466 --> 00:26:23,116
that the host wishes to use
to communicate with the node.


525
00:26:24,286 --> 00:26:26,936
So, we can choose
mono or stereo.


526
00:26:26,936 --> 00:26:28,756
In this example,
I've chosen stereo.


527
00:26:28,756 --> 00:26:32,136
Here is where we're using
the hardware sample rate


528
00:26:32,756 --> 00:26:38,166
And these lines of code here
are basically specifying 32 bit


529
00:26:38,166 --> 00:26:40,206
floating-point, non-interleaved.


530
00:26:40,596 --> 00:26:43,866
Now, the host application can
choose any format it likes here


531
00:26:44,006 --> 00:26:47,356
and the system will perform
whatever conversions are


532
00:26:47,356 --> 00:26:50,926
necessary as long as there's
not a sample rate conversion


533
00:26:51,086 --> 00:26:51,786
being requested.


534
00:26:51,786 --> 00:26:56,196
Again, you must use the sample
rate that matches the hardware.


535
00:26:57,426 --> 00:26:58,136
All right.


536
00:26:58,136 --> 00:27:00,896
Now, we have built up an
audio stream basic description


537
00:27:00,896 --> 00:27:02,906
and we can use
AudioUnitSetProperty


538
00:27:02,906 --> 00:27:06,496
on the node AudioUnit for
the stream format property


539
00:27:07,366 --> 00:27:10,476
and this is specifying-- since
it's in the output scope,


540
00:27:10,476 --> 00:27:14,596
this is specifying the output
format of the audio we need


541
00:27:14,596 --> 00:27:15,746
to receive from the node.


542
00:27:15,866 --> 00:27:18,856
If we're working with a
generator or instrument


543
00:27:18,856 --> 00:27:21,276
which don't take audio
input, that's it, we've--


544
00:27:21,426 --> 00:27:25,786
we're done, we've just
specified the output format.


545
00:27:26,146 --> 00:27:27,566
But if we're dealing
with an effect,


546
00:27:27,686 --> 00:27:29,966
then we should also
specify the input format.


547
00:27:30,426 --> 00:27:32,856
And in many cases, it's
going to be identical


548
00:27:32,956 --> 00:27:35,396
to the output format and so,


549
00:27:35,396 --> 00:27:39,486
we can make that same call
using the input scope just


550
00:27:39,516 --> 00:27:42,936
that the input format that we're
going to supply to the node.


551
00:27:44,076 --> 00:27:49,806
So, having specified formats,
we can look how we're going


552
00:27:49,806 --> 00:27:52,426
to get audio from the
host into the node


553
00:27:52,926 --> 00:27:56,496
and this is starting it get into
the details of multiple ways


554
00:27:56,496 --> 00:27:58,776
that your host may
be interacting


555
00:27:58,776 --> 00:27:59,866
with the node AudioUnit.


556
00:28:00,216 --> 00:28:03,256
Now, since we're connecting
input, this is only for effects


557
00:28:04,616 --> 00:28:08,426
and the host at this point
can supply input to a node


558
00:28:08,426 --> 00:28:12,926
from another audio unit using
AUGraphConnectNodeInput.


559
00:28:13,056 --> 00:28:15,836
AUGraph is a higher level API
which I'm just going to touch


560
00:28:15,836 --> 00:28:20,586
on a few times today but you can
use AUGraph to build up graphs


561
00:28:20,586 --> 00:28:23,236
or a series of connections
between audio units.


562
00:28:24,316 --> 00:28:27,626
The other way to make a
connection to the node's input


563
00:28:27,916 --> 00:28:30,096
from some other audio unit is


564
00:28:30,096 --> 00:28:32,336
with the AudioUnitProperty
MakeConnection.


565
00:28:33,606 --> 00:28:36,956
Alternatively, a host can simply
supply a callback function


566
00:28:37,396 --> 00:28:39,236
with the SetRenderCallback
property.


567
00:28:39,646 --> 00:28:42,066
This callback function
gets called at render time


568
00:28:42,066 --> 00:28:46,016
and the host supplies the audio
samples to be given to the node.


569
00:28:46,426 --> 00:28:49,036
Now, as far as connecting
the output of the node,


570
00:28:49,116 --> 00:28:52,646
this too depends on the way
you built your host engine.


571
00:28:53,196 --> 00:28:54,996
If you're using audio
units, you want to connect


572
00:28:54,996 --> 00:28:57,366
to the node output to
some other audio unit.


573
00:28:57,796 --> 00:29:00,416
You can use the MakeConnection
property again.


574
00:29:01,146 --> 00:29:04,846
If however you're pulling
audio into a custom engine,


575
00:29:04,846 --> 00:29:06,426
then you would call
AudioUnitRender


576
00:29:06,996 --> 00:29:09,946
but there's no setup
at this time for that.


577
00:29:11,276 --> 00:29:12,956
We'll look at the
rendering process


578
00:29:12,956 --> 00:29:14,356
in more detail a little later.


579
00:29:14,956 --> 00:29:22,006
OK. One last a bit of mechanics
here that a host needs to do


580
00:29:22,006 --> 00:29:25,486
to establish a reliable
connection to a node or actually


581
00:29:25,486 --> 00:29:29,706
to reliably handle bad things
happening with the node is


582
00:29:30,166 --> 00:29:32,866
to look out for what happens
when nodes become disconnected.


583
00:29:33,196 --> 00:29:35,996
This could happen automatically
if the node app crashes,


584
00:29:36,326 --> 00:29:39,556
if the system ejects it from
this memory before being


585
00:29:39,936 --> 00:29:41,726
under memory pressure.


586
00:29:42,456 --> 00:29:44,196
Also, if the host fails


587
00:29:44,196 --> 00:29:47,836
to render the node
application regularly enough,


588
00:29:47,836 --> 00:29:49,526
the system will evict it from--


589
00:29:49,866 --> 00:29:51,746
or I'm sorry, will
break the connection.


590
00:29:52,206 --> 00:29:56,196
When these things happen, then
the node AudioUnit becomes,


591
00:29:56,256 --> 00:29:59,466
in effect of zombie,
meaning that it's--


592
00:29:59,596 --> 00:30:01,326
there's still an
audio unit there.


593
00:30:01,426 --> 00:30:05,356
You can make API calls on
it but they won't crash


594
00:30:06,256 --> 00:30:08,606
but you will get errors
back and that's the error


595
00:30:08,606 --> 00:30:10,746
that you'll get back, the
InstanceInvalidated error.


596
00:30:11,996 --> 00:30:16,136
The mechanics of establishing
that disconnection callback -


597
00:30:16,456 --> 00:30:20,836
we call
AudioUnitAddPropertyListener


598
00:30:20,956 --> 00:30:23,556
for this new property
IsInterAppConnected.


599
00:30:24,696 --> 00:30:26,876
Here is what you would do
in the connection listener,


600
00:30:26,876 --> 00:30:30,996
you can fetch current value of
the property and see if it is 0


601
00:30:31,146 --> 00:30:35,446
and if the local variable here
connected has become zero,


602
00:30:35,946 --> 00:30:38,136
then you know the node
application has become


603
00:30:38,576 --> 00:30:40,646
disconnected and you
should react accordingly.


604
00:30:41,716 --> 00:30:46,806
So, all of that prep work
has led us up to the point


605
00:30:46,806 --> 00:30:49,066
or we're ready to actually
initialize the node.


606
00:30:49,296 --> 00:30:53,736
Now, the AudioUnit
initialize call basically says


607
00:30:53,736 --> 00:30:55,986
to the system and
the other AudioUnit.


608
00:30:56,286 --> 00:30:57,436
Here, allocate all


609
00:30:57,436 --> 00:30:59,406
of the resources you
need for rendering.


610
00:30:59,986 --> 00:31:01,496
In the case of inter-app audio,


611
00:31:01,886 --> 00:31:05,146
the system at this point is
also allocating some resources


612
00:31:05,146 --> 00:31:07,626
on behalf of that
connection such as the buffers


613
00:31:07,626 --> 00:31:10,706
between the applications and
the real-time rendering thread


614
00:31:10,706 --> 00:31:11,966
in the node application.


615
00:31:12,956 --> 00:31:14,656
So, it's important to realize.


616
00:31:14,656 --> 00:31:18,756
This is a point at which you are
beginning to consume resources


617
00:31:19,046 --> 00:31:21,526
and as such, you have
the responsibility now


618
00:31:22,296 --> 00:31:24,706
of calling AudioUnitRender
regularly


619
00:31:25,746 --> 00:31:27,846
on this node audio unit.


620
00:31:29,296 --> 00:31:32,506
So, that's the process
of setting up a host


621
00:31:32,506 --> 00:31:33,666
to communicate with a node.


622
00:31:33,806 --> 00:31:36,896
You activate your audio session,
you set your stream formats,


623
00:31:37,026 --> 00:31:39,966
you connect your audio input,
add a disconnection listener,


624
00:31:40,026 --> 00:31:42,256
and finally call
AudioUnitInitialize.


625
00:31:42,256 --> 00:31:46,666
So having done that, you're at
the point now where you're ready


626
00:31:46,666 --> 00:31:50,116
to begin streaming audio
between the two applications.


627
00:31:51,246 --> 00:31:54,466
Let's look inside a host
application's engine


628
00:31:54,466 --> 00:31:55,206
in more detail.


629
00:31:55,656 --> 00:31:59,036
This is kind of a wonderfully
simple way to do things


630
00:31:59,036 --> 00:32:03,166
if you can get your work
done using Apple AudioUnits.


631
00:32:03,166 --> 00:32:06,506
So, the green box, the
green dotted lines--


632
00:32:07,256 --> 00:32:09,116
box represents a host engine


633
00:32:09,116 --> 00:32:12,606
but those red boxes inside are
all Apple supplied audio units.


634
00:32:12,876 --> 00:32:15,136
So, there is the AURemoteIO,


635
00:32:15,136 --> 00:32:17,176
we have a mixer AudioUnite
feeding that.


636
00:32:17,586 --> 00:32:20,356
In feeding the mixer, we
have a file player AudioUnit


637
00:32:20,856 --> 00:32:22,836
and the node AudioUnit.


638
00:32:23,736 --> 00:32:27,116
But of course, there are many
things you would want to do


639
00:32:27,116 --> 00:32:29,586
with audio that Apple doesn't
give you AudioUnits for.


640
00:32:29,586 --> 00:32:31,876
If you want to that, then
you're going to write some code


641
00:32:31,876 --> 00:32:34,956
of your own represented
by the green box


642
00:32:34,956 --> 00:32:36,556
with the squiggly brackets.


643
00:32:36,946 --> 00:32:41,336
So here, your engine is
feeding the AURemoteIO


644
00:32:41,536 --> 00:32:44,136
and if you've written
an app like this before,


645
00:32:44,136 --> 00:32:47,386
you know the way to provide
input to an AURemoteIO


646
00:32:47,386 --> 00:32:50,526
from your own engine is with
the SetRenderCallback property.


647
00:32:51,266 --> 00:32:53,666
And now in this case,
to fetch the audio


648
00:32:53,666 --> 00:32:57,316
from the node AudioUnit, you
would call AudioUnitRender.


649
00:32:57,916 --> 00:33:03,126
OK. So, that's a bunch
of stuff about how we--


650
00:33:03,286 --> 00:33:06,116
a host application
interact with a node.


651
00:33:06,116 --> 00:33:09,676
One final nice thing to
do for the user here is


652
00:33:09,676 --> 00:33:11,896
to provide a way for the user


653
00:33:12,666 --> 00:33:15,806
to bring the node
application to the foreground.


654
00:33:16,176 --> 00:33:22,296
So, we can do this by asking
the audio unit for a PeerURL


655
00:33:22,296 --> 00:33:25,436
and this URL is only
valid during the life


656
00:33:25,436 --> 00:33:26,276
of the connection.


657
00:33:26,276 --> 00:33:28,166
You don't want to hold on to it


658
00:33:28,166 --> 00:33:30,266
because it's not going
to be useful later.


659
00:33:30,716 --> 00:33:33,576
But right before the user
wants to switch in response


660
00:33:33,606 --> 00:33:35,016
to that Icon tap or whatever,


661
00:33:35,556 --> 00:33:38,966
you can fetch the PeerURL then
pass that to UIApplication


662
00:33:39,026 --> 00:33:43,196
and ask it to open that URL and
that will accomplish the switch


663
00:33:43,446 --> 00:33:45,936
of bringing the node
application to the foreground.


664
00:33:50,936 --> 00:33:54,446
So, let's go back just
a little bit and look


665
00:33:54,516 --> 00:33:57,156
at how node applications
see the process


666
00:33:57,156 --> 00:33:58,866
of becoming connected to hosts.


667
00:34:01,556 --> 00:34:04,956
So, the most important thing to
think about here as the author


668
00:34:04,956 --> 00:34:06,336
of a node application is


669
00:34:06,416 --> 00:34:10,936
that when the user opens
your application explicitly


670
00:34:10,936 --> 00:34:12,496
from the home screen,
you're launched


671
00:34:12,496 --> 00:34:15,366
into the foreground state,
you're ready start making music.


672
00:34:16,576 --> 00:34:19,306
But if you're being
launched from the context


673
00:34:19,306 --> 00:34:22,545
of a host application, you're
actually going to get launched


674
00:34:22,545 --> 00:34:25,045
into the background state and
there are some limitations


675
00:34:25,045 --> 00:34:26,326
about what you can do at this--


676
00:34:26,446 --> 00:34:28,436
in this state and there's
also a requirement here.


677
00:34:29,096 --> 00:34:33,366
You can't start running from the
background but you must create


678
00:34:33,366 --> 00:34:35,646
and publish your I/O
unit as I showed earlier.


679
00:34:36,116 --> 00:34:40,126
So, it's probably going
to be necessary and useful


680
00:34:40,456 --> 00:34:41,696
in your node application


681
00:34:42,016 --> 00:34:44,906
to ask UIApplication
what's the state here,


682
00:34:45,056 --> 00:34:47,196
Am I in the background or
am I in the foreground?


683
00:34:47,496 --> 00:34:48,556
and proceed accordingly.


684
00:34:50,976 --> 00:34:53,315
So, node applications to find


685
00:34:53,315 --> 00:34:55,235
out when they're
becoming connected


686
00:34:55,235 --> 00:34:57,966
and disconnected can also listen


687
00:34:58,036 --> 00:35:01,166
for the IsInterAppConnected
property just as I described


688
00:35:01,526 --> 00:35:03,086
for host applications earlier.


689
00:35:04,596 --> 00:35:07,106
For a node application,
you listen to this property


690
00:35:07,526 --> 00:35:10,036
on your AURemoteIO instance.


691
00:35:11,456 --> 00:35:15,576
So, in your property listener,
you can notice the transitions


692
00:35:15,576 --> 00:35:17,916
of this property value
from zero to one.


693
00:35:18,236 --> 00:35:20,406
When you see it becoming
true, then you know


694
00:35:20,406 --> 00:35:22,936
that you're output unit has
been initialized underneath you


695
00:35:23,166 --> 00:35:26,076
and that you should set
your audio session active


696
00:35:26,126 --> 00:35:27,816
if you're going to
access the microphone.


697
00:35:29,076 --> 00:35:31,886
You should at this time start
running because that's kind


698
00:35:31,886 --> 00:35:35,516
of your final step of consent
saying, My engine is all hooked


699
00:35:35,516 --> 00:35:39,426
up and ready to render,
start pulling on me.


700
00:35:39,986 --> 00:35:43,206
You can, at this time,
start running even


701
00:35:43,206 --> 00:35:44,346
if you are in the background.


702
00:35:44,346 --> 00:35:45,676
This is the exception
to the rule


703
00:35:45,676 --> 00:35:46,826
about running in the background.


704
00:35:47,466 --> 00:35:48,966
When you are connected
to the host,


705
00:35:48,966 --> 00:35:52,066
you can start running
in the background.


706
00:35:52,066 --> 00:35:53,306
One further note, if you want


707
00:35:53,306 --> 00:35:55,476
to draw an icon representing
the host


708
00:35:55,476 --> 00:35:56,856
that you've become connected to,


709
00:35:57,186 --> 00:36:00,286
there's a new API called
AudioOutputUnitGetHostIcon.


710
00:36:03,546 --> 00:36:07,036
Pertaining further to the
IsInterAppConnected property,


711
00:36:07,036 --> 00:36:10,016
you also want to watch
for the transition to zero


712
00:36:10,016 --> 00:36:13,166
or false meaning that the host
has disconnected from you.


713
00:36:13,656 --> 00:36:17,466
What you want to do at this
point is understand your output


714
00:36:17,466 --> 00:36:20,186
unit has been uninitialized
and stopped for--


715
00:36:20,226 --> 00:36:21,296
out from underneath you.


716
00:36:22,636 --> 00:36:25,646
Now, if you were
accessing the microphone,


717
00:36:25,926 --> 00:36:28,206
you should set your session
inactive at this time.


718
00:36:28,906 --> 00:36:31,476
However, you might,
in some situations,


719
00:36:31,476 --> 00:36:35,336
find yourself disconnected
while in the foreground.


720
00:36:35,336 --> 00:36:37,076
Maybe the host application
crashed


721
00:36:37,076 --> 00:36:39,496
or the system didn't have enough
memory to keep it running.


722
00:36:39,886 --> 00:36:42,666
So, if that happens,
you probably do want


723
00:36:42,716 --> 00:36:45,196
to start running and keep
your audio session active


724
00:36:45,196 --> 00:36:47,606
or make it active
if it isn't already.


725
00:36:48,926 --> 00:36:50,746
But again, you can only start--


726
00:36:50,836 --> 00:36:53,906
you can only make your session
active and start running


727
00:36:53,906 --> 00:36:54,906
when you're in the foreground.


728
00:36:55,506 --> 00:36:59,576
So, just to reemphasize that.


729
00:37:00,586 --> 00:37:04,246
Your node application can
start if you've been connected


730
00:37:04,246 --> 00:37:05,956
to the host or you're
in the foreground


731
00:37:06,386 --> 00:37:08,326
but you can keep running
in to the background


732
00:37:08,946 --> 00:37:11,366
if you're connected, of
course, or if you are


733
00:37:11,366 --> 00:37:15,156
in some other standalone
non inter-app scenario


734
00:37:15,316 --> 00:37:17,556
where your app wants to keep
running in to the background.


735
00:37:21,506 --> 00:37:25,916
Let's look again now at a few
different scenarios involving


736
00:37:25,946 --> 00:37:27,476
how nodes render audio.


737
00:37:28,276 --> 00:37:30,486
This is your normal
standalone mode


738
00:37:30,486 --> 00:37:31,686
when the user has launched you.


739
00:37:32,346 --> 00:37:34,486
You've got your engine
connected to the RemoteIO,


740
00:37:34,486 --> 00:37:37,036
connected to the
audio I/O system.


741
00:37:37,446 --> 00:37:40,066
If you're a generator
or instrument,


742
00:37:41,406 --> 00:37:44,406
you may have your output
completely redirected


743
00:37:44,406 --> 00:37:45,706
to the host.


744
00:37:47,156 --> 00:37:49,626
But if you leave your
input bus enabled


745
00:37:49,766 --> 00:37:52,986
but you advertise yourself
as a generator or instrument,


746
00:37:53,676 --> 00:37:56,566
then you've continued
to receive input


747
00:37:56,566 --> 00:38:00,126
from the microphone even while
your output has been redirected


748
00:38:00,126 --> 00:38:01,346
to the host application.


749
00:38:02,186 --> 00:38:04,386
Now, this doesn't
add any extra latency


750
00:38:05,406 --> 00:38:07,436
because the system
is smart enough


751
00:38:07,436 --> 00:38:11,866
to deliver your application, the
microphone input first and then


752
00:38:11,866 --> 00:38:15,936
in that same I/O cycle, the
host application will pull


753
00:38:15,936 --> 00:38:16,646
your output.


754
00:38:17,256 --> 00:38:21,466
In the final node
rendering scenario -


755
00:38:21,866 --> 00:38:24,456
is you have in effect
both your input


756
00:38:24,456 --> 00:38:26,796
and output streams are
connected to the host rather


757
00:38:26,796 --> 00:38:29,386
than the audio I/O system.


758
00:38:29,956 --> 00:38:34,026
Node applications can also use


759
00:38:34,026 --> 00:38:36,506
that PeerURL property
I described earlier


760
00:38:36,666 --> 00:38:40,496
to show an icon as
Alec did in his demo.


761
00:38:41,286 --> 00:38:44,606
He showed-- the Garageband
icon in the sampler app.


762
00:38:45,216 --> 00:38:48,446
So, you can fetch that
icon from your remote--


763
00:38:48,446 --> 00:38:51,046
your AURemoteIO instance
in this case.


764
00:38:51,046 --> 00:38:52,666
You can-- I'm sorry.


765
00:38:52,666 --> 00:38:55,176
You can fetch that URL
to accomplish the switch.


766
00:38:58,936 --> 00:39:00,926
OK. Back on the host
side of things,


767
00:39:01,706 --> 00:39:04,496
there are a few considerations
about stopping audio rendering.


768
00:39:05,476 --> 00:39:08,886
The normal API calls for this
doing are AudioOutputUnitStop


769
00:39:08,886 --> 00:39:12,566
or AUGraphStop and
what you want to do


770
00:39:12,566 --> 00:39:16,236
at this point is promptly
uninitialize your AudioUnit


771
00:39:16,236 --> 00:39:17,276
representing the node.


772
00:39:17,976 --> 00:39:20,696
That releases the
resources that were allocated


773
00:39:20,756 --> 00:39:24,446
when you initialized it and it
releases you from the promise


774
00:39:24,496 --> 00:39:25,836
to keep rendering frequently.


775
00:39:27,076 --> 00:39:29,806
You can turn around and
reinitialize when the user wants


776
00:39:29,806 --> 00:39:32,766
to start communicating again
or if you're completely done


777
00:39:32,766 --> 00:39:33,996
with that node AudioUnit,


778
00:39:33,996 --> 00:39:36,346
you can call
AudioComponentInstanceDispose


779
00:39:37,066 --> 00:39:39,746
and that's what you would do
the if the user, for example,


780
00:39:40,006 --> 00:39:43,176
explicitly breaks the
connection or if you discover


781
00:39:43,176 --> 00:39:45,816
that the node application
has become invalidated.


782
00:39:46,426 --> 00:39:50,326
So, that's the process
of audio rendering.


783
00:39:51,056 --> 00:39:54,516
Next, I'd like to look at how
we can communicate MIDI events


784
00:39:54,516 --> 00:39:57,256
from host applications
to node applications.


785
00:39:57,726 --> 00:40:01,746
Now, this of course, is
for remote instrument


786
00:40:01,816 --> 00:40:03,806
and remote music effect nodes.


787
00:40:05,606 --> 00:40:07,986
You would want to use this
if you have MIDI events


788
00:40:07,986 --> 00:40:11,486
that are tightly coupled to your
audio that's being rendered.


789
00:40:12,126 --> 00:40:16,186
It lets you sample-accurately
schedule MIDI note-ons,


790
00:40:16,186 --> 00:40:19,506
control events, pitch-bends,
et cetera.


791
00:40:19,506 --> 00:40:23,436
But this is not recommended as
a way of communicating clock


792
00:40:23,436 --> 00:40:24,876
and time code information.


793
00:40:25,256 --> 00:40:28,516
That's sort of a funny
way to communicate


794
00:40:28,516 --> 00:40:31,936
that you're using seven
bit numbers to break


795
00:40:31,936 --> 00:40:33,596
up timing information.


796
00:40:33,596 --> 00:40:35,456
We actually have a
better way to do that.


797
00:40:35,916 --> 00:40:39,746
I should also mention that this
does not replace the coreMIDI


798
00:40:39,746 --> 00:40:41,526
framework which still has a role


799
00:40:41,926 --> 00:40:46,786
when you're dealing USB MIDI
input and output devices or,


800
00:40:46,786 --> 00:40:48,326
for example, the
MIDI network driver.


801
00:40:49,056 --> 00:40:51,046
You might also be
dealing with applications


802
00:40:51,046 --> 00:40:53,126
that don't support inter-app
audio and you still want


803
00:40:53,126 --> 00:40:57,236
to communicate with them.


804
00:40:57,416 --> 00:41:00,276
So, let's look at how a
host application can send


805
00:41:00,276 --> 00:41:01,246
MIDI events.


806
00:41:01,526 --> 00:41:03,796
You might do something
like this in this--


807
00:41:03,886 --> 00:41:06,526
like the sampler demo
app, Alex showed.


808
00:41:06,686 --> 00:41:08,536
It had an on-screen keyboard.


809
00:41:09,026 --> 00:41:11,966
So, whenever the user touches
the key, you send a note-on.


810
00:41:11,966 --> 00:41:13,946
When the key is released,
you send a note-off.


811
00:41:14,476 --> 00:41:17,816
So, the APIs for
sending MIDI events are


812
00:41:17,816 --> 00:41:22,826
in the header file MusicDevice.h
and there is a function


813
00:41:22,826 --> 00:41:24,966
in there called
MusicDeviceMIDIEvent.


814
00:41:25,846 --> 00:41:28,466
Here, you pass the
node AudioUnit,


815
00:41:28,956 --> 00:41:31,106
the three byte MIDI--
MIDI message.


816
00:41:31,696 --> 00:41:34,826
And here, offsetSampleFrames,
the final parameter,


817
00:41:35,386 --> 00:41:37,716
that would be used for
sample-accurate scheduling


818
00:41:37,716 --> 00:41:42,226
but since we're doing this
in kind of a UI context,


819
00:41:42,446 --> 00:41:45,956
we don't really know how to have
that kind of sample accuracy.


820
00:41:46,256 --> 00:41:49,066
I'll get into how
we do in a moment.


821
00:41:49,066 --> 00:41:51,966
So, we just passed this sample
offset frames of zero that--


822
00:41:52,036 --> 00:41:54,176
at that note-on will
appear at the beginning


823
00:41:54,176 --> 00:41:55,526
of the next rendered buffer.


824
00:41:55,526 --> 00:41:59,446
Now, if we do want to
do sample-accurate,


825
00:41:59,446 --> 00:42:03,006
scheduling then we have to
schedule our MIDI events


826
00:42:03,006 --> 00:42:05,536
on the same thread that
were rendering the audio,


827
00:42:05,976 --> 00:42:08,036
because in that thread context,


828
00:42:08,136 --> 00:42:11,376
we can say where the MIDI
events need to land relative


829
00:42:11,376 --> 00:42:12,806
to the beginning of
that audio buffer.


830
00:42:13,346 --> 00:42:14,996
For instance if that MIDI buffer


831
00:42:14,996 --> 00:42:19,416
or audio buffer rather is 1,024
frames, we might do some math


832
00:42:19,416 --> 00:42:23,396
and figure out, oh, that note-on
needs to land at 412 samples


833
00:42:23,396 --> 00:42:27,576
in to that sample buffer and
we can specify that in our call


834
00:42:27,636 --> 00:42:29,966
to MusicDeviceMIDIEvent.


835
00:42:30,416 --> 00:42:33,306
Now, of course, we can call
MusicDeviceMIDIEvent any number


836
00:42:33,306 --> 00:42:36,716
of times to schedule any number
of events for one render cycle.


837
00:42:36,716 --> 00:42:40,036
I just put these next to each
other to emphasize that you have


838
00:42:40,036 --> 00:42:43,076
to be in the rendering
thread context to be able


839
00:42:43,116 --> 00:42:45,466
to schedule sample-accurately.


840
00:42:46,076 --> 00:42:48,616
Now, for you're using
AUGraph and you want


841
00:42:48,616 --> 00:42:51,356
to schedule sample-accurately,
it's similar


842
00:42:51,356 --> 00:42:54,566
but a little different because
you're not calling AUGgraph--


843
00:42:54,566 --> 00:42:56,586
I'm sorry, you're not
calling AudioUnitRender,


844
00:42:56,946 --> 00:42:58,776
the graph is doing
that on your behalf.


845
00:42:59,846 --> 00:43:03,306
So, the way to do this,
there's an AUGraph API


846
00:43:03,306 --> 00:43:06,296
that lets you get called
back in the render context


847
00:43:06,296 --> 00:43:08,716
and that's
AUGraphAddRenderNotify.


848
00:43:09,886 --> 00:43:12,926
That gives you a callback
function that the graph calls


849
00:43:13,206 --> 00:43:15,876
at the beginning of the render
cycle before actually pulling


850
00:43:15,876 --> 00:43:16,866
audio from the node.


851
00:43:17,316 --> 00:43:20,456
And that turns out to be
the precisely corrects time


852
00:43:20,866 --> 00:43:23,956
to call MusicDeviceMIDIEvent
to schedule events


853
00:43:24,376 --> 00:43:25,666
for that render cycle.


854
00:43:26,266 --> 00:43:29,926
So, that's the process
of sending MIDIEvents,


855
00:43:30,586 --> 00:43:33,826
let's look at how nodes
receive MIDI Events.


856
00:43:33,946 --> 00:43:35,756
So, we have two basic
functions for sending


857
00:43:35,756 --> 00:43:37,416
and then there's
MusicDeviceMIDIEvent


858
00:43:37,416 --> 00:43:38,836
and MusicDeviceSysEx.


859
00:43:39,316 --> 00:43:42,846
And we have two corresponding
callback functions for the use


860
00:43:42,846 --> 00:43:45,136
of the node application,
the MIDIEventProc


861
00:43:45,136 --> 00:43:46,496
and the MIDISysExProc.


862
00:43:48,826 --> 00:43:50,636
So, in the node application,


863
00:43:50,636 --> 00:43:52,866
here we have an example
of MIDIEventProc.


864
00:43:52,866 --> 00:43:55,456
Well, it doesn't
do much but here is


865
00:43:55,456 --> 00:43:58,366
where you receive each event
that's coming from the host


866
00:43:58,526 --> 00:44:02,376
and typically, you would just
save it up in a local structure


867
00:44:02,976 --> 00:44:06,616
and use it the next
time you render a buffer


868
00:44:08,276 --> 00:44:11,236
because this function will
get called at the beginning


869
00:44:11,236 --> 00:44:14,046
of each render cycle
with new events


870
00:44:14,046 --> 00:44:15,576
that apply to that render cycle.


871
00:44:16,186 --> 00:44:18,586
So, having created
that callback function,


872
00:44:18,586 --> 00:44:21,276
we can populate a
structure of callbacks.


873
00:44:21,666 --> 00:44:23,906
You can notice I left
the SysExProc null,


874
00:44:23,906 --> 00:44:25,876
that just means I'm
not going to get called


875
00:44:25,936 --> 00:44:27,956
if there is any SysEx.


876
00:44:28,036 --> 00:44:32,376
We use AudioUnitSetProperty to
install those callbacks and now,


877
00:44:32,956 --> 00:44:34,486
on the node application side,


878
00:44:34,486 --> 00:44:37,686
I'm going to receive each
MIDIEvent as it arrives.


879
00:44:39,256 --> 00:44:43,256
So, that's how hosts
can sent MIDI to nodes.


880
00:44:43,756 --> 00:44:47,016
Let's look now at how host can
communicate their transport


881
00:44:47,016 --> 00:44:48,656
and timeline information
to nodes.


882
00:44:49,226 --> 00:44:53,476
So, the important thing
about this model is


883
00:44:53,476 --> 00:44:55,546
that the host is
always the master here.


884
00:44:56,256 --> 00:44:58,846
The nodes can just find
out where the host is


885
00:44:58,846 --> 00:45:00,016
and synchronize to that.


886
00:45:00,266 --> 00:45:04,976
We'll look at how the host can
communicate its musical position


887
00:45:05,866 --> 00:45:08,126
as well as the state
of its transport.


888
00:45:08,126 --> 00:45:12,366
And all of this is highly
precise and it's called


889
00:45:12,366 --> 00:45:15,246
and pertains to the
render context.


890
00:45:15,596 --> 00:45:21,116
So here too, we have a structure
full of callback functions,


891
00:45:21,226 --> 00:45:22,496
we'll look at each of these.


892
00:45:23,466 --> 00:45:26,496
So, this is probably
the most common one


893
00:45:26,496 --> 00:45:27,816
that a host will implement,


894
00:45:28,206 --> 00:45:30,346
this is called the
BeatAndTempo callback.


895
00:45:31,166 --> 00:45:33,666
Here, the host can
say for the beginning


896
00:45:33,666 --> 00:45:37,136
of the current audio buffer,
Where am I in the track


897
00:45:37,136 --> 00:45:40,826
and that could be
in between beats.


898
00:45:40,826 --> 00:45:44,756
The host can also communicate
what the current tempo is.


899
00:45:44,866 --> 00:45:48,116
And so with these two pieces
of information, even--


900
00:45:48,116 --> 00:45:50,126
even only these two
pieces information,


901
00:45:50,126 --> 00:45:52,776
the node can do beat
synchronized effects


902
00:45:52,776 --> 00:45:54,546
from the host for instance.


903
00:45:55,116 --> 00:46:01,016
There's also some more detailed
musical location information


904
00:46:01,016 --> 00:46:03,816
supplied by the host such as
the current time signature.


905
00:46:03,896 --> 00:46:08,556
And finally, the host
can communicate some bits


906
00:46:08,556 --> 00:46:11,126
of transport state, most
notably whether it's playing


907
00:46:11,126 --> 00:46:11,806
or recording.


908
00:46:11,806 --> 00:46:14,546
There's also a facility
for the host


909
00:46:14,546 --> 00:46:17,576
to express whether it's
cycling or looping.


910
00:46:20,636 --> 00:46:23,546
So here too, we're installing
a set of callback from--


911
00:46:23,606 --> 00:46:26,876
callback functions
on an audio unit.


912
00:46:26,876 --> 00:46:29,576
The host populates the host
callback info structure,


913
00:46:29,926 --> 00:46:32,766
installs the callback
functions that it implements


914
00:46:32,956 --> 00:46:34,886
and calls AudioUnitSetProperty.


915
00:46:35,416 --> 00:46:39,576
So, once the host does this, the
system will call those callbacks


916
00:46:39,576 --> 00:46:42,466
at the beginning of each
render cycle and communicate


917
00:46:42,466 --> 00:46:45,436
that over-- that information
over to the node process


918
00:46:46,456 --> 00:46:50,256
where the node application
will have access to them.


919
00:46:50,586 --> 00:46:54,906
And the way the node
application gets that access is


920
00:46:54,906 --> 00:46:56,936
by fetching the host
callback property.


921
00:46:57,926 --> 00:47:01,076
It will receive that structure
full of function pointers.


922
00:47:01,076 --> 00:47:02,656
They won't actually
point to functions


923
00:47:02,656 --> 00:47:05,326
in the host process, of course.


924
00:47:05,526 --> 00:47:08,496
We can't make a cross process
call there, but the information,


925
00:47:08,496 --> 00:47:11,506
as I just said, has been
communicated over to the node.


926
00:47:11,506 --> 00:47:16,006
And it can access them there
within its own process.


927
00:47:17,016 --> 00:47:21,016
There are some considerations
of thread safety here.


928
00:47:21,256 --> 00:47:25,856
Most people importantly, since
this information is accurate


929
00:47:25,856 --> 00:47:28,736
as of the beginning of the
render cycle, if you call it


930
00:47:28,736 --> 00:47:33,446
in some other context, you
might get inconsistent results.


931
00:47:33,826 --> 00:47:38,856
It's easiest if you fetch this
information on the render thread


932
00:47:39,236 --> 00:47:41,466
but of course, there are
some cases where you want


933
00:47:41,466 --> 00:47:43,936
to observe a transport
state for instance.


934
00:47:44,716 --> 00:47:49,986
So, we give you a better
way to receive notifications


935
00:47:49,986 --> 00:47:54,146
of transport state changes on
a non-render thread context.


936
00:47:54,576 --> 00:47:56,236
You can install this
property listener


937
00:47:56,236 --> 00:47:59,176
for the HostTransportState
and get a callback


938
00:47:59,526 --> 00:48:00,796
on a non-render thread.


939
00:48:01,456 --> 00:48:05,176
Okay, so that's the
process of transport


940
00:48:05,176 --> 00:48:06,386
and timeline information.


941
00:48:06,386 --> 00:48:09,136
Finally, I'd like to look
at the whole mechanism


942
00:48:09,136 --> 00:48:12,366
by which node applications
can send remote control events


943
00:48:12,606 --> 00:48:13,856
to host applications.


944
00:48:14,016 --> 00:48:16,966
To accomplish that, we
have something called


945
00:48:17,206 --> 00:48:19,476
AudioUnitRemoteControlEvents.


946
00:48:19,646 --> 00:48:21,916
Now, there's something
called RemoteControlEvents


947
00:48:21,916 --> 00:48:23,116
in UIKit as well.


948
00:48:23,116 --> 00:48:26,386
Those are kind of in
a different world.


949
00:48:27,036 --> 00:48:30,286
These are more specific to the
needs of audio applications.


950
00:48:30,776 --> 00:48:33,896
So with these events, the
node can control the host


951
00:48:33,896 --> 00:48:35,366
application's transport.


952
00:48:35,896 --> 00:48:38,566
And for now, we have these
three events to find.


953
00:48:38,716 --> 00:48:42,186
You can toggle-- you being a
node application-can toggle the


954
00:48:42,806 --> 00:48:49,116
host's play or pause state, its
recording state and the node,


955
00:48:49,246 --> 00:48:51,066
through an event, can
send the host back


956
00:48:51,066 --> 00:48:54,096
to the beginning of
the song or track.


957
00:48:54,636 --> 00:48:56,666
We do have some sample
applications


958
00:48:56,666 --> 00:49:00,116
where our node applications
have some standard looking


959
00:49:00,116 --> 00:49:01,166
transport controls.


960
00:49:01,166 --> 00:49:03,486
And we'd like to encourage you
to check those out and use them


961
00:49:03,486 --> 00:49:06,836
in your application so that
we can have a consistent look


962
00:49:06,836 --> 00:49:08,146
and feel for these controls.


963
00:49:09,796 --> 00:49:14,436
So, looking at how node
applications can send


964
00:49:15,046 --> 00:49:17,356
RemoteControlEvents,
first, we want to find


965
00:49:17,356 --> 00:49:19,736
out whether the host actually
is listening and is going


966
00:49:19,736 --> 00:49:21,416
to support them because
if it doesn't,


967
00:49:21,416 --> 00:49:22,326
maybe we don't even want


968
00:49:22,326 --> 00:49:24,486
to bother drawing the
transport controls at all.


969
00:49:24,996 --> 00:49:26,836
So to do that, we can
fetch this property


970
00:49:26,836 --> 00:49:28,946
HostReceivesRemoteControlEvents.


971
00:49:29,826 --> 00:49:32,186
And to actually send
the RemoteControlEvent,


972
00:49:32,756 --> 00:49:35,916
the node calls
AudioUnitSetProperty using the


973
00:49:35,916 --> 00:49:37,996
remote control to
event or I'm sorry,


974
00:49:37,996 --> 00:49:39,476
remote control to host event.


975
00:49:39,716 --> 00:49:43,066
And the value of that
property is the actual control


976
00:49:43,066 --> 00:49:45,766
to be sent, toggle, or
record on this example.


977
00:49:46,806 --> 00:49:49,386
So, there's a node sending
a RemoteControlEvent.


978
00:49:50,286 --> 00:49:55,146
Here is a host receiving
one or rather preparing


979
00:49:55,146 --> 00:49:56,266
to receive them, I should say.


980
00:49:57,006 --> 00:50:00,046
So, to do that, the host creates
a block called the listenerBlock


981
00:50:01,766 --> 00:50:06,016
and in that block, the host
simply takes the incoming


982
00:50:06,016 --> 00:50:09,246
AudioUnitRemoteControlEvent
and passes it to one


983
00:50:09,246 --> 00:50:11,886
of its own methods called
handleRemoteControlEvent.


984
00:50:12,626 --> 00:50:16,426
Now, that block is in
turn a property value


985
00:50:16,426 --> 00:50:18,576
for the
RemoteControlEventListener


986
00:50:18,576 --> 00:50:22,266
property so the host only
has to set that property


987
00:50:22,596 --> 00:50:27,836
on the node AudioUnit and that
accomplishes the installation


988
00:50:27,866 --> 00:50:29,856
of the listener for
RemoteControlEvents.


989
00:50:30,306 --> 00:50:33,656
Next, I'd like to bring up
my colleague Harry Tormey


990
00:50:33,656 --> 00:50:35,996
to show you about some
of these other aspects


991
00:50:35,996 --> 00:50:38,096
of the inter-app
audio API in action.


992
00:50:38,886 --> 00:50:42,136
>> Thanks Doug.


993
00:50:42,486 --> 00:50:46,586
Hey everybody, my name is Harry
Tormey and I work with Doug


994
00:50:46,586 --> 00:50:47,946
in the Core Audio
Group at Apple.


995
00:50:48,626 --> 00:50:51,626
And today, I'm going to be
giving you a demonstration


996
00:50:51,626 --> 00:50:53,526
of some of the sample
applications we're going


997
00:50:53,526 --> 00:50:55,136
to be releasing on
the developer portal


998
00:50:55,206 --> 00:50:57,616
to illustrate how
inter-app audio works.


999
00:50:59,066 --> 00:51:00,936
The first demo I'm going
to be giving you is


1000
00:51:00,936 --> 00:51:05,616
of a host application connecting
to a sampler node application


1001
00:51:05,896 --> 00:51:07,686
and sending it some MIDI events.


1002
00:51:08,506 --> 00:51:11,616
So what you see in the screen
up there is a host application


1003
00:51:11,616 --> 00:51:13,276
and I'm going to bring up a list


1004
00:51:13,276 --> 00:51:16,996
of all the remote instrument
node applications installed


1005
00:51:16,996 --> 00:51:18,186
on this device and
I'm going to do


1006
00:51:18,186 --> 00:51:19,776
that by touching the
add instrument button.


1007
00:51:21,026 --> 00:51:24,016
So, none of these applications
are currently running.


1008
00:51:24,476 --> 00:51:25,886
They have just published
themselves


1009
00:51:25,886 --> 00:51:27,506
with their audio
component descriptions.


1010
00:51:27,806 --> 00:51:30,516
When I select one of these
applications from the list,


1011
00:51:30,746 --> 00:51:33,596
it will launch into the
background and connect


1012
00:51:33,596 --> 00:51:35,076
to the host application.


1013
00:51:35,456 --> 00:51:37,636
So I'm going to do that, I'm
going to select the sampler.


1014
00:51:38,756 --> 00:51:41,786
OK. So, you can see
the sampler's icon


1015
00:51:41,786 --> 00:51:44,056
up there underneath
the instrument label.


1016
00:51:44,276 --> 00:51:46,246
That means it's connected
to the host application.


1017
00:51:46,576 --> 00:51:49,106
So, I'm going to bring
up a keyboard in the host


1018
00:51:49,106 --> 00:51:52,166
by touching the show
keyboard button and I'm going


1019
00:51:52,166 --> 00:51:55,466
to send some MIDI
events from the host


1020
00:51:55,466 --> 00:52:00,556
to the sampler by
playing the keys.


1021
00:52:00,686 --> 00:52:01,436
[Music] Totally awesome.


1022
00:52:02,336 --> 00:52:06,866
OK. So, what if I want
to change the sample bank


1023
00:52:06,866 --> 00:52:08,056
that the sampler is using?


1024
00:52:08,386 --> 00:52:10,616
Well, I'm going to have to do
to the sampler and do that.


1025
00:52:11,106 --> 00:52:13,216
I'm going to do that by
touching the sampler's icon.


1026
00:52:14,016 --> 00:52:16,816
We're now in a separate
application and I'm going


1027
00:52:16,816 --> 00:52:19,826
to select a different
sample bank to use so how


1028
00:52:19,826 --> 00:52:21,596
about something nice
like a harpsichord?


1029
00:52:22,026 --> 00:52:24,256
Let me just do that there.


1030
00:52:24,256 --> 00:52:26,026
OK. So now, we're
in harpsichord,


1031
00:52:26,026 --> 00:52:28,546
I'm going to touch the
host icon there and go back


1032
00:52:28,546 --> 00:52:29,666
to the host application.


1033
00:52:30,356 --> 00:52:36,736
Touch the show keyboard
again and listen for it.


1034
00:52:36,736 --> 00:52:36,803
[ Music ]


1035
00:52:36,803 --> 00:52:38,576
That's a harpsichord.


1036
00:52:39,746 --> 00:52:42,186
OK. So, the next thing that
I'm going to show you is how


1037
00:52:42,186 --> 00:52:45,716
to use the callbacks that the
host application has published


1038
00:52:45,996 --> 00:52:49,676
to get the time code of the
host application when it records


1039
00:52:49,676 --> 00:52:50,616
and plays back things.


1040
00:52:50,896 --> 00:52:53,446
So one again, I'm going to go
the sampler by touching its icon


1041
00:52:53,446 --> 00:52:57,286
and I'm going to touch the
record button and I'm going


1042
00:52:57,286 --> 00:52:59,346
to record some audio in the host


1043
00:52:59,346 --> 00:53:01,816
so I'm sending a remote
message to the host.


1044
00:53:02,876 --> 00:53:06,836
[Music] I'm going
to stop recoding


1045
00:53:06,836 --> 00:53:07,966
by touching record button again.


1046
00:53:08,216 --> 00:53:09,366
Now, what I want
you to pay attention


1047
00:53:09,366 --> 00:53:12,196
to is the blue text
over the play button.


1048
00:53:12,806 --> 00:53:15,286
This text is going to be
updated with the callbacks


1049
00:53:15,286 --> 00:53:17,396
that the host application
has published and were going


1050
00:53:17,396 --> 00:53:20,686
to use this to display a
time code indicating how far


1051
00:53:20,686 --> 00:53:21,786
into the recording we are.


1052
00:53:21,786 --> 00:53:28,126
So, I'm going to touch the play
button and watch that text.


1053
00:53:28,126 --> 00:53:33,766
[Music] So, if I do
that again and I go


1054
00:53:33,766 --> 00:53:36,596
to the host application, you'll
see the time code is consistent


1055
00:53:36,596 --> 00:53:38,856
across both applications
so let me do that.


1056
00:53:38,856 --> 00:53:41,666
Let me press play again and go
to-- back to host application.


1057
00:53:42,116 --> 00:53:50,986
[Music] Okay, so
for my grand finale,


1058
00:53:51,326 --> 00:53:54,756
I'm going to add an effect
and that effect is going


1059
00:53:54,756 --> 00:53:56,426
to be the delay effect
that you saw.


1060
00:53:56,466 --> 00:53:59,046
So once again, I touched
the add effect button.


1061
00:53:59,316 --> 00:54:00,786
It shows you all of the effects


1062
00:54:00,786 --> 00:54:02,746
that are installed
on this device.


1063
00:54:02,746 --> 00:54:04,876
I'm going to select the delay
one, it's going to launch it


1064
00:54:04,876 --> 00:54:07,446
and connect to the host.


1065
00:54:07,446 --> 00:54:11,486
OK. So in the host, if I touched
the show keyboard button again


1066
00:54:11,486 --> 00:54:13,636
and play a note, it's
going to be delayed.


1067
00:54:18,236 --> 00:54:18,836
[Music] How about that?


1068
00:54:18,966 --> 00:54:20,776
Much cooler than
remote controlled cars.


1069
00:54:21,256 --> 00:54:24,546
Okay everyone, that's
me, these demos are all


1070
00:54:24,546 --> 00:54:28,296
up on the developer portal and
I'm done with my demo so back


1071
00:54:28,296 --> 00:54:29,746
over to you Doug and
thank you very much.


1072
00:54:30,246 --> 00:54:32,806
[Applause]


1073
00:54:33,306 --> 00:54:33,896
>> Thank you Harry.


1074
00:54:33,896 --> 00:54:37,556
Hey, I found the right button.


1075
00:54:38,006 --> 00:54:44,726
So, back to some more
mundane matters here.


1076
00:54:44,826 --> 00:54:47,516
Dealing with audio session
interruptions, both host


1077
00:54:47,516 --> 00:54:49,346
and node applications
need to deal


1078
00:54:49,346 --> 00:54:50,706
with audio session
interruptions.


1079
00:54:51,296 --> 00:54:53,996
Here, the usual rules
apply namely


1080
00:54:53,996 --> 00:54:56,696
that your AURemoteIO gets
stopped underneath you.


1081
00:54:56,696 --> 00:54:59,296
But furthermore, in
a host application,


1082
00:54:59,296 --> 00:55:02,096
the system will uninitialize
any node AudioUnits


1083
00:55:02,096 --> 00:55:02,956
that you have open.


1084
00:55:03,366 --> 00:55:06,126
This will reclaim the
resources I've been talking


1085
00:55:06,126 --> 00:55:10,506
about that you acquire when you
initialize the node AudioUnit.


1086
00:55:12,076 --> 00:55:14,376
One other bit of
housekeeping here,


1087
00:55:14,836 --> 00:55:16,866
you can make your
application more robust


1088
00:55:16,866 --> 00:55:19,886
if you handle a media
services reset correctly.


1089
00:55:19,946 --> 00:55:25,506
It's a little bit hard to test
this sometimes-- oops, but--


1090
00:55:25,506 --> 00:55:28,946
let me find my way back.


1091
00:55:29,796 --> 00:55:33,146
But if you implement this,


1092
00:55:33,146 --> 00:55:37,476
your application will
survive calamities.


1093
00:55:38,276 --> 00:55:40,506
So, when this happens, you can--


1094
00:55:40,506 --> 00:55:42,256
you will find out that all


1095
00:55:42,256 --> 00:55:44,646
of your inter-app audio
connections have been broken,


1096
00:55:44,646 --> 00:55:46,826
the component instances
have been invalidated.


1097
00:55:47,236 --> 00:55:50,086
So, in a host audio--
host application,


1098
00:55:50,086 --> 00:55:53,376
you should dispose your node
AudioUnit and your AURemoteIO.


1099
00:55:53,906 --> 00:55:55,486
And in a node application,


1100
00:55:55,636 --> 00:55:57,836
you should also dispose
your AURemoteIO.


1101
00:55:58,346 --> 00:56:00,016
So in general, it's simplest


1102
00:56:00,016 --> 00:56:03,026
to dispose your entire audio
engine including those Apple


1103
00:56:03,026 --> 00:56:03,936
Audio objects.


1104
00:56:04,716 --> 00:56:07,166
And then, start over
from scratch


1105
00:56:07,166 --> 00:56:09,246
as if your app has
just been launched


1106
00:56:09,476 --> 00:56:10,786
and that's the simplest way


1107
00:56:10,786 --> 00:56:14,316
to robustly handle the
media services being reset.


1108
00:56:16,856 --> 00:56:19,976
Some questions that have come
up in showing this feature


1109
00:56:19,976 --> 00:56:21,626
to people, in talking with them,


1110
00:56:21,986 --> 00:56:24,506
can you have multiple
host applications?


1111
00:56:24,906 --> 00:56:26,456
Yes, if they are all mixable.


1112
00:56:26,996 --> 00:56:29,056
If one is unmixable, of course,


1113
00:56:29,056 --> 00:56:32,126
it will interrupt everything
else as it takes control.


1114
00:56:32,896 --> 00:56:36,336
Also, if you were to have
multiple host that are mixable


1115
00:56:36,336 --> 00:56:41,686
and one node application,
only one host can connect


1116
00:56:41,776 --> 00:56:42,936
to that node at a time.


1117
00:56:43,686 --> 00:56:45,536
Can you have multiple
node applications?


1118
00:56:45,536 --> 00:56:48,646
Yes, Harry just showed us that
that's more than possible.


1119
00:56:49,246 --> 00:56:53,016
A couple of debugging
tips here you may find


1120
00:56:53,106 --> 00:56:55,076
when creating a node application


1121
00:56:55,076 --> 00:56:57,356
that you're having
trouble getting it to show


1122
00:56:57,356 --> 00:56:59,236
up in host applications.


1123
00:57:00,346 --> 00:57:03,116
If you see that happening, you
should watch the system log.


1124
00:57:03,246 --> 00:57:05,896
We try to leave some
clues for you there


1125
00:57:06,316 --> 00:57:07,696
in the form of error messages.


1126
00:57:08,016 --> 00:57:10,376
If you see a problem with
your Info.plist entry


1127
00:57:10,376 --> 00:57:13,076
which is a little bit
easy to do unfortunately


1128
00:57:13,076 --> 00:57:15,356
but if you do see a problem
there, we'll tell you that


1129
00:57:15,846 --> 00:57:18,786
and I would recommend going
and comparing your Info.plist


1130
00:57:19,036 --> 00:57:21,656
with the one-- in one of
our example applications.


1131
00:57:21,896 --> 00:57:26,556
I should also mention here
the infamous error of 12,985


1132
00:57:26,586 --> 00:57:29,666
which many people stub
their toes on in a lot


1133
00:57:29,666 --> 00:57:30,786
of different contexts.


1134
00:57:31,256 --> 00:57:34,786
I can tell you that what it
means is operation denied.


1135
00:57:35,436 --> 00:57:40,256
And in the context of inter-app
audio, you're likely to hit it


1136
00:57:40,416 --> 00:57:41,986
if you start playing
from the background.


1137
00:57:42,886 --> 00:57:47,136
We do hope to in an upcoming
release give that a proper name


1138
00:57:47,136 --> 00:57:50,036
and maybe another
value but in any case,


1139
00:57:50,036 --> 00:57:51,546
if you do see it,
that's what it means.


1140
00:57:52,636 --> 00:57:55,866
So we've looked at how node
applications register themselves


1141
00:57:55,866 --> 00:57:58,156
with the system,
hosts discover them.


1142
00:57:58,846 --> 00:58:01,856
Hosts create connections
to node applications.


1143
00:58:03,166 --> 00:58:06,946
Once that connection is up, host
and node apps can stream audio


1144
00:58:06,946 --> 00:58:08,206
to and from each other.


1145
00:58:08,526 --> 00:58:11,066
Host apps can send MIDI
to node applications.


1146
00:58:11,736 --> 00:58:13,646
Hosts can communicate
their transport


1147
00:58:13,646 --> 00:58:14,916
and timeline information.


1148
00:58:15,416 --> 00:58:19,176
And finally, we have seen how
nodes can remotely control hosts


1149
00:58:20,396 --> 00:58:23,246
so I think if you
have an existing music


1150
00:58:23,246 --> 00:58:26,086
or audio application,
it's not that much work


1151
00:58:26,086 --> 00:58:27,466
to convert it to a node.


1152
00:58:27,996 --> 00:58:30,206
It's mostly adding a
little bit of code to deal


1153
00:58:30,206 --> 00:58:32,966
with the transitions to and
from the connected state


1154
00:58:33,016 --> 00:58:34,596
and you can look how that works


1155
00:58:34,966 --> 00:58:37,286
in the example apps
we have posted.


1156
00:58:37,896 --> 00:58:40,156
Creating a host application
is a bit more work


1157
00:58:40,156 --> 00:58:43,016
but you're using existing
API for audio units


1158
00:58:43,016 --> 00:58:45,726
and there's a lot of history
there as well as powerful--


1159
00:58:46,046 --> 00:58:47,706
there's a lot of
power and flexibility.


1160
00:58:49,186 --> 00:58:50,276
We also like you to--


1161
00:58:50,726 --> 00:58:53,786
we encourage you to look
at our sample applications.


1162
00:58:54,336 --> 00:58:56,646
They'll help you with a lot
of the little ins and outs


1163
00:58:56,996 --> 00:58:58,156
and we're really looking forward


1164
00:58:58,156 --> 00:59:01,296
to the great music apps
you're going to make.


1165
00:59:01,846 --> 00:59:05,416
On to some housekeeping matters
here, if you wish to talk


1166
00:59:05,416 --> 00:59:07,656
to an Apple Evangelist,
there's John Geleynse.


1167
00:59:07,956 --> 00:59:11,056
Here are some links
to some documentation


1168
00:59:11,056 --> 00:59:12,546
and our developer forums.


1169
00:59:13,176 --> 00:59:15,346
This is the only Core
Audio Session this year


1170
00:59:15,806 --> 00:59:18,756
but here are some other media
sessions later this week


1171
00:59:18,756 --> 00:59:19,926
that you might be interested in.


1172
00:59:20,766 --> 00:59:21,806
Thank you very much.


1173
00:59:22,306 --> 00:59:29,690
[ Silence ]

